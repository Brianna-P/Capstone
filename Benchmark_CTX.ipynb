{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61725e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_translation(n_ctx_value, queries, metadata_lookup, batch_size=20, output_file=None):\n",
    "    from llama_cpp import Llama\n",
    "    import time\n",
    "\n",
    "    llm = Llama(\n",
    "        model_path=\"/Users/pierr/Desktop/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "        n_ctx=n_ctx_value\n",
    "    )\n",
    "\n",
    "    def translate_query(query, metadata_lookup):\n",
    "        # Dynamically add metadata descriptions for each field used in the query\n",
    "        query_fields = [field.strip(\"[]\") for field in query.split() if field.startswith(\"[\") and field.endswith(\"]\")]\n",
    "        \n",
    "        # Ensure that metadata descriptions exist for each field used in the query\n",
    "        metadata_descriptions = [metadata_lookup.get(field, f\"No metadata found for {field}\") for field in query_fields]\n",
    "\n",
    "        # Combine metadata descriptions into one string\n",
    "        metadata = \"\\n\".join(metadata_descriptions)\n",
    "\n",
    "        # Formulate the prompt using query and the gathered metadata\n",
    "        prompt = f\"Translate the following ESGish query into a natural English sentence:\\n\\nQuery: {query}\\n\\nMetadata: {metadata}\\n\\n### Response:\"\n",
    "\n",
    "        # Truncate query and metadata if needed to fit within token limits\n",
    "        prompt_tokens = llm.tokenize(prompt.encode(\"utf-8\"))\n",
    "        prompt_token_count = len(prompt_tokens)\n",
    "\n",
    "        # Calculate how many tokens are available for the response\n",
    "        max_response_tokens = n_ctx_value - prompt_token_count\n",
    "        if max_response_tokens <= 0:\n",
    "            raise ValueError(f\"Prompt too long for context size {n_ctx_value}. Try increasing n_ctx or shortening input.\")\n",
    "        \n",
    "        # # If the prompt exceeds n_ctx, truncate the query or metadata (whichever is larger)\n",
    "        # if prompt_token_count > n_ctx_value:\n",
    "        #     # First, try truncating the query\n",
    "        #     available_tokens_for_query = n_ctx_value - len(llm.tokenize(f\"Query: {metadata}\".encode(\"utf-8\"))) - 50  # leave 50 tokens for response\n",
    "        #     query_tokens = llm.tokenize(query.encode(\"utf-8\"))\n",
    "        #     query_tokens = query_tokens[:available_tokens_for_query]  # Truncate the query if necessary\n",
    "        #     query = llm.detokenize(query_tokens).decode(\"utf-8\", errors=\"ignore\")\n",
    "            \n",
    "        #     # Regenerate the prompt after truncation\n",
    "        #     prompt = f\"Translate the following ESGish query into a natural English sentence:\\n\\nQuery: {query}\\n\\nMetadata: {metadata}\\n\\n### Response:\"\n",
    "\n",
    "        #     # Check again if we still exceed the context size and truncate metadata if necessary\n",
    "        #     prompt_tokens = llm.tokenize(prompt.encode(\"utf-8\"))\n",
    "        #     prompt_token_count = len(prompt_tokens)\n",
    "\n",
    "        if prompt_token_count > n_ctx_value:\n",
    "            available_tokens_for_metadata = n_ctx_value - len(llm.tokenize(f\"Query: {query}\".encode(\"utf-8\"))) - 50\n",
    "            metadata_tokens = llm.tokenize(metadata.encode(\"utf-8\"))\n",
    "            metadata_tokens = metadata_tokens[:available_tokens_for_metadata]  # Truncate the metadata if necessary\n",
    "            metadata = llm.detokenize(metadata_tokens).decode(\"utf-8\", errors=\"ignore\")\n",
    "            # Regenerate the final prompt with truncated metadata\n",
    "            prompt = f\"Translate the following ESGish query into a natural English sentence:\\n\\nQuery: {query}\\n\\nMetadata: {metadata}\\n\\n### Response:\"\n",
    "\n",
    "        # Now send the truncated (or non-truncated) prompt to the model\n",
    "        response = llm(prompt, max_tokens=max_response_tokens, temperature=0.1)\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    translated_all = []\n",
    "    start = time.time()\n",
    "\n",
    "    # Loop through each query and process the translation in batches\n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch = queries[i:i + batch_size]\n",
    "        \n",
    "        # For each query in the batch, get the translation using the metadata lookup\n",
    "        translated_batch = [translate_query(query, metadata_lookup) for query in batch]\n",
    "        translated_all.extend(translated_batch)\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    avg_time = duration / len(queries)\n",
    "\n",
    "    return {\n",
    "        \"n_ctx\": n_ctx_value,\n",
    "        \"total_time_sec\": duration,\n",
    "        \"avg_time_per_query_sec\": avg_time,\n",
    "        \"num_queries\": len(queries)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Assuming the file is named 'metadata.json'\n",
    "with open('metadata.json', 'r') as f:\n",
    "    raw_metadata = json.load(f)\n",
    "\n",
    "metadata_lookup = {\n",
    "    entry[\"code\"]: f'{entry[\"name\"]}. {entry[\"description\"]}' \n",
    "    for entry in raw_metadata\n",
    "}\n",
    "\n",
    "results = []\n",
    "context_sizes = [1024, 2048, 3072, 4096]\n",
    "\n",
    "df = pd.read_csv(\"results2.csv\")\n",
    "queries = df[\"query_text\"].tolist()\n",
    "\n",
    "for ctx in context_sizes:\n",
    "    result = benchmark_translation(ctx, queries, metadata_lookup)\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c2be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_results[\"n_ctx\"], df_results[\"avg_time_per_query_sec\"], marker='o')\n",
    "plt.title(\"LLM Performance vs Context Size\")\n",
    "plt.xlabel(\"Context Size (n_ctx)\")\n",
    "plt.ylabel(\"Avg Time per Query (sec)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
