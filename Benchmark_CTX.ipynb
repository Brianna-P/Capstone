{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61725e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_translation(n_ctx_value, queries, batch_size=20, output_file=None):\n",
    "    from llama_cpp import Llama\n",
    "    import time\n",
    "\n",
    "    llm = Llama(\n",
    "        model_path=\"/Users/pierr/Desktop/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "        n_ctx=n_ctx_value\n",
    "    )\n",
    "\n",
    "    def translate_query(query):\n",
    "        if n_ctx_value == 512: # abbreviated form\n",
    "            cheat_sheet = \"\"\"Rephrase this ESGish query as a natural English sentence. Each query is asking for all companies or issuers that match some paramater.\n",
    "            \"\"\"  \n",
    "        else:\n",
    "            cheat_sheet =\"\"\"\n",
    "            ### ESGish Query Language â€“ Cheat Sheet\n",
    "\n",
    "            1. Query Structure:\n",
    "            - A query can be a single condition or a group of conditions using AND, OR, NOT.\n",
    "\n",
    "            2. Filters:\n",
    "            - Format: [FieldName] Operator 'Value'\n",
    "            - Operators: =, >, <, >=, <=, !=\n",
    "            - Example: [BoardIndependencePct] > '0.7'\n",
    "\n",
    "            3. Functions:\n",
    "            - RATIO(F1, F2): ratio of F1 to F2\n",
    "            - SUM(F1, F2, ...): total of multiple fields\n",
    "            - CASE_COUNT(Query): number of cases matching a query\n",
    "\n",
    "            4. Logical Operators:\n",
    "            - AND(...), OR(...), NOT(...)\n",
    "            - Used to combine multiple conditions\n",
    "\n",
    "            5. Syntax Notes:\n",
    "            - Field names use square brackets []\n",
    "            - Values are quoted: 'Yes', '0.5', 'High'\n",
    "            - Fields can be numeric, boolean, date, or string\n",
    "\n",
    "            6. Your Task:\n",
    "            - Reprhase the ESGish queries provided as natural English statements\n",
    "            - Use lead-ins like: \"Find companies where...\", \"Show issuers with...\", etc.\n",
    "            - Output should be clear, concise, and human-friendly\n",
    "            \"\"\"\n",
    "        \n",
    "        # Truncate query if needed\n",
    "        cheat_sheet_tokens = llm.tokenize(cheat_sheet.encode(\"utf-8\"))\n",
    "        max_query_tokens = n_ctx_value - len(cheat_sheet_tokens) - 100  # leave 100 tokens for response\n",
    "\n",
    "        query_tokens = llm.tokenize(query.encode(\"utf-8\"))\n",
    "        if len(query_tokens) > max_query_tokens:\n",
    "            query_tokens = query_tokens[:max_query_tokens]\n",
    "            query = llm.detokenize(query_tokens).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "        prompt = f\"{cheat_sheet}\\n\\n{query}\\n\\n### Response:\"\n",
    "        prompt_tokens = llm.tokenize(prompt.encode(\"utf-8\"))\n",
    "        prompt_token_count = len(prompt_tokens)\n",
    "\n",
    "        max_response_tokens = n_ctx_value - prompt_token_count\n",
    "        if max_response_tokens <= 0:\n",
    "            raise ValueError(f\"Prompt too long for context size {n_ctx_value}. Try increasing n_ctx or shortening input.\")\n",
    "\n",
    "        response = llm(prompt, max_tokens=max_response_tokens, temperature=0.1)\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    translated_all = []\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch = queries[i:i + batch_size]\n",
    "        translated_batch = [translate_query(q) for q in batch]\n",
    "        translated_all.extend(translated_batch)\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    avg_time = duration / len(queries)\n",
    "\n",
    "    return {\n",
    "        \"n_ctx\": n_ctx_value,\n",
    "        \"total_time_sec\": duration,\n",
    "        \"avg_time_per_query_sec\": avg_time,\n",
    "        \"num_queries\": len(queries)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "context_sizes = [512, 2048, 3072, 4096]\n",
    "df = pd.read_csv(\"results2.csv\")\n",
    "queries = df[\"query_text\"].tolist()\n",
    "\n",
    "for ctx in context_sizes:\n",
    "    result = benchmark_translation(ctx, queries)\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c2be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_results[\"n_ctx\"], df_results[\"avg_time_per_query_sec\"], marker='o')\n",
    "plt.title(\"LLM Performance vs Context Size\")\n",
    "plt.xlabel(\"Context Size (n_ctx)\")\n",
    "plt.ylabel(\"Avg Time per Query (sec)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
