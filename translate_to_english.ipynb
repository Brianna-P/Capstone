{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11329b8f-ae0a-48d4-9da9-7c1c06875c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (3.1.5)\n",
      "Requirement already satisfied: transformers in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-cpp-python in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (2.2.3)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas openpyxl transformers torch\n",
    "%pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a607fdc-3c30-4006-af0d-60789c816fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/solar/OneDrive/Documents/Capstone/Capstone-Jupyter/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 4096\n",
      "llama_init_from_model: n_ctx_per_seq = 4096\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_init_from_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "model_path = \"/Users/solar/OneDrive/Documents/Capstone/Capstone-Jupyter/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "max_context = 4096\n",
    "llm = Llama(\n",
    "    model_path,\n",
    "    n_ctx=max_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4566bf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest entry code: EUTaxManIntSerElcRevOverAlign\n",
      "Length: 1353\n",
      "Content: EU Taxon - Man, Instal, Serv of Elec Equip Overall Align. EU Taxon - Man, Instal, Serv of Elec Equip Overall Align: This factor identifies the overall alignment for Manufacture, installation, and servicing of high, medium and low voltage electrical equipment for electrical transmission and distribution that result in or enable a substantial contribution to climate change mitigation, covering the substantial contribution criteria, the do no significant harm criteria, and the minimum social safeguards. This is the aggregated result across all Taxonomy objectives. The possible values are: Aligned,Aligned (>90%),Aligned (>80%),Aligned (>70%),Aligned (>60%),Aligned (>50%),Aligned (>40%),Aligned (>30%),Aligned (>20%),Aligned (>10%),Aligned (>0%),Likely aligned (100%),Likely aligned (>90%),Likely aligned (>80%),Likely aligned (>70%),Likely aligned (>60%),Likely aligned (>50%),Likely aligned (>40%),Likely aligned (>30%),Likely aligned (>20%),Likely aligned (>10%),Likely aligned (>0%),Potentially aligned (100%),Potentially aligned (>90%),Potentially aligned (>80%),Potentially aligned (>70%),Potentially aligned (>60%),Potentially aligned (>50%),Potentially aligned (>40%),Potentially aligned (>30%),Potentially aligned (>20%),Potentially aligned (>10%),Potentially aligned (>0%),Likely not aligned,Not aligned, Not collected, and Not applicable.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open(\"metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_metadata = json.load(f)\n",
    "\n",
    "# Create a lookup table\n",
    "metadata_lookup = {\n",
    "    entry[\"code\"]: f'{entry[\"name\"]}. {entry[\"description\"]}' \n",
    "    for entry in raw_metadata\n",
    "}\n",
    "\n",
    "def extract_codes(query):\n",
    "    return re.findall(r\"\\[([A-Za-z0-9_]+)\\]\", query)\n",
    "\n",
    "# Find the entry with the longest name + description combo\n",
    "longest_entry = max(metadata_lookup.items(), key=lambda item: len(item[1]))\n",
    "\n",
    "# Print the result\n",
    "print(f'Longest entry code: {longest_entry[0]}')\n",
    "print(f'Length: {len(longest_entry[1])}')\n",
    "print(f'Content: {longest_entry[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb3fc4c-e7b9-4bfb-9340-779aaba31116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_tokens(text, max_tokens=40):\n",
    "    tokens = llm.tokenize(text.encode(\"utf-8\"))\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "    truncated = llm.detokenize(tokens[:max_tokens]).decode(\"utf-8\", errors=\"ignore\")\n",
    "    return truncated + \"...\"\n",
    "\n",
    "def max_afforded_tokens(codes):\n",
    "    count = len(codes)\n",
    "    m_tokens = int(max_context/count)\n",
    "    if m_tokens < 100:\n",
    "        m_tokens = 100\n",
    "    return m_tokens\n",
    "\n",
    "\n",
    "def translate_query(query, max_total_tokens=2048, max_output_tokens=256):\n",
    "    codes = extract_codes(query)\n",
    "\n",
    "    m_tokens = max_afforded_tokens(codes)\n",
    "\n",
    "    # Build full context blocks for each code\n",
    "    context_blocks = [\n",
    "        f\"{code}: {max_tokens(metadata_lookup.get(code, 'No metadata found.'), max_tokens=m_tokens)}\" for code in codes\n",
    "    ]\n",
    "\n",
    "    # Initial prompt pieces\n",
    "    instruction = \"### Instruction:\\nRephrase the following ESGish query into a concise natural English sentence. Each query is asking for all companies or issuers that match some paramater. Use the following metadata definitions for clarity:\\n\\n\"\n",
    "    query_part = f\"\\n\\nQuery: {query}\\n\\n### Response:\"\n",
    "\n",
    "    # Tokenize instruction and query to calculate remaining token budget\n",
    "    tokenizer = llm.tokenize  # Built-in tokenizer\n",
    "    instruction_tokens = len(tokenizer(instruction.encode(\"utf-8\")))\n",
    "    query_tokens = len(tokenizer(query_part.encode(\"utf-8\")))\n",
    "    token_budget = max_total_tokens - max_output_tokens - instruction_tokens - query_tokens\n",
    "\n",
    "    # Now iteratively add context blocks until budget is exhausted\n",
    "    context = \"\"\n",
    "    used_tokens = 0\n",
    "    for block in context_blocks:\n",
    "        block_tokens = len(tokenizer(block.encode(\"utf-8\"))) + 1  # +1 for newline\n",
    "        if used_tokens + block_tokens <= token_budget:\n",
    "            context += block + \"\\n\"\n",
    "            used_tokens += block_tokens\n",
    "        else:\n",
    "            break  # stop once we're out of budget\n",
    "\n",
    "    # Final prompt\n",
    "    prompt = instruction + context + query_part\n",
    "\n",
    "    # Call model\n",
    "    response = llm(prompt, max_tokens=max_output_tokens, temperature=0.1)\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81772e84-bbeb-4000-a351-281347bf87a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 50 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   26855.67 ms /   126 tokens (  213.14 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6790.93 ms /    44 runs   (  154.34 ms per token,     6.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    9882.38 ms /   170 tokens\n",
      "Llama.generate: 164 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     337.02 ms /    12 tokens (   28.09 ms per token,    35.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6845.04 ms /    44 runs   (  155.57 ms per token,     6.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    7189.78 ms /    56 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2706.70 ms /   116 tokens (   23.33 ms per token,    42.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5566.77 ms /    36 runs   (  154.63 ms per token,     6.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    8280.03 ms /   152 tokens\n",
      "Llama.generate: 55 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3383.34 ms /   136 tokens (   24.88 ms per token,    40.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2856.65 ms /    17 runs   (  168.04 ms per token,     5.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    6243.28 ms /   153 tokens\n",
      "Llama.generate: 183 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     214.34 ms /     8 tokens (   26.79 ms per token,    37.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2084.02 ms /    14 runs   (  148.86 ms per token,     6.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    2300.77 ms /    22 tokens\n",
      "Llama.generate: 185 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     255.51 ms /     9 tokens (   28.39 ms per token,    35.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2907.83 ms /    19 runs   (  153.04 ms per token,     6.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    3166.76 ms /    28 tokens\n",
      "Llama.generate: 188 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     231.40 ms /     8 tokens (   28.92 ms per token,    34.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4034.41 ms /    27 runs   (  149.42 ms per token,     6.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    4270.28 ms /    35 tokens\n",
      "Llama.generate: 183 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     191.59 ms /     7 tokens (   27.37 ms per token,    36.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2458.45 ms /    17 runs   (  144.61 ms per token,     6.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    2652.98 ms /    24 tokens\n",
      "Llama.generate: 55 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2928.15 ms /   130 tokens (   22.52 ms per token,    44.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3815.69 ms /    26 runs   (  146.76 ms per token,     6.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    6748.34 ms /   156 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2773.44 ms /   121 tokens (   22.92 ms per token,    43.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3067.76 ms /    20 runs   (  153.39 ms per token,     6.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    5844.95 ms /   141 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2744.36 ms /   108 tokens (   25.41 ms per token,    39.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6784.29 ms /    45 runs   (  150.76 ms per token,     6.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    9536.66 ms /   153 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1099.10 ms /    47 tokens (   23.39 ms per token,    42.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1606.58 ms /    11 runs   (  146.05 ms per token,     6.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    2707.79 ms /    58 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1822.77 ms /    79 tokens (   23.07 ms per token,    43.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4287.35 ms /    29 runs   (  147.84 ms per token,     6.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    6115.03 ms /   108 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3461.78 ms /   130 tokens (   26.63 ms per token,    37.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1962.73 ms /    12 runs   (  163.56 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    5426.79 ms /   142 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1774.31 ms /    60 tokens (   29.57 ms per token,    33.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2934.49 ms /    19 runs   (  154.45 ms per token,     6.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    4712.46 ms /    79 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     727.07 ms /    29 tokens (   25.07 ms per token,    39.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2324.89 ms /    15 runs   (  154.99 ms per token,     6.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    3054.94 ms /    44 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2509.08 ms /   105 tokens (   23.90 ms per token,    41.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4355.63 ms /    30 runs   (  145.19 ms per token,     6.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    6869.71 ms /   135 tokens\n",
      "Llama.generate: 143 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     296.66 ms /    12 tokens (   24.72 ms per token,    40.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3386.44 ms /    22 runs   (  153.93 ms per token,     6.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    3686.90 ms /    34 tokens\n",
      "Llama.generate: 147 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     193.63 ms /     7 tokens (   27.66 ms per token,    36.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3567.85 ms /    24 runs   (  148.66 ms per token,     6.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    3765.39 ms /    31 tokens\n",
      "Llama.generate: 143 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     277.89 ms /    11 tokens (   25.26 ms per token,    39.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3775.44 ms /    26 runs   (  145.21 ms per token,     6.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    4057.58 ms /    37 tokens\n",
      "Llama.generate: 54 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2270.65 ms /   100 tokens (   22.71 ms per token,    44.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3399.66 ms /    23 runs   (  147.81 ms per token,     6.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    5674.35 ms /   123 tokens\n",
      "Llama.generate: 143 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     294.28 ms /    12 tokens (   24.52 ms per token,    40.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2472.67 ms /    17 runs   (  145.45 ms per token,     6.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    2769.66 ms /    29 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2386.05 ms /   104 tokens (   22.94 ms per token,    43.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4799.69 ms /    32 runs   (  149.99 ms per token,     6.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    7191.41 ms /   136 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5282.58 ms /   227 tokens (   23.27 ms per token,    42.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2912.09 ms /    20 runs   (  145.60 ms per token,     6.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    8198.19 ms /   247 tokens\n",
      "Llama.generate: 273 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     280.67 ms /    11 tokens (   25.52 ms per token,    39.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4080.54 ms /    28 runs   (  145.73 ms per token,     6.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    4365.82 ms /    39 tokens\n",
      "Llama.generate: 271 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     215.78 ms /     8 tokens (   26.97 ms per token,    37.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1681.66 ms /    11 runs   (  152.88 ms per token,     6.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    1899.48 ms /    19 tokens\n",
      "Llama.generate: 273 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     277.35 ms /    11 tokens (   25.21 ms per token,    39.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4139.17 ms /    28 runs   (  147.83 ms per token,     6.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    4421.30 ms /    39 tokens\n",
      "Llama.generate: 274 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     193.91 ms /     7 tokens (   27.70 ms per token,    36.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3433.70 ms /    23 runs   (  149.29 ms per token,     6.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    3631.39 ms /    30 tokens\n",
      "Llama.generate: 271 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     194.61 ms /     7 tokens (   27.80 ms per token,    35.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2668.09 ms /    18 runs   (  148.23 ms per token,     6.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    2865.65 ms /    25 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1897.46 ms /    82 tokens (   23.14 ms per token,    43.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4334.16 ms /    29 runs   (  149.45 ms per token,     6.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    6236.52 ms /   111 tokens\n",
      "Llama.generate: 122 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     336.36 ms /    14 tokens (   24.03 ms per token,    41.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3528.57 ms /    24 runs   (  147.02 ms per token,     6.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    3868.85 ms /    38 tokens\n",
      "Llama.generate: 120 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1462.36 ms /    61 tokens (   23.97 ms per token,    41.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5569.20 ms /    37 runs   (  150.52 ms per token,     6.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    7037.97 ms /    98 tokens\n",
      "Llama.generate: 123 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1211.31 ms /    52 tokens (   23.29 ms per token,    42.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11629.33 ms /    79 runs   (  147.21 ms per token,     6.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   12855.22 ms /   131 tokens\n",
      "Llama.generate: 155 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     315.31 ms /    13 tokens (   24.25 ms per token,    41.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9565.34 ms /    65 runs   (  147.16 ms per token,     6.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    9892.44 ms /    78 tokens\n",
      "Llama.generate: 123 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     879.76 ms /    37 tokens (   23.78 ms per token,    42.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9103.91 ms /    62 runs   (  146.84 ms per token,     6.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    9994.67 ms /    99 tokens\n",
      "Llama.generate: 122 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     684.80 ms /    29 tokens (   23.61 ms per token,    42.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7679.02 ms /    50 runs   (  153.58 ms per token,     6.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    8373.17 ms /    79 tokens\n",
      "Llama.generate: 122 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     477.73 ms /    19 tokens (   25.14 ms per token,    39.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7178.16 ms /    48 runs   (  149.55 ms per token,     6.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    7664.43 ms /    67 tokens\n",
      "Llama.generate: 135 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     937.78 ms /    40 tokens (   23.44 ms per token,    42.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5339.69 ms /    36 runs   (  148.32 ms per token,     6.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    6283.47 ms /    76 tokens\n",
      "Llama.generate: 123 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     315.51 ms /    13 tokens (   24.27 ms per token,    41.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4234.48 ms /    28 runs   (  151.23 ms per token,     6.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    4554.86 ms /    41 tokens\n",
      "Llama.generate: 120 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     258.07 ms /     9 tokens (   28.67 ms per token,    34.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2529.91 ms /    17 runs   (  148.82 ms per token,     6.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    2790.84 ms /    26 tokens\n",
      "Llama.generate: 54 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1871.97 ms /    78 tokens (   24.00 ms per token,    41.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3257.97 ms /    22 runs   (  148.09 ms per token,     6.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    5133.60 ms /   100 tokens\n",
      "Llama.generate: 120 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     294.66 ms /    12 tokens (   24.55 ms per token,    40.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2904.42 ms /    19 runs   (  152.86 ms per token,     6.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    3202.42 ms /    31 tokens\n",
      "Llama.generate: 124 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     196.48 ms /     7 tokens (   28.07 ms per token,    35.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2972.44 ms /    20 runs   (  148.62 ms per token,     6.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    3172.36 ms /    27 tokens\n",
      "Llama.generate: 120 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     295.88 ms /    12 tokens (   24.66 ms per token,    40.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3206.74 ms /    22 runs   (  145.76 ms per token,     6.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    3506.57 ms /    34 tokens\n",
      "Llama.generate: 124 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     194.00 ms /     7 tokens (   27.71 ms per token,    36.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3189.72 ms /    21 runs   (  151.89 ms per token,     6.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    3387.28 ms /    28 tokens\n",
      "Llama.generate: 54 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1781.10 ms /    78 tokens (   22.83 ms per token,    43.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3921.91 ms /    27 runs   (  145.26 ms per token,     6.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    5707.60 ms /   105 tokens\n",
      "Llama.generate: 120 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     294.66 ms /    12 tokens (   24.55 ms per token,    40.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3692.79 ms /    25 runs   (  147.71 ms per token,     6.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    3991.49 ms /    37 tokens\n",
      "Llama.generate: 124 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     194.67 ms /     7 tokens (   27.81 ms per token,    35.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3666.74 ms /    25 runs   (  146.67 ms per token,     6.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    3865.61 ms /    32 tokens\n",
      "Llama.generate: 123 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     189.38 ms /     6 tokens (   31.56 ms per token,    31.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3504.23 ms /    24 runs   (  146.01 ms per token,     6.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    3697.76 ms /    30 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 1 to translated_queries_TEST.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "#Reads and stores the Esgish queries\n",
    "df = pd.read_excel(\"esgish_short.xlsx\")\n",
    "queries = df[\"Esgish\"].tolist()\n",
    "#Ensures no overload and efficiency\n",
    "batch_size = 100 \n",
    "output_file = \"translated_queries_TEST.xlsx\"\n",
    "\n",
    "#Looks at each query in each batch, calls the translate_query function, and stores it\n",
    "for i in range(0, len(queries), batch_size):\n",
    "    batch = queries[i:i + batch_size]\n",
    "    translated_batch = []\n",
    "    \n",
    "    for query in batch:\n",
    "        translated = translate_query(query)\n",
    "        translated_batch.append(translated)\n",
    "    \n",
    "    df_batch = pd.DataFrame({\n",
    "        \"Esgish\": batch,\n",
    "        \"English\": translated_batch\n",
    "    })\n",
    "\n",
    "    #Makes a new file if needed, or adds onto the current file during each batch in case the program crashes at some point\n",
    "    if i == 0:\n",
    "        df_batch.to_excel(output_file, index=False)  \n",
    "    else:\n",
    "        with pd.ExcelWriter(output_file, mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"overlay\") as writer:\n",
    "            df_batch.to_excel(writer, index=False, header=False, startrow=i + 1)\n",
    "    \n",
    "    print(f\"Saved batch {i // batch_size + 1} to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814e3a0-0ddd-467e-8074-18a33f13a0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f42872-6939-4c34-b74f-f177f8eb3d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
