{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11329b8f-ae0a-48d4-9da9-7c1c06875c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (3.1.5)\n",
      "Requirement already satisfied: transformers in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-cpp-python in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (2.2.3)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas openpyxl transformers torch\n",
    "%pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a607fdc-3c30-4006-af0d-60789c816fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/solar/OneDrive/Documents/Capstone/Capstone-Jupyter/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 4096\n",
      "llama_init_from_model: n_ctx_per_seq = 4096\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_init_from_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "model_path = \"/Users/solar/OneDrive/Documents/Capstone/Capstone-Jupyter/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "max_context = 4096\n",
    "llm = Llama(\n",
    "    model_path,\n",
    "    n_ctx=max_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4566bf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest entry code: EUTaxManIntSerElcRevOverAlign\n",
      "Length: 1353\n",
      "Content: EU Taxon - Man, Instal, Serv of Elec Equip Overall Align. EU Taxon - Man, Instal, Serv of Elec Equip Overall Align: This factor identifies the overall alignment for Manufacture, installation, and servicing of high, medium and low voltage electrical equipment for electrical transmission and distribution that result in or enable a substantial contribution to climate change mitigation, covering the substantial contribution criteria, the do no significant harm criteria, and the minimum social safeguards. This is the aggregated result across all Taxonomy objectives. The possible values are: Aligned,Aligned (>90%),Aligned (>80%),Aligned (>70%),Aligned (>60%),Aligned (>50%),Aligned (>40%),Aligned (>30%),Aligned (>20%),Aligned (>10%),Aligned (>0%),Likely aligned (100%),Likely aligned (>90%),Likely aligned (>80%),Likely aligned (>70%),Likely aligned (>60%),Likely aligned (>50%),Likely aligned (>40%),Likely aligned (>30%),Likely aligned (>20%),Likely aligned (>10%),Likely aligned (>0%),Potentially aligned (100%),Potentially aligned (>90%),Potentially aligned (>80%),Potentially aligned (>70%),Potentially aligned (>60%),Potentially aligned (>50%),Potentially aligned (>40%),Potentially aligned (>30%),Potentially aligned (>20%),Potentially aligned (>10%),Potentially aligned (>0%),Likely not aligned,Not aligned, Not collected, and Not applicable.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open(\"metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_metadata = json.load(f)\n",
    "\n",
    "# Create a lookup table\n",
    "metadata_lookup = {\n",
    "    entry[\"code\"]: f'{entry[\"name\"]}. {entry[\"description\"]}' \n",
    "    for entry in raw_metadata\n",
    "}\n",
    "\n",
    "def extract_codes(query):\n",
    "    return re.findall(r\"\\[([A-Za-z0-9_]+)\\]\", query)\n",
    "\n",
    "list_keywords = {\"IN\", \"ANY\", \"NONE\"}\n",
    "null_keywords = {\":NC\": \"Null type \\\"Not collected\\\"\", \":NA\": \"Null type \\\"Not applicable\\\"\", \":ND\": \"Null type \\\"Not disclosed\\\"\", \":NI\": \"Null type \\\"No information\\\"\", \":NM\": \"Null type \\\"Not meaningful\\\"\"}\n",
    "def extract_nulls_and_lists(query):\n",
    "    nulls = []\n",
    "    lists = []\n",
    "    contains_null = re.findall(r\"\\[([A-Za-z0-9_]+)\\]\\s+IS\\s+(:[A-Z]{2})\", query)\n",
    "    for code, null_keyword in contains_null:\n",
    "        nulls.append((code, null_keyword))\n",
    "    \n",
    "    contains_list = re.findall(r\"\\[([A-Za-z0-9_]+)\\]\\s+(IN|ANY|NONE)\\b\", query, re.IGNORECASE)\n",
    "    for code, list_keyword in contains_list:\n",
    "        lists.append((code, list_keyword.upper()))\n",
    "    \n",
    "    return nulls, lists\n",
    "\n",
    "# Find the entry with the longest name + description combo\n",
    "longest_entry = max(metadata_lookup.items(), key=lambda item: len(item[1]))\n",
    "\n",
    "# Print the result\n",
    "print(f'Longest entry code: {longest_entry[0]}')\n",
    "print(f'Length: {len(longest_entry[1])}')\n",
    "print(f'Content: {longest_entry[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efb3fc4c-e7b9-4bfb-9340-779aaba31116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_tokens(text, max_tokens=40):\n",
    "    #moved this to use in below function\n",
    "    tokens = llm.tokenize(text.encode(\"utf-8\"))\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "    truncated = llm.detokenize(tokens[:max_tokens]).decode(\"utf-8\", errors=\"ignore\")\n",
    "    return truncated + \"...\"\n",
    "\n",
    "def build_ordered_context(query, token_budget):\n",
    "    #builds context in-order: null type definitions, enumerations for list types, and metadata lookups\n",
    "    context_lines = []\n",
    "    seen = set()\n",
    "    used_tokens = 0\n",
    "\n",
    "    null_hits, list_hits = extract_nulls_and_lists(query)\n",
    "    codes_in_order = re.findall(r\"\\[([A-Za-z0-9_]+)\\]\", query)\n",
    "\n",
    "    #tokenizer = llm.tokenizer\n",
    "    for code in codes_in_order:\n",
    "        if code in seen:\n",
    "            continue\n",
    "        seen.add(code)\n",
    "\n",
    "        null_entry = next((kw for c, kw in null_hits if c == code), None)\n",
    "        if null_entry:\n",
    "            null_def = null_keywords.get(null_entry, f\"No definition for {null_entry}\")\n",
    "            line = f\"{code} = {null_entry} → {null_def}\"\n",
    "            tokens = len(llm.tokenize(line.encode(\"utf-8\")))\n",
    "            if used_tokens + tokens > token_budget:\n",
    "                break\n",
    "            context_lines.append(line)\n",
    "            used_tokens += tokens\n",
    "\n",
    "        base_meta = metadata_lookup.get(code, \"No metadata found.\")\n",
    "        metadata_line = f\"{code}: {max_tokens(base_meta, 100)}\" \n",
    "        tokens = len(llm.tokenize(metadata_line.encode(\"utf-8\")))\n",
    "        if used_tokens + tokens > token_budget:\n",
    "            break\n",
    "        context_lines.append(metadata_line)\n",
    "        used_tokens += tokens\n",
    "\n",
    "        if any(c == code for c, _ in list_hits):\n",
    "            enum_line = f\"{code} (enumeration): {max_tokens(base_meta, 100)}\"\n",
    "            tokens = len(llm.tokenize(enum_line.encode(\"utf-8\")))\n",
    "            if used_tokens + tokens > token_budget:\n",
    "                break\n",
    "            context_lines.append(enum_line)\n",
    "            used_tokens += tokens\n",
    "\n",
    "    return \"\\n\".join(context_lines)\n",
    "\n",
    "def max_afforded_tokens(codes):\n",
    "    return max(4096 // max(1, len(codes)), 100)\n",
    "\n",
    "\n",
    "def translate_query(query, max_total_tokens=2048, max_output_tokens=256):\n",
    "    codes = extract_codes(query)\n",
    "\n",
    "    m_tokens = max_afforded_tokens(codes)\n",
    "\n",
    "    # Initial prompt pieces\n",
    "    instruction = \"### Instruction:\\nRephrase the following ESGish query into a concise natural English sentence. Each query is asking for all companies or issuers that match some paramater. Use the following metadata definitions for clarity:\\n\\n\"\n",
    "    query_part = f\"\\n\\nQuery: {query}\\n\\n### Response:\"\n",
    "\n",
    "    # Tokenize instruction and query to calculate remaining token budget\n",
    "    #tokenizer = llm.tokenize  # Built-in tokenizer\n",
    "    instruction_tokens = len(llm.tokenize(instruction.encode(\"utf-8\")))\n",
    "    query_tokens = len(llm.tokenize(query_part.encode(\"utf-8\")))\n",
    "    token_budget = max_total_tokens - max_output_tokens - instruction_tokens - query_tokens\n",
    "\n",
    "    # Build full context blocks for each code\n",
    "    context = build_ordered_context(query, token_budget)\n",
    "    print(\"query: \", query)\n",
    "    print(context)\n",
    "\n",
    "    \"\"\"\n",
    "    # Now iteratively add context blocks until budget is exhausted\n",
    "    context = \"\"\n",
    "    used_tokens = 0\n",
    "    for block in context_blocks:\n",
    "        block_tokens = len(tokenizer(block.encode(\"utf-8\"))) + 1  # +1 for newline\n",
    "        if used_tokens + block_tokens <= token_budget:\n",
    "            context += block + \"\\n\"\n",
    "            used_tokens += block_tokens\n",
    "        else:\n",
    "            break  # stop once we're out of budget\n",
    "    \"\"\"\n",
    "    # Final prompt\n",
    "    prompt = instruction + context + query_part\n",
    "\n",
    "    # Call model\n",
    "    response = llm(prompt, max_tokens=max_output_tokens, temperature=0.1)\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81772e84-bbeb-4000-a351-281347bf87a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  AND([ClawBackProvisionDisclosure] IS :ND,[CEOStockOwnershipGuideRatio] > '1')\n",
      "ClawBackProvisionDisclosure = :ND → Null type \"Not disclosed\"\n",
      "ClawBackProvisionDisclosure: Q155: Clawback Provision. Does the company have a clawback or malus provision? (Q155)\n",
      "CEOStockOwnershipGuideRatio: Q145: CEO Stock Ownership Guidelines (% of Salary). What salary multiple of the CEO is subject to stock ownership guidelines? (Q145)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    4865.43 ms /   191 tokens (   25.47 ms per token,    39.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5296.44 ms /    33 runs   (  160.50 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =   10167.99 ms /   224 tokens\n",
      "Llama.generate: 190 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  AND([ClawBackProvisionDisclosure] IS :ND,[CEOStockOwnershipGuideRatio] > '1')\n",
      "ClawBackProvisionDisclosure = :ND → Null type \"Not disclosed\"\n",
      "ClawBackProvisionDisclosure: Q155: Clawback Provision. Does the company have a clawback or malus provision? (Q155)\n",
      "CEOStockOwnershipGuideRatio: Q145: CEO Stock Ownership Guidelines (% of Salary). What salary multiple of the CEO is subject to stock ownership guidelines? (Q145)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    5488.63 ms /    34 runs   (  161.43 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    5494.40 ms /    35 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 169 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  AND([ClimateEmissionsFiscalYear] == '2022',[ClimateParentEntityID] IS :NA)\n",
      "ClimateEmissionsFiscalYear: GHG Emissions - Fiscal Year. GHG Emissions - Fiscal Year: This factor provides the fiscal year associated with the corresponding GHG emissions values.\n",
      "ClimateParentEntityID = :NA → Null type \"Not applicable\"\n",
      "ClimateParentEntityID: Climate Parent Entity ID. Climate Parent Entity ID: This factor provides the identifier of the issuer‘s parent, from which the climate data is assigned. The climate data from the parent company is assigned to a subsidiary based on criteria such as industry classification, ownership structure, and business activities.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    4074.35 ms /   169 tokens (   24.11 ms per token,    41.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4719.95 ms /    30 runs   (  157.33 ms per token,     6.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    8799.89 ms /   199 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 1177 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  AND([FossilFuelCoalPowerRevShareMin] IS :NC,[CoalMiningServMinRev] IS :NC,[CivFADistMinRev] IS :NC,[MilitaryEqmtProdServMinRev] IS :NC,[APMinesTotalReds] IS :NC,[BiologicalWeaponsTotalReds] IS :NC,[ChemicalWeaponsTotalReds] IS :NC,[ClusterMunitionsTotalReds] IS :NC,[IncendiaryWeaponsTotalReds] IS :NC,[NBSOverallScore] IS :NC,[CCAAuthoritarianRegimeFreedom] IS :NC)\n",
      "FossilFuelCoalPowerRevShareMin = :NC → Null type \"Not collected\"\n",
      "FossilFuelCoalPowerRevShareMin: Fossil Fuel - Coal Power Minimum Revenue Share (%). Fossil Fuel - Coal Power Minimum Revenue Share (%): This factor provides the minimum percentage of recent-year revenues for the company's involvement in the generation of electric power sourced from coal.\n",
      "CoalMiningServMinRev = :NC → Null type \"Not collected\"\n",
      "CoalMiningServMinRev: Coal Mining - Services Minimum Percentage of Revenues (%). Coal Mining - Services Minimum Percentage of Revenues (%):  This factor provides the minimum percentage of revenues for the company's involvement in the provision of services for the extraction/mining of coal for the most recent fiscal year period.\n",
      "CivFADistMinRev = :NC → Null type \"Not collected\"\n",
      "CivFADistMinRev: Civilian Firearms - Distribution Revenue Share Min (%). Civilian Firearms - Distribution Revenue Share Min (%): This factor identifies the minimum share of revenue estimated or reported to be derived from involvement in the distribution of civilian firearms as a percentage of the issuers’ total revenue in the latest financial year.\n",
      "MilitaryEqmtProdServMinRev = :NC → Null type \"Not collected\"\n",
      "MilitaryEqmtProdServMinRev: Military Equipment and Services - Prod & Serv Rev Share Min (%). Military Equipment and Services - Prod & Serv Rev Share Min (%): This factor identifies the minimum share of revenue estimated or reported to be derived from involvement in the production of military equipment and/or the provision of related services as a percentage of the issuers’ total revenue in the latest financial year.\n",
      "APMinesTotalReds = :NC → Null type \"Not collected\"\n",
      "APMinesTotalReds: Anti-personnel Mines - Total Red Assessments. Anti-personnel Mines - Total Red Assessments: This factor provides the number of Red signals assigned to an issuer based on the issuer's involvement in any anti-personnel mines programme\n",
      "BiologicalWeaponsTotalReds = :NC → Null type \"Not collected\"\n",
      "BiologicalWeaponsTotalReds: Biological Weapons - Total Red Assessments. Biological Weapons - Total Red Assessments: This factor provides the number of Red signals assigned to an issuer based on the issuer's involvement in any biological weapons programme\n",
      "ChemicalWeaponsTotalReds = :NC → Null type \"Not collected\"\n",
      "ChemicalWeaponsTotalReds: Chemical Weapons - Total Red Assessments. Chemical Weapons - Total Red Assessments: This factor provides the number of Red signals assigned to an issuer based on the issuer's involvement in any chemical weapons programme\n",
      "ClusterMunitionsTotalReds = :NC → Null type \"Not collected\"\n",
      "ClusterMunitionsTotalReds: Cluster Munitions - Total Red Assessments. Cluster Munitions - Total Red Assessments: This factor provides the number of Red signals assigned to an issuer based on the issuer's involvement in any cluster munitions programme\n",
      "IncendiaryWeaponsTotalReds = :NC → Null type \"Not collected\"\n",
      "IncendiaryWeaponsTotalReds: Incendiary Weapons - Total Red Assessments. Incendiary Weapons - Total Red Assessments: This factor provides the number of Red signals assigned to an issuer based on the issuer's involvement in any incendiary weapons programme\n",
      "NBSOverallScore = :NC → Null type \"Not collected\"\n",
      "NBSOverallScore: NBR - Overall Score. NBR Overall Score: This factor assigns a 1-10 score to an issuer based on the issuer's link with any violations of international standards. Score 10 aligns with a Red flag; scores from 6 to 9 align with the Amber flag; scores from 2 to 5 reflect an \"active\" Green flag; and a score of 1 communicates a Green \"No allegation\" signal.\n",
      "CCAAuthoritarianRegimeFreedom = :NC → Null type \"Not collected\"\n",
      "CCAAuthoritarianRegimeFreedom: CCA - Authoritarian Regime / Freedom Status. CCA - Authoritarian Regime / Freedom Status: This factor identifies whether the country is considered a free, partly free or authoritarian society, per Freedom House. The options are \"Free\", \"Party free\", and \"Not free\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =   29805.58 ms /  1177 tokens (   25.32 ms per token,    39.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2491.12 ms /    15 runs   (  166.07 ms per token,     6.02 tokens per second)\n",
      "llama_perf_context_print:       total time =   32300.33 ms /  1192 tokens\n",
      "Llama.generate: 55 prefix-match hit, remaining 384 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  AND([FossilFuelInvolvement] ANY 'Exploration',[FundESGRatingStars] IS :NC)\n",
      "FossilFuelInvolvement: Fossil Fuel - Involvement Tie. Fossil Fuel - Involvement Tie: This factor identifies issuers engaged in the production, distribution, exploration or provision of services related to fossil fuels. The \"Production\" value identifies issuers engaged in the production of fossil fuels through extraction, processing, and electricity generation. \"Distribution\" includes issuers engaged in essential infrastructure specifically used for the transportation of foss...\n",
      "FossilFuelInvolvement (enumeration): Fossil Fuel - Involvement Tie. Fossil Fuel - Involvement Tie: This factor identifies issuers engaged in the production, distribution, exploration or provision of services related to fossil fuels. The \"Production\" value identifies issuers engaged in the production of fossil fuels through extraction, processing, and electricity generation. \"Distribution\" includes issuers engaged in essential infrastructure specifically used for the transportation of foss...\n",
      "FundESGRatingStars = :NC → Null type \"Not collected\"\n",
      "FundESGRatingStars: ISS ESG Fund Rating Stars. ISS ESG Fund Rating Stars: This factor provides the fund's overall star rating based on the fund's relative performance in comparison to peer funds in the same Lipper Global Classification class. The rating is derived from a weighted average ESG Performance Score which evaluates issuers across environmental, social, and governance performance metrics. The overall rating is graded on a scale from 1 (worst) to 5 (...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    9398.96 ms /   384 tokens (   24.48 ms per token,    40.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5496.83 ms /    35 runs   (  157.05 ms per token,     6.37 tokens per second)\n",
      "llama_perf_context_print:       total time =   14902.33 ms /   419 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 212 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  AND([FundESGRatingStars] IS :NC,OR([CountryOfIncorporation] == 'Malaysia',[StockExchange] == 'Malaysia Stock Exchange'))\n",
      "FundESGRatingStars = :NC → Null type \"Not collected\"\n",
      "FundESGRatingStars: ISS ESG Fund Rating Stars. ISS ESG Fund Rating Stars: This factor provides the fund's overall star rating based on the fund's relative performance in comparison to peer funds in the same Lipper Global Classification class. The rating is derived from a weighted average ESG Performance Score which evaluates issuers across environmental, social, and governance performance metrics. The overall rating is graded on a scale from 1 (worst) to 5 (...\n",
      "CountryOfIncorporation: Country Of Incorporation. Country issuer is incorporated in\n",
      "StockExchange: Stock Exchange. Stock exchange for primary security\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    5147.30 ms /   212 tokens (   24.28 ms per token,    41.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5270.53 ms /    33 runs   (  159.71 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =   10423.78 ms /   245 tokens\n",
      "Llama.generate: 198 prefix-match hit, remaining 154 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  AND([FundESGRatingStars] IS :NC,OR([CountryOfIncorporation] IN 'Brazil|Egypt|New Zealand|Singapore|Switzerland|United Kingdom',[CountryOfIncorporation] IN 'Australia|Canada|Switzerland|USA',[IssuerRegion] == 'Europe'))\n",
      "FundESGRatingStars = :NC → Null type \"Not collected\"\n",
      "FundESGRatingStars: ISS ESG Fund Rating Stars. ISS ESG Fund Rating Stars: This factor provides the fund's overall star rating based on the fund's relative performance in comparison to peer funds in the same Lipper Global Classification class. The rating is derived from a weighted average ESG Performance Score which evaluates issuers across environmental, social, and governance performance metrics. The overall rating is graded on a scale from 1 (worst) to 5 (...\n",
      "CountryOfIncorporation: Country Of Incorporation. Country issuer is incorporated in\n",
      "CountryOfIncorporation (enumeration): Country Of Incorporation. Country issuer is incorporated in\n",
      "IssuerRegion: IssuerRegion. Region: This factor provides the issuer's reference region as assigned to the following values: Africa, Asia, Europe, Latin America, Middle East, North America, Pacific.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    3816.56 ms /   154 tokens (   24.78 ms per token,    40.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10914.54 ms /    69 runs   (  158.18 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =   14744.14 ms /   223 tokens\n",
      "Llama.generate: 177 prefix-match hit, remaining 1343 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  AND([FundESGRatingStars] IS :NC,OR([CRARating] IN 'Needs to Improve',[NBSIsLabourDiscriminationGender] True,[NBSIsLabourDiscriminationRacial] True,[APMinesOverallFlag] == 'RED',[NuclearWeaponsOverallFlag] == 'RED',[NuclearWeaponsNonNPTOverallFlag] == 'RED',[ChemicalWeaponsOverallFlag] == 'RED',[BiologicalWeaponsOverallFlag] == 'RED',[PornographyRevShareMax] > '0',[TobaccoRevShareMax] > '0',[AlcoholRevShareMax] > '0',[GamblingRevShareMax] > '0'))\n",
      "FundESGRatingStars = :NC → Null type \"Not collected\"\n",
      "FundESGRatingStars: ISS ESG Fund Rating Stars. ISS ESG Fund Rating Stars: This factor provides the fund's overall star rating based on the fund's relative performance in comparison to peer funds in the same Lipper Global Classification class. The rating is derived from a weighted average ESG Performance Score which evaluates issuers across environmental, social, and governance performance metrics. The overall rating is graded on a scale from 1 (worst) to 5 (...\n",
      "CRARating: Community Reinvestment Act - CRA Rating. Community Reinvestment Act - CRA Rating: This factor identifies issuers that have a \"Needs to Improve\" or \"Substantial Noncompliance\" rating under the U.S. Community Reinvestment Act\n",
      "CRARating (enumeration): Community Reinvestment Act - CRA Rating. Community Reinvestment Act - CRA Rating: This factor identifies issuers that have a \"Needs to Improve\" or \"Substantial Noncompliance\" rating under the U.S. Community Reinvestment Act\n",
      "NBSIsLabourDiscriminationGender: Norm-Based Research Assessment - Gender Discrimination. NBR Assessment - Gender Discrimination: This factor identifies companies involved in one or more workplace gender discrimination controversies where there is verification by an authoritative body but the issue remains unaddressed; where the issuer has entered contract(s) that would, when actualised, lead to a failure to respect established norms; where there are credible allegations; where the failure to respect established norms...\n",
      "NBSIsLabourDiscriminationRacial: Norm-Based Research Assessment - Racial Discrimination. NBR Assessment - Racial Discrimination: This factor identifies companies involved in one or more workplace racial discrimination controversies where there is verification by an authoritative body but the issue remains unaddressed; where the issuer has entered contract(s) that would, when actualised, lead to a failure to respect established norms; where there are credible allegations; where the failure to respect established...\n",
      "APMinesOverallFlag: Anti-personnel Mines - Overall Flag. Anti-personnel Mines - Overall Flag: This factor assigns an overall Red, Amber, or Green flag to an issuer based on the issuer's involvement in any anti-personnel mines programme. The Overall Flag is determined by the lowest individual assessment signal within the issue area. For example, if the issuer is assigned both a Red signal and an Amber signal for different assessments in this issue area, the...\n",
      "NuclearWeaponsOverallFlag: Nuclear Weapons Inside NPT - Overall Flag. Nuclear Weapons Inside NPT - Overall Flag: This factor assigns an overall Red, Amber, or Green flag to an issuer based on the issuer's involvement in any nuclear weapons programme inside of the Non-Proliferation Treaty (NPT). The Overall Flag is determined by the lowest individual assessment signal within the issue area. For example, if the issuer is assigned both a Red...\n",
      "NuclearWeaponsNonNPTOverallFlag: Nuclear Weapons Outside NPT - Overall Flag. Nuclear Weapons Outside NPT - Overall Flag: This factor assigns an overall Red, Amber, or Green flag to an issuer based on the issuer's involvement in any nuclear weapons programme outside of the Non-Proliferation Treaty (NPT). The Overall Flag is determined by the lowest individual assessment signal within the issue area. For example, if the issuer is assigned both a Red...\n",
      "ChemicalWeaponsOverallFlag: Chemical Weapons - Overall Flag. Chemical Weapons - Overall Flag: This factor assigns an overall Red, Amber, or Green flag to an issuer based on the issuer's involvement in any chemical weapons programme. The Overall Flag is determined by the lowest individual assessment signal within the issue area. For example, if the issuer is assigned both a Red signal and an Amber signal for different assessments in this issue area, the issuer's Overall...\n",
      "BiologicalWeaponsOverallFlag: Biological Weapons - Overall Flag. Biological Weapons - Overall Flag: This factor assigns an overall Red, Amber, or Green flag to an issuer based on the issuer's involvement in any biological weapons programme. The Overall Flag is determined by the lowest individual assessment signal within the issue area. For example, if the issuer is assigned both a Red signal and an Amber signal for different assessments in this issue area, the issuer's Overall...\n",
      "PornographyRevShareMax: Pornography - Revenue Share Max (%). Pornography - Revenue Share Max (%): This factor identifies the maximum share of revenue estimated or reported to be derived from pornography as a percentage of the issuers’ total revenue in the latest financial year.\n",
      "TobaccoRevShareMax: Tobacco - Revenue Share Max (%). Tobacco - Revenue Share Max (%): This factor identifies the maximum share of revenue estimated or reported to be derived from tobacco as a percentage of the issuers’ total revenue in the latest financial year.\n",
      "AlcoholRevShareMax: Alcohol - Revenue Share Max (%). Alcohol - Revenue Share Max (%)This factor identifies the maximum share of revenue estimated or reported to be derived from alcohol as a percentage of the issuers’ total revenue in the latest financial year.\n",
      "GamblingRevShareMax: Gambling - Revenue Share Max (%). Gambling - Revenue Share Max (%)This factor identifies the maximum share of revenue estimated or reported to be derived from gambling as a percentage of the issuers’ total revenue in the latest financial year.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =   34629.83 ms /  1343 tokens (   25.79 ms per token,    38.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20064.40 ms /   120 runs   (  167.20 ms per token,     5.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   54722.02 ms /  1463 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 251 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  OR(AND([PornographyNotAccessibeToMinors] IS :NA,[PornographyDistMinRev] IS :NA),AND([PornographyDistMinRev] <= '0.02',[PornographyNotAccessibeToMinors] False))\n",
      "PornographyNotAccessibeToMinors = :NA → Null type \"Not applicable\"\n",
      "PornographyNotAccessibeToMinors: Pornography - Not accessible to minors. Pornography - Not accessible to minors: This factor identifies pornographic materials which are specifically marked as not accessible to minors or are prohibited/restricted for those persons below 18 years of age.\n",
      "PornographyDistMinRev = :NA → Null type \"Not applicable\"\n",
      "PornographyDistMinRev: Pornography - Distribution Minimum Percentage of Revenues (%). Pornography - Distribution Minimum Percentage of Revenues (%): This factor identifies the minimum share of revenue estimated or reported to be derived from involvement in the distribution of pornography as a percentage of the issuers’ total revenue in the latest financial year.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    6071.49 ms /   251 tokens (   24.19 ms per token,    41.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15037.55 ms /    95 runs   (  158.29 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =   21128.50 ms /   346 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 273 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  OR([TobaccoInvolvement] ANY 'Distribution|Services',[TobaccoInvolvement] IS :NC)\n",
      "TobaccoInvolvement = :NC → Null type \"Not collected\"\n",
      "TobaccoInvolvement: Tobacco - Involvement Tie. Tobacco - Involvement Tie: This factor identifies issuers engaged in the production, distribution, or provision of services related to tobacco. The \"Production\" value identifies issuers engaged in manufacturing and producing tobacco products, as well as companies that grow or process raw tobacco leaves, \"Distribution\" includes issuers engaged in the wholesale or retail distribution of tobacco products, and \"Services\"  identifies iss...\n",
      "TobaccoInvolvement (enumeration): Tobacco - Involvement Tie. Tobacco - Involvement Tie: This factor identifies issuers engaged in the production, distribution, or provision of services related to tobacco. The \"Production\" value identifies issuers engaged in manufacturing and producing tobacco products, as well as companies that grow or process raw tobacco leaves, \"Distribution\" includes issuers engaged in the wholesale or retail distribution of tobacco products, and \"Services\"  identifies iss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    6650.51 ms /   273 tokens (   24.36 ms per token,    41.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5861.69 ms /    37 runs   (  158.42 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =   12518.54 ms /   310 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 1 to translated_queries_TEST.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "#Reads and stores the Esgish queries\n",
    "df = pd.read_excel(\"NullType_10Queries.xlsx\")\n",
    "queries = df[\"Esgish\"].tolist()\n",
    "\n",
    "#Ensures no overload and efficiency\n",
    "batch_size = 100 \n",
    "output_file = \"translated_queries_TEST.xlsx\"\n",
    "\n",
    "#Looks at each query in each batch, calls the translate_query function, and stores it\n",
    "for i in range(0, len(queries), batch_size):\n",
    "    batch = queries[i:i + batch_size]\n",
    "    translated_batch = []\n",
    "    \n",
    "    for query in batch:\n",
    "        translated = translate_query(query)\n",
    "        translated_batch.append(translated)\n",
    "    \n",
    "    df_batch = pd.DataFrame({\n",
    "        \"Esgish\": batch,\n",
    "        \"English\": translated_batch\n",
    "    })\n",
    "\n",
    "    #Makes a new file if needed, or adds onto the current file during each batch in case the program crashes at some point\n",
    "    if i == 0:\n",
    "        df_batch.to_excel(output_file, index=False)  \n",
    "    else:\n",
    "        with pd.ExcelWriter(output_file, mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"overlay\") as writer:\n",
    "            df_batch.to_excel(writer, index=False, header=False, startrow=i + 1)\n",
    "    \n",
    "    print(f\"Saved batch {i // batch_size + 1} to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814e3a0-0ddd-467e-8074-18a33f13a0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 63 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1261.82 ms /    48 tokens (   26.29 ms per token,    38.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =     316.26 ms /     2 runs   (  158.13 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    1579.26 ms /    50 tokens\n",
      "Llama.generate: 90 prefix-match hit, remaining 20 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     556.81 ms /    20 tokens (   27.84 ms per token,    35.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =     391.39 ms /     2 runs   (  195.70 ms per token,     5.11 tokens per second)\n",
      "llama_perf_context_print:       total time =     948.92 ms /    22 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 44 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1127.64 ms /    44 tokens (   25.63 ms per token,    39.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =     314.37 ms /     2 runs   (  157.18 ms per token,     6.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1442.72 ms /    46 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     535.73 ms /    21 tokens (   25.51 ms per token,    39.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =     325.04 ms /     2 runs   (  162.52 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =     861.34 ms /    23 tokens\n",
      "Llama.generate: 70 prefix-match hit, remaining 11 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     306.40 ms /    11 tokens (   27.85 ms per token,    35.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =     335.62 ms /     2 runs   (  167.81 ms per token,     5.96 tokens per second)\n",
      "llama_perf_context_print:       total time =     642.59 ms /    13 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 28 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     690.85 ms /    28 tokens (   24.67 ms per token,    40.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =     317.89 ms /     2 runs   (  158.95 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1009.60 ms /    30 tokens\n",
      "Llama.generate: 74 prefix-match hit, remaining 25 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     652.52 ms /    25 tokens (   26.10 ms per token,    38.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =     318.26 ms /     2 runs   (  159.13 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =     971.45 ms /    27 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     570.20 ms /    21 tokens (   27.15 ms per token,    36.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =     330.42 ms /     2 runs   (  165.21 ms per token,     6.05 tokens per second)\n",
      "llama_perf_context_print:       total time =     901.43 ms /    23 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 27 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     790.65 ms /    27 tokens (   29.28 ms per token,    34.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =     351.92 ms /     2 runs   (  175.96 ms per token,     5.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1143.40 ms /    29 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 18 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     563.20 ms /    18 tokens (   31.29 ms per token,    31.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =     354.69 ms /     2 runs   (  177.34 ms per token,     5.64 tokens per second)\n",
      "llama_perf_context_print:       total time =     918.75 ms /    20 tokens\n",
      "Llama.generate: 70 prefix-match hit, remaining 42 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1111.79 ms /    42 tokens (   26.47 ms per token,    37.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =     341.66 ms /     2 runs   (  170.83 ms per token,     5.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1454.59 ms /    44 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 13 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     355.02 ms /    13 tokens (   27.31 ms per token,    36.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =     334.75 ms /     2 runs   (  167.38 ms per token,     5.97 tokens per second)\n",
      "llama_perf_context_print:       total time =     690.49 ms /    15 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 31 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     789.86 ms /    31 tokens (   25.48 ms per token,    39.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =     309.30 ms /     2 runs   (  154.65 ms per token,     6.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1100.03 ms /    33 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 14 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     371.45 ms /    14 tokens (   26.53 ms per token,    37.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =     359.28 ms /     2 runs   (  179.64 ms per token,     5.57 tokens per second)\n",
      "llama_perf_context_print:       total time =     731.33 ms /    16 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     566.32 ms /    21 tokens (   26.97 ms per token,    37.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =     318.20 ms /     2 runs   (  159.10 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =     885.40 ms /    23 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 14 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     373.90 ms /    14 tokens (   26.71 ms per token,    37.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =     315.26 ms /     2 runs   (  157.63 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =     689.78 ms /    16 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 36 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     913.46 ms /    36 tokens (   25.37 ms per token,    39.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =     326.96 ms /     2 runs   (  163.48 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    1241.18 ms /    38 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 25 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     702.59 ms /    25 tokens (   28.10 ms per token,    35.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =     330.89 ms /     2 runs   (  165.45 ms per token,     6.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1034.56 ms /    27 tokens\n",
      "Llama.generate: 78 prefix-match hit, remaining 13 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     343.37 ms /    13 tokens (   26.41 ms per token,    37.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =     323.62 ms /     2 runs   (  161.81 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =     667.56 ms /    15 tokens\n",
      "Llama.generate: 74 prefix-match hit, remaining 13 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     351.41 ms /    13 tokens (   27.03 ms per token,    36.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     320.26 ms /     2 runs   (  160.13 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =     672.44 ms /    15 tokens\n",
      "Llama.generate: 67 prefix-match hit, remaining 23 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     605.37 ms /    23 tokens (   26.32 ms per token,    37.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     328.97 ms /     2 runs   (  164.48 ms per token,     6.08 tokens per second)\n",
      "llama_perf_context_print:       total time =     934.97 ms /    25 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 24 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     683.31 ms /    24 tokens (   28.47 ms per token,    35.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =     334.49 ms /     2 runs   (  167.24 ms per token,     5.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    1018.63 ms /    26 tokens\n",
      "Llama.generate: 72 prefix-match hit, remaining 27 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     738.02 ms /    27 tokens (   27.33 ms per token,    36.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =     318.00 ms /     2 runs   (  159.00 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1056.89 ms /    29 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 18 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     496.49 ms /    18 tokens (   27.58 ms per token,    36.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =     325.24 ms /     2 runs   (  162.62 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =     822.24 ms /    20 tokens\n",
      "Llama.generate: 74 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     552.25 ms /    21 tokens (   26.30 ms per token,    38.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =     344.58 ms /     2 runs   (  172.29 ms per token,     5.80 tokens per second)\n",
      "llama_perf_context_print:       total time =     897.58 ms /    23 tokens\n",
      "Llama.generate: 75 prefix-match hit, remaining 35 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     903.37 ms /    35 tokens (   25.81 ms per token,    38.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =     313.75 ms /     2 runs   (  156.88 ms per token,     6.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1218.05 ms /    37 tokens\n",
      "Llama.generate: 75 prefix-match hit, remaining 20 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     529.03 ms /    20 tokens (   26.45 ms per token,    37.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =     318.10 ms /     2 runs   (  159.05 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =     848.08 ms /    22 tokens\n",
      "Llama.generate: 75 prefix-match hit, remaining 15 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     397.25 ms /    15 tokens (   26.48 ms per token,    37.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =     307.08 ms /     2 runs   (  153.54 ms per token,     6.51 tokens per second)\n",
      "llama_perf_context_print:       total time =     704.86 ms /    17 tokens\n",
      "Llama.generate: 70 prefix-match hit, remaining 15 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     418.38 ms /    15 tokens (   27.89 ms per token,    35.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =     313.89 ms /     2 runs   (  156.94 ms per token,     6.37 tokens per second)\n",
      "llama_perf_context_print:       total time =     732.98 ms /    17 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 27 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     669.98 ms /    27 tokens (   24.81 ms per token,    40.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =     328.45 ms /     2 runs   (  164.23 ms per token,     6.09 tokens per second)\n",
      "llama_perf_context_print:       total time =     999.23 ms /    29 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 14 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     365.96 ms /    14 tokens (   26.14 ms per token,    38.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =     315.53 ms /     2 runs   (  157.77 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =     682.57 ms /    16 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 33 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     862.49 ms /    33 tokens (   26.14 ms per token,    38.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =     310.07 ms /     2 runs   (  155.03 ms per token,     6.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1173.33 ms /    35 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 72 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1756.35 ms /    72 tokens (   24.39 ms per token,    40.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     312.86 ms /     2 runs   (  156.43 ms per token,     6.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    2069.94 ms /    74 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 63 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1540.33 ms /    63 tokens (   24.45 ms per token,    40.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =     314.64 ms /     2 runs   (  157.32 ms per token,     6.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1855.75 ms /    65 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 41 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1000.78 ms /    41 tokens (   24.41 ms per token,    40.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =     313.29 ms /     2 runs   (  156.65 ms per token,     6.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    1314.85 ms /    43 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 48 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1208.46 ms /    48 tokens (   25.18 ms per token,    39.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =     301.12 ms /     2 runs   (  150.56 ms per token,     6.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1510.29 ms /    50 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 46 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1130.37 ms /    46 tokens (   24.57 ms per token,    40.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =     312.81 ms /     2 runs   (  156.40 ms per token,     6.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1443.91 ms /    48 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 26 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     665.13 ms /    26 tokens (   25.58 ms per token,    39.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =     314.37 ms /     2 runs   (  157.18 ms per token,     6.36 tokens per second)\n",
      "llama_perf_context_print:       total time =     980.23 ms /    28 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 26 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     678.17 ms /    26 tokens (   26.08 ms per token,    38.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =     345.26 ms /     2 runs   (  172.63 ms per token,     5.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1024.17 ms /    28 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 15 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     460.82 ms /    15 tokens (   30.72 ms per token,    32.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =     314.16 ms /     2 runs   (  157.08 ms per token,     6.37 tokens per second)\n",
      "llama_perf_context_print:       total time =     775.68 ms /    17 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 20 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     540.41 ms /    20 tokens (   27.02 ms per token,    37.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =     307.88 ms /     2 runs   (  153.94 ms per token,     6.50 tokens per second)\n",
      "llama_perf_context_print:       total time =     848.95 ms /    22 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 24 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     611.33 ms /    24 tokens (   25.47 ms per token,    39.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =     310.60 ms /     2 runs   (  155.30 ms per token,     6.44 tokens per second)\n",
      "llama_perf_context_print:       total time =     922.63 ms /    26 tokens\n",
      "Llama.generate: 67 prefix-match hit, remaining 20 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     516.90 ms /    20 tokens (   25.85 ms per token,    38.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =     317.25 ms /     2 runs   (  158.62 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =     834.84 ms /    22 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 24 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     609.51 ms /    24 tokens (   25.40 ms per token,    39.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =     313.59 ms /     2 runs   (  156.79 ms per token,     6.38 tokens per second)\n",
      "llama_perf_context_print:       total time =     923.81 ms /    26 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 23 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     586.95 ms /    23 tokens (   25.52 ms per token,    39.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =     312.96 ms /     2 runs   (  156.48 ms per token,     6.39 tokens per second)\n",
      "llama_perf_context_print:       total time =     900.68 ms /    25 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     702.50 ms /    29 tokens (   24.22 ms per token,    41.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =     307.89 ms /     2 runs   (  153.95 ms per token,     6.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1011.10 ms /    31 tokens\n",
      "Llama.generate: 75 prefix-match hit, remaining 16 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     398.93 ms /    16 tokens (   24.93 ms per token,    40.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =     316.17 ms /     2 runs   (  158.08 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =     715.86 ms /    18 tokens\n",
      "Llama.generate: 79 prefix-match hit, remaining 13 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     342.29 ms /    13 tokens (   26.33 ms per token,    37.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =     309.23 ms /     2 runs   (  154.61 ms per token,     6.47 tokens per second)\n",
      "llama_perf_context_print:       total time =     652.24 ms /    15 tokens\n",
      "Llama.generate: 70 prefix-match hit, remaining 22 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     576.49 ms /    22 tokens (   26.20 ms per token,    38.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =     312.86 ms /     2 runs   (  156.43 ms per token,     6.39 tokens per second)\n",
      "llama_perf_context_print:       total time =     890.08 ms /    24 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 28 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     721.16 ms /    28 tokens (   25.76 ms per token,    38.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =     310.41 ms /     2 runs   (  155.20 ms per token,     6.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1032.41 ms /    30 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 23 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     601.03 ms /    23 tokens (   26.13 ms per token,    38.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =     321.67 ms /     2 runs   (  160.83 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =     923.37 ms /    25 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 49 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1205.02 ms /    49 tokens (   24.59 ms per token,    40.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =     310.41 ms /     2 runs   (  155.21 ms per token,     6.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    1516.20 ms /    51 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 30 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     736.79 ms /    30 tokens (   24.56 ms per token,    40.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =     315.39 ms /     2 runs   (  157.70 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1052.91 ms /    32 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 247 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    5916.63 ms /   247 tokens (   23.95 ms per token,    41.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42836.80 ms /   255 runs   (  167.99 ms per token,     5.95 tokens per second)\n",
      "llama_perf_context_print:       total time =   48840.45 ms /   502 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 13 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All companies or issuers with a Corruption Perception Index (Transparency International) score between 30 and 40.\n",
      "\n",
      "Query: [CCACorruptionPerceptionIndex] null '>=50'\n",
      "\n",
      "### Response:\n",
      "All companies or issuers with a Corruption Perception Index (Transparency International) score of 50 or higher.\n",
      "\n",
      "Query: [CCACorruptionPerceptionIndex] null '<30'\n",
      "\n",
      "### Response:\n",
      "All companies or issuers with a Corruption Perception Index (Transparency International) score below 30.\n",
      "\n",
      "Query: [CCACorruptionPerceptionIndex] null '40'\n",
      "\n",
      "### Response:\n",
      "All companies or issuers with a Corruption Perception Index (Transparency International) score of 40.\n",
      "\n",
      "Query: [CCACorruptionPerceptionIndex] null '>=30 but <40'\n",
      "\n",
      "### Response:\n",
      "All companies or issuers with a Corruption Perception Index (Transparency International) score between 30 and 40.\n",
      "\n",
      "Query: [CCACorruptionPerceptionIndex] null '>=50'\n",
      "\n",
      "### Response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     434.00 ms /    13 tokens (   33.38 ms per token,    29.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =     372.99 ms /     2 runs   (  186.49 ms per token,     5.36 tokens per second)\n",
      "llama_perf_context_print:       total time =     807.79 ms /    15 tokens\n",
      "Llama.generate: 78 prefix-match hit, remaining 11 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     489.12 ms /    11 tokens (   44.47 ms per token,    22.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =     406.31 ms /     2 runs   (  203.15 ms per token,     4.92 tokens per second)\n",
      "llama_perf_context_print:       total time =     896.13 ms /    13 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 15 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     476.05 ms /    15 tokens (   31.74 ms per token,    31.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =     329.95 ms /     2 runs   (  164.97 ms per token,     6.06 tokens per second)\n",
      "llama_perf_context_print:       total time =     806.68 ms /    17 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     683.92 ms /    21 tokens (   32.57 ms per token,    30.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =     372.90 ms /     2 runs   (  186.45 ms per token,     5.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1057.44 ms /    23 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 24 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     662.55 ms /    24 tokens (   27.61 ms per token,    36.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =     349.58 ms /     2 runs   (  174.79 ms per token,     5.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1012.92 ms /    26 tokens\n",
      "Llama.generate: 75 prefix-match hit, remaining 14 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     400.63 ms /    14 tokens (   28.62 ms per token,    34.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =     332.16 ms /     2 runs   (  166.08 ms per token,     6.02 tokens per second)\n",
      "llama_perf_context_print:       total time =     733.42 ms /    16 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 26 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     725.77 ms /    26 tokens (   27.91 ms per token,    35.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =     328.56 ms /     2 runs   (  164.28 ms per token,     6.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1055.00 ms /    28 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     627.91 ms /    21 tokens (   29.90 ms per token,    33.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =     348.18 ms /     2 runs   (  174.09 ms per token,     5.74 tokens per second)\n",
      "llama_perf_context_print:       total time =     977.52 ms /    23 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 27 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     746.14 ms /    27 tokens (   27.63 ms per token,    36.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =     333.22 ms /     2 runs   (  166.61 ms per token,     6.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1080.09 ms /    29 tokens\n",
      "Llama.generate: 92 prefix-match hit, remaining 80 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    2202.97 ms /    80 tokens (   27.54 ms per token,    36.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =     344.73 ms /     2 runs   (  172.37 ms per token,     5.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    2548.58 ms /    82 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 31 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     824.77 ms /    31 tokens (   26.61 ms per token,    37.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =     345.27 ms /     2 runs   (  172.64 ms per token,     5.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1170.82 ms /    33 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     573.70 ms /    21 tokens (   27.32 ms per token,    36.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =     333.76 ms /     2 runs   (  166.88 ms per token,     5.99 tokens per second)\n",
      "llama_perf_context_print:       total time =     908.06 ms /    23 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     573.18 ms /    21 tokens (   27.29 ms per token,    36.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =     406.48 ms /     2 runs   (  203.24 ms per token,     4.92 tokens per second)\n",
      "llama_perf_context_print:       total time =     980.28 ms /    23 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 28 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     951.50 ms /    28 tokens (   33.98 ms per token,    29.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =     394.50 ms /     2 runs   (  197.25 ms per token,     5.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1346.97 ms /    30 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 24 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     969.48 ms /    24 tokens (   40.39 ms per token,    24.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =     453.20 ms /     2 runs   (  226.60 ms per token,     4.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1423.39 ms /    26 tokens\n",
      "Llama.generate: 84 prefix-match hit, remaining 9 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     359.50 ms /     9 tokens (   39.94 ms per token,    25.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =     344.70 ms /     2 runs   (  172.35 ms per token,     5.80 tokens per second)\n",
      "llama_perf_context_print:       total time =     705.42 ms /    11 tokens\n",
      "Llama.generate: 72 prefix-match hit, remaining 17 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     588.62 ms /    17 tokens (   34.62 ms per token,    28.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =     342.10 ms /     2 runs   (  171.05 ms per token,     5.85 tokens per second)\n",
      "llama_perf_context_print:       total time =     931.33 ms /    19 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     932.59 ms /    29 tokens (   32.16 ms per token,    31.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =     352.31 ms /     2 runs   (  176.15 ms per token,     5.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1285.67 ms /    31 tokens\n",
      "Llama.generate: 70 prefix-match hit, remaining 15 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     451.12 ms /    15 tokens (   30.07 ms per token,    33.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =     344.30 ms /     2 runs   (  172.15 ms per token,     5.81 tokens per second)\n",
      "llama_perf_context_print:       total time =     796.03 ms /    17 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 7 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     265.56 ms /     7 tokens (   37.94 ms per token,    26.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =     389.00 ms /     2 runs   (  194.50 ms per token,     5.14 tokens per second)\n",
      "llama_perf_context_print:       total time =     655.27 ms /     9 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 19 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     553.71 ms /    19 tokens (   29.14 ms per token,    34.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =     340.71 ms /     2 runs   (  170.35 ms per token,     5.87 tokens per second)\n",
      "llama_perf_context_print:       total time =     895.31 ms /    21 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 24 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     634.52 ms /    24 tokens (   26.44 ms per token,    37.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =     354.25 ms /     2 runs   (  177.13 ms per token,     5.65 tokens per second)\n",
      "llama_perf_context_print:       total time =     989.37 ms /    26 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 25 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     640.49 ms /    25 tokens (   25.62 ms per token,    39.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =     324.65 ms /     2 runs   (  162.32 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =     965.95 ms /    27 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 30 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     735.29 ms /    30 tokens (   24.51 ms per token,    40.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =     325.34 ms /     2 runs   (  162.67 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1061.30 ms /    32 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 27 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     671.70 ms /    27 tokens (   24.88 ms per token,    40.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =     329.12 ms /     2 runs   (  164.56 ms per token,     6.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1001.51 ms /    29 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 14 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     352.44 ms /    14 tokens (   25.17 ms per token,    39.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =     319.44 ms /     2 runs   (  159.72 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =     672.48 ms /    16 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 23 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     588.05 ms /    23 tokens (   25.57 ms per token,    39.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =     317.44 ms /     2 runs   (  158.72 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =     906.38 ms /    25 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 22 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     567.25 ms /    22 tokens (   25.78 ms per token,    38.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =     320.03 ms /     2 runs   (  160.02 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =     887.89 ms /    24 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     810.74 ms /    29 tokens (   27.96 ms per token,    35.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =     338.49 ms /     2 runs   (  169.24 ms per token,     5.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    1149.78 ms /    31 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 16 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     496.53 ms /    16 tokens (   31.03 ms per token,    32.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =     323.06 ms /     2 runs   (  161.53 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =     820.32 ms /    18 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     535.95 ms /    21 tokens (   25.52 ms per token,    39.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =     318.07 ms /     2 runs   (  159.04 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =     854.84 ms /    23 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 24 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     629.93 ms /    24 tokens (   26.25 ms per token,    38.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =     313.55 ms /     2 runs   (  156.78 ms per token,     6.38 tokens per second)\n",
      "llama_perf_context_print:       total time =     944.23 ms /    26 tokens\n",
      "Llama.generate: 75 prefix-match hit, remaining 16 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     408.20 ms /    16 tokens (   25.51 ms per token,    39.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =     318.82 ms /     2 runs   (  159.41 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =     727.71 ms /    18 tokens\n",
      "Llama.generate: 78 prefix-match hit, remaining 8 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     235.19 ms /     8 tokens (   29.40 ms per token,    34.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =     318.92 ms /     2 runs   (  159.46 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =     554.85 ms /    10 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     538.35 ms /    21 tokens (   25.64 ms per token,    39.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =     318.49 ms /     2 runs   (  159.24 ms per token,     6.28 tokens per second)\n",
      "llama_perf_context_print:       total time =     857.45 ms /    23 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 31 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     766.89 ms /    31 tokens (   24.74 ms per token,    40.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =     323.62 ms /     2 runs   (  161.81 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1091.16 ms /    33 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 31 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     761.00 ms /    31 tokens (   24.55 ms per token,    40.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =     311.41 ms /     2 runs   (  155.71 ms per token,     6.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1073.16 ms /    33 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 14 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     362.14 ms /    14 tokens (   25.87 ms per token,    38.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =     341.12 ms /     2 runs   (  170.56 ms per token,     5.86 tokens per second)\n",
      "llama_perf_context_print:       total time =     704.53 ms /    16 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 16 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     415.44 ms /    16 tokens (   25.97 ms per token,    38.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =     343.75 ms /     2 runs   (  171.88 ms per token,     5.82 tokens per second)\n",
      "llama_perf_context_print:       total time =     759.86 ms /    18 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 16 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     408.85 ms /    16 tokens (   25.55 ms per token,    39.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =     312.53 ms /     2 runs   (  156.27 ms per token,     6.40 tokens per second)\n",
      "llama_perf_context_print:       total time =     722.29 ms /    18 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 24 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     607.14 ms /    24 tokens (   25.30 ms per token,    39.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =     317.58 ms /     2 runs   (  158.79 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =     925.37 ms /    26 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 27 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     691.32 ms /    27 tokens (   25.60 ms per token,    39.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =     317.90 ms /     2 runs   (  158.95 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1010.07 ms /    29 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 28 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     705.18 ms /    28 tokens (   25.19 ms per token,    39.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =     314.46 ms /     2 runs   (  157.23 ms per token,     6.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    1020.41 ms /    30 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 37 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     979.96 ms /    37 tokens (   26.49 ms per token,    37.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =     319.24 ms /     2 runs   (  159.62 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1299.92 ms /    39 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 20 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     528.42 ms /    20 tokens (   26.42 ms per token,    37.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =     326.51 ms /     2 runs   (  163.26 ms per token,     6.13 tokens per second)\n",
      "llama_perf_context_print:       total time =     855.69 ms /    22 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 104 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    2523.17 ms /   104 tokens (   24.26 ms per token,    41.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =     319.95 ms /     2 runs   (  159.98 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    2844.14 ms /   106 tokens\n",
      "Llama.generate: 72 prefix-match hit, remaining 26 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     735.75 ms /    26 tokens (   28.30 ms per token,    35.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =     353.05 ms /     2 runs   (  176.52 ms per token,     5.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    1089.45 ms /    28 tokens\n",
      "Llama.generate: 70 prefix-match hit, remaining 38 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1105.27 ms /    38 tokens (   29.09 ms per token,    34.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =     352.45 ms /     2 runs   (  176.23 ms per token,     5.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1458.45 ms /    40 tokens\n",
      "Llama.generate: 73 prefix-match hit, remaining 47 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1248.72 ms /    47 tokens (   26.57 ms per token,    37.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =     365.99 ms /     2 runs   (  182.99 ms per token,     5.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1615.41 ms /    49 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 42 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1124.82 ms /    42 tokens (   26.78 ms per token,    37.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =     307.02 ms /     2 runs   (  153.51 ms per token,     6.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1432.79 ms /    44 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 169 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    4003.51 ms /   169 tokens (   23.69 ms per token,    42.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =     304.39 ms /     2 runs   (  152.19 ms per token,     6.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    4308.94 ms /   171 tokens\n",
      "Llama.generate: 74 prefix-match hit, remaining 248 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    5952.52 ms /   248 tokens (   24.00 ms per token,    41.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =     324.00 ms /     2 runs   (  162.00 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    6277.32 ms /   250 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 25 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     656.73 ms /    25 tokens (   26.27 ms per token,    38.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =     330.14 ms /     2 runs   (  165.07 ms per token,     6.06 tokens per second)\n",
      "llama_perf_context_print:       total time =     987.59 ms /    27 tokens\n",
      "Llama.generate: 70 prefix-match hit, remaining 47 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1156.65 ms /    47 tokens (   24.61 ms per token,    40.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =     346.59 ms /     2 runs   (  173.29 ms per token,     5.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    1504.00 ms /    49 tokens\n",
      "Llama.generate: 73 prefix-match hit, remaining 48 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1176.86 ms /    48 tokens (   24.52 ms per token,    40.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =     311.89 ms /     2 runs   (  155.95 ms per token,     6.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1489.38 ms /    50 tokens\n",
      "Llama.generate: 70 prefix-match hit, remaining 32 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     778.32 ms /    32 tokens (   24.32 ms per token,    41.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =     313.02 ms /     2 runs   (  156.51 ms per token,     6.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1092.16 ms /    34 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 23 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     604.83 ms /    23 tokens (   26.30 ms per token,    38.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =     325.03 ms /     2 runs   (  162.51 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =     930.87 ms /    25 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 47 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1175.14 ms /    47 tokens (   25.00 ms per token,    40.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =     304.49 ms /     2 runs   (  152.24 ms per token,     6.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    1480.24 ms /    49 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 44 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1083.61 ms /    44 tokens (   24.63 ms per token,    40.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =     344.77 ms /     2 runs   (  172.39 ms per token,     5.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1429.01 ms /    46 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 33 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     863.10 ms /    33 tokens (   26.15 ms per token,    38.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =     331.31 ms /     2 runs   (  165.65 ms per token,     6.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1195.06 ms /    35 tokens\n",
      "Llama.generate: 85 prefix-match hit, remaining 14 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     386.64 ms /    14 tokens (   27.62 ms per token,    36.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =     347.11 ms /     2 runs   (  173.55 ms per token,     5.76 tokens per second)\n",
      "llama_perf_context_print:       total time =     734.56 ms /    16 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 33 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     876.41 ms /    33 tokens (   26.56 ms per token,    37.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =     321.16 ms /     2 runs   (  160.58 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1198.40 ms /    35 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 38 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     956.60 ms /    38 tokens (   25.17 ms per token,    39.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =     329.90 ms /     2 runs   (  164.95 ms per token,     6.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.21 ms /    40 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 40 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1017.38 ms /    40 tokens (   25.43 ms per token,    39.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =     337.14 ms /     2 runs   (  168.57 ms per token,     5.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1355.30 ms /    42 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 122 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    3211.78 ms /   122 tokens (   26.33 ms per token,    37.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     310.50 ms /     2 runs   (  155.25 ms per token,     6.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    3523.40 ms /   124 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 35 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     884.13 ms /    35 tokens (   25.26 ms per token,    39.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =     327.39 ms /     2 runs   (  163.69 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1212.29 ms /    37 tokens\n",
      "Llama.generate: 74 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     821.96 ms /    29 tokens (   28.34 ms per token,    35.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =     328.02 ms /     2 runs   (  164.01 ms per token,     6.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    1150.74 ms /    31 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 33 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     844.79 ms /    33 tokens (   25.60 ms per token,    39.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =     316.92 ms /     2 runs   (  158.46 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1162.40 ms /    35 tokens\n",
      "Llama.generate: 73 prefix-match hit, remaining 26 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     737.96 ms /    26 tokens (   28.38 ms per token,    35.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =     347.32 ms /     2 runs   (  173.66 ms per token,     5.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    1086.60 ms /    28 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 40 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1071.27 ms /    40 tokens (   26.78 ms per token,    37.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =     333.91 ms /     2 runs   (  166.95 ms per token,     5.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    1405.90 ms /    42 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 26 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     703.85 ms /    26 tokens (   27.07 ms per token,    36.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =     337.07 ms /     2 runs   (  168.54 ms per token,     5.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1041.92 ms /    28 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 39 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1072.13 ms /    39 tokens (   27.49 ms per token,    36.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =     422.82 ms /     2 runs   (  211.41 ms per token,     4.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1496.18 ms /    41 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 71 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1831.38 ms /    71 tokens (   25.79 ms per token,    38.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =     344.89 ms /     2 runs   (  172.45 ms per token,     5.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    2177.11 ms /    73 tokens\n",
      "Llama.generate: 72 prefix-match hit, remaining 250 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    5992.16 ms /   250 tokens (   23.97 ms per token,    41.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =     303.28 ms /     2 runs   (  151.64 ms per token,     6.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    6296.46 ms /   252 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1728.78 ms /    70 tokens (   24.70 ms per token,    40.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =     314.67 ms /     2 runs   (  157.33 ms per token,     6.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    2044.43 ms /    72 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 239 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    5779.70 ms /   239 tokens (   24.18 ms per token,    41.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =     313.62 ms /     2 runs   (  156.81 ms per token,     6.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    6094.09 ms /   241 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 62 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1508.68 ms /    62 tokens (   24.33 ms per token,    41.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =     329.78 ms /     2 runs   (  164.89 ms per token,     6.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1839.16 ms /    64 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 54 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1345.27 ms /    54 tokens (   24.91 ms per token,    40.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =     317.21 ms /     2 runs   (  158.61 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1663.31 ms /    56 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 44 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1066.75 ms /    44 tokens (   24.24 ms per token,    41.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =     325.55 ms /     2 runs   (  162.77 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1393.00 ms /    46 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 96 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    2339.26 ms /    96 tokens (   24.37 ms per token,    41.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =     333.07 ms /     2 runs   (  166.54 ms per token,     6.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    2673.15 ms /    98 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 82 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    2026.64 ms /    82 tokens (   24.72 ms per token,    40.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =     315.72 ms /     2 runs   (  157.86 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    2343.24 ms /    84 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 63 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1580.76 ms /    63 tokens (   25.09 ms per token,    39.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =     358.55 ms /     2 runs   (  179.28 ms per token,     5.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    1940.11 ms /    65 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 259 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    6432.41 ms /   259 tokens (   24.84 ms per token,    40.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =     332.92 ms /     2 runs   (  166.46 ms per token,     6.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    6766.44 ms /   261 tokens\n",
      "Llama.generate: 81 prefix-match hit, remaining 241 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    5994.64 ms /   241 tokens (   24.87 ms per token,    40.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =     351.54 ms /     2 runs   (  175.77 ms per token,     5.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    6347.38 ms /   243 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 77 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1847.98 ms /    77 tokens (   24.00 ms per token,    41.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =     310.59 ms /     2 runs   (  155.30 ms per token,     6.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    2159.35 ms /    79 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 66 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1686.06 ms /    66 tokens (   25.55 ms per token,    39.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =     327.21 ms /     2 runs   (  163.60 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    2013.93 ms /    68 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 57 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1389.92 ms /    57 tokens (   24.38 ms per token,    41.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =     323.77 ms /     2 runs   (  161.89 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1714.54 ms /    59 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 62 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1482.37 ms /    62 tokens (   23.91 ms per token,    41.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =     320.17 ms /     2 runs   (  160.08 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1803.24 ms /    64 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 105 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    2560.65 ms /   105 tokens (   24.39 ms per token,    41.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =     348.72 ms /     2 runs   (  174.36 ms per token,     5.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    2910.67 ms /   107 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 99 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    2477.56 ms /    99 tokens (   25.03 ms per token,    39.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =     306.90 ms /     2 runs   (  153.45 ms per token,     6.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    2785.20 ms /   101 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 24 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     612.54 ms /    24 tokens (   25.52 ms per token,    39.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =     310.10 ms /     2 runs   (  155.05 ms per token,     6.45 tokens per second)\n",
      "llama_perf_context_print:       total time =     923.37 ms /    26 tokens\n",
      "Llama.generate: 64 prefix-match hit, remaining 38 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     955.37 ms /    38 tokens (   25.14 ms per token,    39.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =     323.70 ms /     2 runs   (  161.85 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1280.06 ms /    40 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 41 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1003.13 ms /    41 tokens (   24.47 ms per token,    40.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =     315.82 ms /     2 runs   (  157.91 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    1319.59 ms /    43 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 68 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1662.18 ms /    68 tokens (   24.44 ms per token,    40.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =     331.86 ms /     2 runs   (  165.93 ms per token,     6.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1994.90 ms /    70 tokens\n",
      "Llama.generate: 65 prefix-match hit, remaining 36 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     904.79 ms /    36 tokens (   25.13 ms per token,    39.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =     309.91 ms /     2 runs   (  154.95 ms per token,     6.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1215.40 ms /    38 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 56 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1352.61 ms /    56 tokens (   24.15 ms per token,    41.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =     322.72 ms /     2 runs   (  161.36 ms per token,     6.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1676.21 ms /    58 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 42 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1033.07 ms /    42 tokens (   24.60 ms per token,    40.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =     319.60 ms /     2 runs   (  159.80 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1353.53 ms /    44 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 46 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1152.09 ms /    46 tokens (   25.05 ms per token,    39.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =     328.91 ms /     2 runs   (  164.45 ms per token,     6.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1481.94 ms /    48 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 19 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     517.22 ms /    19 tokens (   27.22 ms per token,    36.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =     335.15 ms /     2 runs   (  167.57 ms per token,     5.97 tokens per second)\n",
      "llama_perf_context_print:       total time =     853.09 ms /    21 tokens\n",
      "Llama.generate: 70 prefix-match hit, remaining 20 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     544.05 ms /    20 tokens (   27.20 ms per token,    36.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =     337.54 ms /     2 runs   (  168.77 ms per token,     5.93 tokens per second)\n",
      "llama_perf_context_print:       total time =     882.43 ms /    22 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 68 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1684.00 ms /    68 tokens (   24.76 ms per token,    40.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =     348.69 ms /     2 runs   (  174.34 ms per token,     5.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    2033.50 ms /    70 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 31 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     843.34 ms /    31 tokens (   27.20 ms per token,    36.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =     332.95 ms /     2 runs   (  166.47 ms per token,     6.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    1177.16 ms /    33 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     727.86 ms /    29 tokens (   25.10 ms per token,    39.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =     325.78 ms /     2 runs   (  162.89 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    1054.43 ms /    31 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 92 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    2213.57 ms /    92 tokens (   24.06 ms per token,    41.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =     338.37 ms /     2 runs   (  169.18 ms per token,     5.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    2552.72 ms /    94 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 52 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1251.64 ms /    52 tokens (   24.07 ms per token,    41.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =     308.48 ms /     2 runs   (  154.24 ms per token,     6.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1560.90 ms /    54 tokens\n",
      "Llama.generate: 68 prefix-match hit, remaining 69 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1626.17 ms /    69 tokens (   23.57 ms per token,    42.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =     323.71 ms /     2 runs   (  161.85 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1950.53 ms /    71 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 44 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1111.66 ms /    44 tokens (   25.27 ms per token,    39.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =     309.49 ms /     2 runs   (  154.75 ms per token,     6.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1421.93 ms /    46 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 249 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    6099.67 ms /   249 tokens (   24.50 ms per token,    40.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =     349.80 ms /     2 runs   (  174.90 ms per token,     5.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    6450.47 ms /   251 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 245 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    5918.21 ms /   245 tokens (   24.16 ms per token,    41.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =     320.69 ms /     2 runs   (  160.35 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    6239.80 ms /   247 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 40 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     976.30 ms /    40 tokens (   24.41 ms per token,    40.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =     319.02 ms /     2 runs   (  159.51 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1296.35 ms /    42 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 50 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1225.76 ms /    50 tokens (   24.52 ms per token,    40.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =     322.00 ms /     2 runs   (  161.00 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1548.48 ms /    52 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 253 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    6074.93 ms /   253 tokens (   24.01 ms per token,    41.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =     324.42 ms /     2 runs   (  162.21 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    6400.19 ms /   255 tokens\n",
      "Llama.generate: 69 prefix-match hit, remaining 253 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    6146.57 ms /   253 tokens (   24.29 ms per token,    41.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =     304.34 ms /     2 runs   (  152.17 ms per token,     6.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    6451.92 ms /   255 tokens\n",
      "Llama.generate: 102 prefix-match hit, remaining 219 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    5311.51 ms /   219 tokens (   24.25 ms per token,    41.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =     334.48 ms /     2 runs   (  167.24 ms per token,     5.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    5646.75 ms /   221 tokens\n",
      "Llama.generate: 103 prefix-match hit, remaining 163 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    3931.39 ms /   163 tokens (   24.12 ms per token,    41.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =     321.81 ms /     2 runs   (  160.90 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    4254.05 ms /   165 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 229 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    5505.41 ms /   229 tokens (   24.04 ms per token,    41.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =     326.47 ms /     2 runs   (  163.24 ms per token,     6.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    5832.74 ms /   231 tokens\n",
      "Llama.generate: 321 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     490.37 ms /     3 runs   (  163.46 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =     491.02 ms /     4 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 67 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1649.28 ms /    67 tokens (   24.62 ms per token,    40.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =     319.10 ms /     2 runs   (  159.55 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1969.27 ms /    69 tokens\n",
      "Llama.generate: 70 prefix-match hit, remaining 61 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1481.30 ms /    61 tokens (   24.28 ms per token,    41.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =     341.79 ms /     2 runs   (  170.89 ms per token,     5.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    1824.02 ms /    63 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 35 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     880.41 ms /    35 tokens (   25.15 ms per token,    39.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =     326.44 ms /     2 runs   (  163.22 ms per token,     6.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1207.91 ms /    37 tokens\n",
      "Llama.generate: 67 prefix-match hit, remaining 40 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     971.52 ms /    40 tokens (   24.29 ms per token,    41.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =     315.34 ms /     2 runs   (  157.67 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.93 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 19 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     481.87 ms /    19 tokens (   25.36 ms per token,    39.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =     319.38 ms /     2 runs   (  159.69 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =     801.81 ms /    21 tokens\n",
      "Llama.generate: 67 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     702.62 ms /    29 tokens (   24.23 ms per token,    41.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =     306.33 ms /     2 runs   (  153.17 ms per token,     6.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1009.69 ms /    31 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 75 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1769.25 ms /    75 tokens (   23.59 ms per token,    42.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =     305.75 ms /     2 runs   (  152.88 ms per token,     6.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    2075.85 ms /    77 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     521.89 ms /    21 tokens (   24.85 ms per token,    40.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =     311.95 ms /     2 runs   (  155.98 ms per token,     6.41 tokens per second)\n",
      "llama_perf_context_print:       total time =     834.62 ms /    23 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 42 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     995.66 ms /    42 tokens (   23.71 ms per token,    42.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =     300.02 ms /     2 runs   (  150.01 ms per token,     6.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1296.50 ms /    44 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 103 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    2432.28 ms /   103 tokens (   23.61 ms per token,    42.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =     305.71 ms /     2 runs   (  152.85 ms per token,     6.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    2739.12 ms /   105 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 95 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    2230.24 ms /    95 tokens (   23.48 ms per token,    42.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =     298.70 ms /     2 runs   (  149.35 ms per token,     6.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    2529.80 ms /    97 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 62 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1446.04 ms /    62 tokens (   23.32 ms per token,    42.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =     307.67 ms /     2 runs   (  153.84 ms per token,     6.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    1754.62 ms /    64 tokens\n",
      "Llama.generate: 68 prefix-match hit, remaining 99 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    2343.72 ms /    99 tokens (   23.67 ms per token,    42.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =     301.96 ms /     2 runs   (  150.98 ms per token,     6.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    2646.53 ms /   101 tokens\n",
      "Llama.generate: 91 prefix-match hit, remaining 79 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1857.12 ms /    79 tokens (   23.51 ms per token,    42.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =     301.66 ms /     2 runs   (  150.83 ms per token,     6.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    2159.68 ms /    81 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 60 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1431.05 ms /    60 tokens (   23.85 ms per token,    41.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =     315.58 ms /     2 runs   (  157.79 ms per token,     6.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1747.38 ms /    62 tokens\n",
      "Llama.generate: 95 prefix-match hit, remaining 65 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1556.99 ms /    65 tokens (   23.95 ms per token,    41.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =     299.04 ms /     2 runs   (  149.52 ms per token,     6.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1856.74 ms /    67 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 62 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1456.79 ms /    62 tokens (   23.50 ms per token,    42.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =     306.83 ms /     2 runs   (  153.41 ms per token,     6.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1764.49 ms /    64 tokens\n",
      "Llama.generate: 95 prefix-match hit, remaining 31 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     735.92 ms /    31 tokens (   23.74 ms per token,    42.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =     318.13 ms /     2 runs   (  159.06 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1054.76 ms /    33 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 35 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     866.40 ms /    35 tokens (   24.75 ms per token,    40.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =     303.13 ms /     2 runs   (  151.56 ms per token,     6.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    1170.28 ms /    37 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 52 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1210.71 ms /    52 tokens (   23.28 ms per token,    42.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =     316.81 ms /     2 runs   (  158.40 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1528.48 ms /    54 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 149 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    3521.87 ms /   149 tokens (   23.64 ms per token,    42.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =     315.96 ms /     2 runs   (  157.98 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3838.81 ms /   151 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 41 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     969.82 ms /    41 tokens (   23.65 ms per token,    42.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =     309.85 ms /     2 runs   (  154.93 ms per token,     6.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1280.66 ms /    43 tokens\n",
      "Llama.generate: 84 prefix-match hit, remaining 25 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     611.94 ms /    25 tokens (   24.48 ms per token,    40.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =     307.98 ms /     2 runs   (  153.99 ms per token,     6.49 tokens per second)\n",
      "llama_perf_context_print:       total time =     920.70 ms /    27 tokens\n",
      "Llama.generate: 72 prefix-match hit, remaining 85 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    2024.41 ms /    85 tokens (   23.82 ms per token,    41.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     320.12 ms /     2 runs   (  160.06 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    2345.39 ms /    87 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 61 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1397.08 ms /    61 tokens (   22.90 ms per token,    43.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =     305.45 ms /     2 runs   (  152.73 ms per token,     6.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1703.40 ms /    63 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 84 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1961.64 ms /    84 tokens (   23.35 ms per token,    42.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =     304.74 ms /     2 runs   (  152.37 ms per token,     6.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    2267.14 ms /    86 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1631.04 ms /    70 tokens (   23.30 ms per token,    42.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =     309.16 ms /     2 runs   (  154.58 ms per token,     6.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1941.21 ms /    72 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 51 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =    1320.97 ms /    51 tokens (   25.90 ms per token,    38.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =     345.12 ms /     2 runs   (  172.56 ms per token,     5.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1666.94 ms /    53 tokens\n",
      "Llama.generate: 63 prefix-match hit, remaining 26 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4865.62 ms\n",
      "llama_perf_context_print: prompt eval time =     736.69 ms /    26 tokens (   28.33 ms per token,    35.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =     323.55 ms /     2 runs   (  161.78 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1061.14 ms /    28 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Comprehensibility ratings added and saved to 'translated_queries_with_ratings.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_file = \"translated_queries_TEST5.xlsx\"\n",
    "\n",
    "# Extract the English column\n",
    "df = pd.read_excel(output_file)\n",
    "english_queries = df[\"English\"].tolist()\n",
    "\n",
    "# Function to request a comprehensibility rating from the model\n",
    "def rate_comprehensibility(text, max_tokens=256):\n",
    "    # Adjust the prompt to ask the model for a rating from 1 to 10\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Please rate the following text's comprehensibility from 1 to 10, where:\n",
    "- 1 = Completely incomprehensible, nonsensical, or full of errors.\n",
    "- 5 = Understandable with effort; some awkwardness, complexity, or minor errors.\n",
    "- 10 = Perfectly clear, natural, and easy to understand.\n",
    "\n",
    "Here are some examples:\n",
    "Text: \"asjdk asjd aksd\" → Rating: 1\n",
    "Text: \"Provide list companies ESG data incomplete understandable\" → Rating: 4\n",
    "Text: \"Please provide a list of companies with complete ESG data.\" → Rating: 9\n",
    "\n",
    "Now, rate this text:\n",
    "Text: {text}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    # Call model (adjust temperature and other params as needed)\n",
    "    response = llm(prompt, max_tokens=max_tokens, temperature=0.2)\n",
    "    rating = response[\"choices\"][0][\"text\"].strip()\n",
    "    print(rating)\n",
    "    # Ensure the response is a valid number between 1 and 10\n",
    "    try:\n",
    "        rating = int(rating)\n",
    "        if 1 <= rating <= 10:\n",
    "            return rating\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return None  # Return None if no valid rating is obtained\n",
    "\n",
    "# List to store ratings\n",
    "ratings = []\n",
    "\n",
    "# Iterate through each English translation and get a rating\n",
    "for query in english_queries:\n",
    "    rating = rate_comprehensibility(query)\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Add the ratings as a new column to the dataframe\n",
    "df[\"Comprehensibility Rating\"] = ratings\n",
    "\n",
    "# Save the updated dataframe with ratings to a new Excel file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"Comprehensibility ratings added and saved to 'translated_queries_with_ratings.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f42872-6939-4c34-b74f-f177f8eb3d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
