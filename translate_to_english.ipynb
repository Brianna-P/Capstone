{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "id": "11329b8f-ae0a-48d4-9da9-7c1c06875c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (3.1.5)\n",
      "Requirement already satisfied: transformers in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-cpp-python in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (2.2.3)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "11329b8f-ae0a-48d4-9da9-7c1c06875c10",
   "metadata": {},
   "outputs": [],
>>>>>>> 1b082938f708d7b70008a355a076f30a20f6b6e3
   "source": [
    "%pip install pandas openpyxl transformers torch\n",
    "%pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "id": "8a607fdc-3c30-4006-af0d-60789c816fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/solar/OneDrive/Documents/Capstone/Capstone-Jupyter/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 4096\n",
      "llama_init_from_model: n_ctx_per_seq = 4096\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_init_from_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "8a607fdc-3c30-4006-af0d-60789c816fda",
   "metadata": {},
   "outputs": [],
>>>>>>> 1b082938f708d7b70008a355a076f30a20f6b6e3
   "source": [
    "from llama_cpp import Llama\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "model_path = \"/Users/solar/OneDrive/Documents/Capstone/Capstone-Jupyter/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "max_context = 4096\n",
    "llm = Llama(\n",
    "    model_path,\n",
    "    n_ctx=max_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
   "id": "4566bf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest entry code: EUTaxManIntSerElcRevOverAlign\n",
      "Length: 1353\n",
      "Content: EU Taxon - Man, Instal, Serv of Elec Equip Overall Align. EU Taxon - Man, Instal, Serv of Elec Equip Overall Align: This factor identifies the overall alignment for Manufacture, installation, and servicing of high, medium and low voltage electrical equipment for electrical transmission and distribution that result in or enable a substantial contribution to climate change mitigation, covering the substantial contribution criteria, the do no significant harm criteria, and the minimum social safeguards. This is the aggregated result across all Taxonomy objectives. The possible values are: Aligned,Aligned (>90%),Aligned (>80%),Aligned (>70%),Aligned (>60%),Aligned (>50%),Aligned (>40%),Aligned (>30%),Aligned (>20%),Aligned (>10%),Aligned (>0%),Likely aligned (100%),Likely aligned (>90%),Likely aligned (>80%),Likely aligned (>70%),Likely aligned (>60%),Likely aligned (>50%),Likely aligned (>40%),Likely aligned (>30%),Likely aligned (>20%),Likely aligned (>10%),Likely aligned (>0%),Potentially aligned (100%),Potentially aligned (>90%),Potentially aligned (>80%),Potentially aligned (>70%),Potentially aligned (>60%),Potentially aligned (>50%),Potentially aligned (>40%),Potentially aligned (>30%),Potentially aligned (>20%),Potentially aligned (>10%),Potentially aligned (>0%),Likely not aligned,Not aligned, Not collected, and Not applicable.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open(\"metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_metadata = json.load(f)\n",
    "\n",
    "# Create a lookup table\n",
    "metadata_lookup = {\n",
    "    entry[\"code\"]: f'{entry[\"name\"]}. {entry[\"description\"]}' \n",
    "    for entry in raw_metadata\n",
    "}\n",
    "\n",
    "def extract_codes(query):\n",
    "    return re.findall(r\"\\[([A-Za-z0-9_]+)\\]\", query)\n",
    "\n",
    "# Find the entry with the longest name + description combo\n",
    "longest_entry = max(metadata_lookup.items(), key=lambda item: len(item[1]))\n",
    "\n",
    "# Print the result\n",
    "print(f'Longest entry code: {longest_entry[0]}')\n",
    "print(f'Length: {len(longest_entry[1])}')\n",
    "print(f'Content: {longest_entry[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
=======
   "execution_count": 2,
>>>>>>> 1b082938f708d7b70008a355a076f30a20f6b6e3
   "id": "efb3fc4c-e7b9-4bfb-9340-779aaba31116",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "\"\"\"\n",
    "def translate_query(query):\n",
    "    prompt = f\"### Instruction:\\nRephrase this ESGish query as a natural English sentence. Each query is asking for all companies or issuers that match some paramater.\\n\\n{query}\\n\\n### Response:\"\n",
    "\n",
    "    # Use llama_cpp tokenizer to get exact number of tokens in prompt\n",
    "    prompt_tokens = llm.tokenize(prompt.encode(\"utf-8\"))\n",
    "    num_prompt_tokens = len(prompt_tokens)\n",
    "\n",
    "    max_total_tokens = 512\n",
    "    max_response_tokens = max_total_tokens - num_prompt_tokens\n",
    "\n",
    "    if max_response_tokens < 1:\n",
    "        print(f\"Skipping query — prompt too long: {query}\")\n",
    "        return \"[Prompt too long]\"\n",
    "\n",
    "    response = llm(prompt, max_tokens=max_response_tokens, temperature=0.1)\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def translate_query(query):\n",
    "    codes = extract_codes(query)\n",
    "    context = \"\\n\".join(\n",
    "        f\"{code}: {metadata_lookup.get(code, 'No metadata found.')}\"\n",
    "        for code in codes\n",
    "    )\n",
    "    \n",
    "    prompt = f\"### Instruction:\n",
    "Rephrase the following ESGish query into a concise natural English sentence. Each query is asking for all companies or issuers that match some paramater. Use the following metadata definitions for clarity:\n",
    "\n",
    "{context}\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "### Response:\"\n",
    "\n",
    "    response = llm(prompt, max_tokens=2048, temperature=0.1)\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n",
    "\"\"\"\n",
    "def max_tokens(text, max_tokens=40):\n",
    "    tokens = llm.tokenize(text.encode(\"utf-8\"))\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "    truncated = llm.detokenize(tokens[:max_tokens]).decode(\"utf-8\", errors=\"ignore\")\n",
    "    return truncated + \"...\"\n",
    "\n",
    "def max_afforded_tokens(codes):\n",
    "    count = len(codes)\n",
    "    m_tokens = max_context/count  \n",
    "    if m_tokens < 100:\n",
    "        m_tokens = 100\n",
    "    return m_tokens\n",
    "\n",
    "\n",
    "def translate_query(query, max_total_tokens=2048, max_output_tokens=256):\n",
    "    codes = extract_codes(query)\n",
    "\n",
    "    m_tokens = max_afforded_tokens(codes)\n",
    "\n",
    "    # Build full context blocks for each code\n",
    "    context_blocks = [\n",
    "        f\"{code}: {max_tokens(metadata_lookup.get(code, 'No metadata found.'), max_tokens=m_tokens)}\" for code in codes\n",
    "    ]\n",
    "\n",
    "    # Initial prompt pieces\n",
    "    instruction = \"### Instruction:\\nRephrase the following ESGish query into a concise natural English sentence. Each query is asking for all companies or issuers that match some paramater. Use the following metadata definitions for clarity:\\n\\n\"\n",
    "    query_part = f\"\\n\\nQuery: {query}\\n\\n### Response:\"\n",
    "\n",
    "    # Tokenize instruction and query to calculate remaining token budget\n",
    "    tokenizer = llm.tokenize  # Built-in tokenizer\n",
    "    instruction_tokens = len(tokenizer(instruction.encode(\"utf-8\")))\n",
    "    query_tokens = len(tokenizer(query_part.encode(\"utf-8\")))\n",
    "    token_budget = max_total_tokens - max_output_tokens - instruction_tokens - query_tokens\n",
    "\n",
    "    # Now iteratively add context blocks until budget is exhausted\n",
    "    context = \"\"\n",
    "    used_tokens = 0\n",
    "    for block in context_blocks:\n",
    "        block_tokens = len(tokenizer(block.encode(\"utf-8\"))) + 1  # +1 for newline\n",
    "        if used_tokens + block_tokens <= token_budget:\n",
    "            context += block + \"\\n\"\n",
    "            used_tokens += block_tokens\n",
    "        else:\n",
    "            break  # stop once we're out of budget\n",
    "\n",
    "    # Final prompt\n",
    "    prompt = instruction + context + query_part\n",
    "\n",
    "    # Call model\n",
    "    response = llm(prompt, max_tokens=max_output_tokens, temperature=0.1)\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n",
    "\n"
=======
    "import random\n",
    "\n",
    "def translate_query(query):\n",
    "    #prompt = f\"### Instruction:\\nRephrase the following ESGish query into a natural English sentence that begins with 'Return all occurrences of'.\\n\\n{query}\\n\\n### Response:\"\n",
    "    #Changes up prompts each time to ensure variety in English conversions\n",
    "    PROMPTS = [\n",
    "        f\"### Instruction:\\nRephrase the following ESGish query into a natural English sentence that begins with 'Return all occurrences of'.\\n\\n{query}\\n\\n### Response:\",\n",
    "        f\"### Instruction:\\nRephrase the following ESGish query into a natural English sentence that begins with 'Find where'.\\n\\n{query}\\n\\n### Response:\",\n",
    "        f\"### Instruction:\\nRephrase the following ESGish query into a natural English sentence that begins with 'Return issuers that'.\\n\\n{query}\\n\\n### Response:\",\n",
    "        f\"### Instruction:\\nRephrase the following ESGish query into a natural English sentence.\\n\\n{query}\\n\\n### Response:\",\n",
    "        f\"### Instruction:\\nRephrase the following ESGish query into a natural English sentence that begins with 'Return'.\\n\\n{query}\\n\\n### Response:\"\n",
    "    ]\n",
    "    #Error handling \n",
    "    try:\n",
    "        #Chooses a random prompt from the array, queries a response from Mistral, returns response\n",
    "        prompt = random.choice(PROMPTS).format(query=query)\n",
    "        response = llm(prompt, max_tokens=575, temperature=0.1)\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating query: '{query}'\\nError: {str(e)}\")\n",
    "        return \"[TRANSLATION_FAILED]\""
>>>>>>> 1b082938f708d7b70008a355a076f30a20f6b6e3
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
   "id": "81772e84-bbeb-4000-a351-281347bf87a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =   12848.36 ms /   175 tokens (   73.42 ms per token,    13.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6017.56 ms /    44 runs   (  136.76 ms per token,     7.31 tokens per second)\n",
      "llama_perf_context_print:       total time =   18874.01 ms /   219 tokens\n",
      "Llama.generate: 164 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     297.39 ms /    12 tokens (   24.78 ms per token,    40.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5915.26 ms /    43 runs   (  137.56 ms per token,     7.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    6219.35 ms /    55 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2642.09 ms /   116 tokens (   22.78 ms per token,    43.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5771.13 ms /    42 runs   (  137.41 ms per token,     7.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    8419.89 ms /   158 tokens\n",
      "Llama.generate: 55 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3033.40 ms /   136 tokens (   22.30 ms per token,    44.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2567.23 ms /    19 runs   (  135.12 ms per token,     7.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    5603.52 ms /   155 tokens\n",
      "Llama.generate: 183 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     214.66 ms /     8 tokens (   26.83 ms per token,    37.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1884.85 ms /    14 runs   (  134.63 ms per token,     7.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    2101.57 ms /    22 tokens\n",
      "Llama.generate: 185 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     247.63 ms /     9 tokens (   27.51 ms per token,    36.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3245.40 ms /    24 runs   (  135.22 ms per token,     7.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    3496.59 ms /    33 tokens\n",
      "Llama.generate: 188 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     207.47 ms /     8 tokens (   25.93 ms per token,    38.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4338.77 ms /    32 runs   (  135.59 ms per token,     7.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    4550.96 ms /    40 tokens\n",
      "Llama.generate: 183 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     192.84 ms /     7 tokens (   27.55 ms per token,    36.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2295.07 ms /    17 runs   (  135.00 ms per token,     7.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    2490.42 ms /    24 tokens\n",
      "Llama.generate: 55 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3030.04 ms /   130 tokens (   23.31 ms per token,    42.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4050.29 ms /    29 runs   (  139.67 ms per token,     7.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    7085.15 ms /   159 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2862.38 ms /   121 tokens (   23.66 ms per token,    42.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2806.03 ms /    20 runs   (  140.30 ms per token,     7.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    5671.57 ms /   141 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2438.71 ms /   108 tokens (   22.58 ms per token,    44.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6080.04 ms /    44 runs   (  138.18 ms per token,     7.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    8525.91 ms /   152 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1058.28 ms /    47 tokens (   22.52 ms per token,    44.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1555.14 ms /    11 runs   (  141.38 ms per token,     7.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    2615.18 ms /    58 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1899.58 ms /    79 tokens (   24.05 ms per token,    41.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4236.43 ms /    29 runs   (  146.08 ms per token,     6.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    6140.61 ms /   108 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3116.04 ms /   130 tokens (   23.97 ms per token,    41.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1738.73 ms /    12 runs   (  144.89 ms per token,     6.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    4856.87 ms /   142 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1445.36 ms /    60 tokens (   24.09 ms per token,    41.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2776.21 ms /    19 runs   (  146.12 ms per token,     6.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    4224.64 ms /    79 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     718.65 ms /    29 tokens (   24.78 ms per token,    40.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1668.09 ms /    12 runs   (  139.01 ms per token,     7.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    2388.80 ms /    41 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2348.97 ms /   105 tokens (   22.37 ms per token,    44.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4687.04 ms /    34 runs   (  137.85 ms per token,     7.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    7041.36 ms /   139 tokens\n",
      "Llama.generate: 143 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     312.66 ms /    12 tokens (   26.06 ms per token,    38.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3233.36 ms /    23 runs   (  140.58 ms per token,     7.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    3549.61 ms /    35 tokens\n",
      "Llama.generate: 147 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     199.44 ms /     7 tokens (   28.49 ms per token,    35.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3407.38 ms /    24 runs   (  141.97 ms per token,     7.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    3610.56 ms /    31 tokens\n",
      "Llama.generate: 143 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     278.85 ms /    11 tokens (   25.35 ms per token,    39.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2835.32 ms /    20 runs   (  141.77 ms per token,     7.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    3117.40 ms /    31 tokens\n",
      "Llama.generate: 54 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2286.10 ms /   100 tokens (   22.86 ms per token,    43.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3176.50 ms /    23 runs   (  138.11 ms per token,     7.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    5466.34 ms /   123 tokens\n",
      "Llama.generate: 143 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     302.96 ms /    12 tokens (   25.25 ms per token,    39.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3051.72 ms /    22 runs   (  138.71 ms per token,     7.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    3358.20 ms /    34 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2312.22 ms /   104 tokens (   22.23 ms per token,    44.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4447.38 ms /    32 runs   (  138.98 ms per token,     7.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    6764.93 ms /   136 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    5082.02 ms /   227 tokens (   22.39 ms per token,    44.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2780.07 ms /    20 runs   (  139.00 ms per token,     7.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    7865.30 ms /   247 tokens\n",
      "Llama.generate: 273 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     294.16 ms /    11 tokens (   26.74 ms per token,    37.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3983.32 ms /    28 runs   (  142.26 ms per token,     7.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    4281.87 ms /    39 tokens\n",
      "Llama.generate: 271 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     219.45 ms /     8 tokens (   27.43 ms per token,    36.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6242.40 ms /    43 runs   (  145.17 ms per token,     6.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    6469.11 ms /    51 tokens\n",
      "Llama.generate: 273 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     273.41 ms /    11 tokens (   24.86 ms per token,    40.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3954.90 ms /    28 runs   (  141.25 ms per token,     7.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    4232.83 ms /    39 tokens\n",
      "Llama.generate: 274 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     194.49 ms /     7 tokens (   27.78 ms per token,    35.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3244.69 ms /    23 runs   (  141.07 ms per token,     7.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    3442.91 ms /    30 tokens\n",
      "Llama.generate: 271 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     198.47 ms /     7 tokens (   28.35 ms per token,    35.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2539.57 ms /    18 runs   (  141.09 ms per token,     7.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    2740.94 ms /    25 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1859.08 ms /    82 tokens (   22.67 ms per token,    44.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4191.18 ms /    29 runs   (  144.52 ms per token,     6.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    6055.07 ms /   111 tokens\n",
      "Llama.generate: 122 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     373.72 ms /    14 tokens (   26.69 ms per token,    37.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3658.67 ms /    24 runs   (  152.44 ms per token,     6.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    4036.34 ms /    38 tokens\n",
      "Llama.generate: 120 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1510.73 ms /    61 tokens (   24.77 ms per token,    40.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5487.82 ms /    37 runs   (  148.32 ms per token,     6.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    7004.61 ms /    98 tokens\n",
      "Llama.generate: 123 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1233.22 ms /    52 tokens (   23.72 ms per token,    42.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10376.01 ms /    74 runs   (  140.22 ms per token,     7.13 tokens per second)\n",
      "llama_perf_context_print:       total time =   11622.32 ms /   126 tokens\n",
      "Llama.generate: 155 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     335.68 ms /    13 tokens (   25.82 ms per token,    38.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9165.94 ms /    65 runs   (  141.01 ms per token,     7.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    9513.15 ms /    78 tokens\n",
      "Llama.generate: 123 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     888.80 ms /    37 tokens (   24.02 ms per token,    41.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8042.50 ms /    56 runs   (  143.62 ms per token,     6.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    8941.18 ms /    93 tokens\n",
      "Llama.generate: 122 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     668.36 ms /    29 tokens (   23.05 ms per token,    43.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6937.28 ms /    50 runs   (  138.75 ms per token,     7.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    7614.18 ms /    79 tokens\n",
      "Llama.generate: 122 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     463.54 ms /    19 tokens (   24.40 ms per token,    40.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6663.75 ms /    48 runs   (  138.83 ms per token,     7.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    7135.20 ms /    67 tokens\n",
      "Llama.generate: 135 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     926.21 ms /    40 tokens (   23.16 ms per token,    43.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3927.72 ms /    28 runs   (  140.28 ms per token,     7.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    4858.51 ms /    68 tokens\n",
      "Llama.generate: 123 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     328.50 ms /    13 tokens (   25.27 ms per token,    39.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3899.11 ms /    28 runs   (  139.25 ms per token,     7.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    4232.12 ms /    41 tokens\n",
      "Llama.generate: 120 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     231.60 ms /     9 tokens (   25.73 ms per token,    38.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2382.57 ms /    17 runs   (  140.15 ms per token,     7.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    2617.17 ms /    26 tokens\n",
      "Llama.generate: 54 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1856.63 ms /    78 tokens (   23.80 ms per token,    42.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3292.88 ms /    22 runs   (  149.68 ms per token,     6.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    5153.37 ms /   100 tokens\n",
      "Llama.generate: 120 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     308.77 ms /    12 tokens (   25.73 ms per token,    38.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3160.02 ms /    22 runs   (  143.64 ms per token,     6.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    3472.39 ms /    34 tokens\n",
      "Llama.generate: 124 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     214.32 ms /     7 tokens (   30.62 ms per token,    32.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2800.38 ms /    20 runs   (  140.02 ms per token,     7.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    3017.87 ms /    27 tokens\n",
      "Llama.generate: 120 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     291.90 ms /    12 tokens (   24.32 ms per token,    41.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3067.91 ms /    22 runs   (  139.45 ms per token,     7.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    3363.23 ms /    34 tokens\n",
      "Llama.generate: 124 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     192.64 ms /     7 tokens (   27.52 ms per token,    36.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2900.77 ms /    21 runs   (  138.13 ms per token,     7.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    3096.69 ms /    28 tokens\n",
      "Llama.generate: 54 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1766.97 ms /    78 tokens (   22.65 ms per token,    44.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3757.40 ms /    27 runs   (  139.16 ms per token,     7.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    5528.68 ms /   105 tokens\n",
      "Llama.generate: 120 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     300.78 ms /    12 tokens (   25.06 ms per token,    39.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3325.09 ms /    24 runs   (  138.55 ms per token,     7.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    3629.67 ms /    36 tokens\n",
      "Llama.generate: 124 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     193.17 ms /     7 tokens (   27.60 ms per token,    36.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3501.79 ms /    25 runs   (  140.07 ms per token,     7.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    3698.90 ms /    32 tokens\n",
      "Llama.generate: 123 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     175.61 ms /     6 tokens (   29.27 ms per token,    34.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3485.76 ms /    25 runs   (  139.43 ms per token,     7.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    3665.33 ms /    31 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2693.97 ms /   120 tokens (   22.45 ms per token,    44.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4207.28 ms /    30 runs   (  140.24 ms per token,     7.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    6906.20 ms /   150 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2238.36 ms /    99 tokens (   22.61 ms per token,    44.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3754.72 ms /    27 runs   (  139.06 ms per token,     7.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    5997.38 ms /   126 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    4400.41 ms /   195 tokens (   22.57 ms per token,    44.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7412.88 ms /    53 runs   (  139.87 ms per token,     7.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   11822.60 ms /   248 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 151 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3378.45 ms /   151 tokens (   22.37 ms per token,    44.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4477.17 ms /    32 runs   (  139.91 ms per token,     7.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    7860.87 ms /   183 tokens\n",
      "Llama.generate: 184 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     255.87 ms /    10 tokens (   25.59 ms per token,    39.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36859.37 ms /   255 runs   (  144.55 ms per token,     6.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   37191.70 ms /   265 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2291.64 ms /    93 tokens (   24.64 ms per token,    40.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3190.15 ms /    22 runs   (  145.01 ms per token,     6.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    5485.65 ms /   115 tokens\n",
      "Llama.generate: 140 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     332.13 ms /    10 tokens (   33.21 ms per token,    30.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3213.95 ms /    22 runs   (  146.09 ms per token,     6.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    3549.70 ms /    32 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2229.96 ms /    91 tokens (   24.51 ms per token,    40.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2365.74 ms /    17 runs   (  139.16 ms per token,     7.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    4598.44 ms /   108 tokens\n",
      "Llama.generate: 135 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     250.61 ms /    10 tokens (   25.06 ms per token,    39.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3475.28 ms /    25 runs   (  139.01 ms per token,     7.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    3729.74 ms /    35 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2036.55 ms /    91 tokens (   22.38 ms per token,    44.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3721.84 ms /    26 runs   (  143.15 ms per token,     6.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    5762.72 ms /   117 tokens\n",
      "Llama.generate: 124 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     405.33 ms /    15 tokens (   27.02 ms per token,    37.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3166.27 ms /    22 runs   (  143.92 ms per token,     6.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    3575.28 ms /    37 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    4849.74 ms /   212 tokens (   22.88 ms per token,    43.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4131.94 ms /    29 runs   (  142.48 ms per token,     7.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    8986.71 ms /   241 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3134.85 ms /   136 tokens (   23.05 ms per token,    43.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3214.15 ms /    23 runs   (  139.75 ms per token,     7.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    6352.72 ms /   159 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    5097.86 ms /   212 tokens (   24.05 ms per token,    41.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4359.45 ms /    31 runs   (  140.63 ms per token,     7.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    9462.43 ms /   243 tokens\n",
      "Llama.generate: 237 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1854.86 ms /    81 tokens (   22.90 ms per token,    43.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14662.84 ms /   106 runs   (  138.33 ms per token,     7.23 tokens per second)\n",
      "llama_perf_context_print:       total time =   16538.31 ms /   187 tokens\n",
      "Llama.generate: 55 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1950.18 ms /    86 tokens (   22.68 ms per token,    44.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4930.96 ms /    35 runs   (  140.88 ms per token,     7.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    6886.79 ms /   121 tokens\n",
      "Llama.generate: 55 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2333.71 ms /   103 tokens (   22.66 ms per token,    44.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3582.37 ms /    23 runs   (  155.76 ms per token,     6.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    5919.92 ms /   126 tokens\n",
      "Llama.generate: 149 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     255.48 ms /     9 tokens (   28.39 ms per token,    35.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3500.30 ms /    25 runs   (  140.01 ms per token,     7.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    3759.75 ms /    34 tokens\n",
      "Llama.generate: 54 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2561.23 ms /   114 tokens (   22.47 ms per token,    44.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4172.90 ms /    30 runs   (  139.10 ms per token,     7.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    6738.83 ms /   144 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2633.41 ms /   118 tokens (   22.32 ms per token,    44.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3608.07 ms /    26 runs   (  138.77 ms per token,     7.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    6245.80 ms /   144 tokens\n",
      "Llama.generate: 159 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     230.84 ms /     9 tokens (   25.65 ms per token,    38.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3598.07 ms /    26 runs   (  138.39 ms per token,     7.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    3833.05 ms /    35 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1434.58 ms /    64 tokens (   22.42 ms per token,    44.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3034.44 ms /    22 runs   (  137.93 ms per token,     7.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4472.45 ms /    86 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1951.55 ms /    87 tokens (   22.43 ms per token,    44.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3725.55 ms /    27 runs   (  137.98 ms per token,     7.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    5681.29 ms /   114 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1572.30 ms /    70 tokens (   22.46 ms per token,    44.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2498.86 ms /    18 runs   (  138.83 ms per token,     7.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    4074.25 ms /    88 tokens\n",
      "Llama.generate: 113 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     173.97 ms /     6 tokens (   29.00 ms per token,    34.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2210.88 ms /    16 runs   (  138.18 ms per token,     7.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    2387.40 ms /    22 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 53 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1195.73 ms /    53 tokens (   22.56 ms per token,    44.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2347.08 ms /    17 runs   (  138.06 ms per token,     7.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    3545.41 ms /    70 tokens\n",
      "Llama.generate: 95 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     229.13 ms /     9 tokens (   25.46 ms per token,    39.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3021.83 ms /    22 runs   (  137.36 ms per token,     7.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    3254.32 ms /    31 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1469.59 ms /    65 tokens (   22.61 ms per token,    44.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3207.07 ms /    23 runs   (  139.44 ms per token,     7.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    4680.39 ms /    88 tokens\n",
      "Llama.generate: 109 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     230.13 ms /     9 tokens (   25.57 ms per token,    39.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3875.61 ms /    28 runs   (  138.41 ms per token,     7.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    4110.10 ms /    37 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1706.15 ms /    75 tokens (   22.75 ms per token,    43.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4037.16 ms /    29 runs   (  139.21 ms per token,     7.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    5747.98 ms /   104 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1217.85 ms /    54 tokens (   22.55 ms per token,    44.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2489.89 ms /    18 runs   (  138.33 ms per token,     7.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    3710.56 ms /    72 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 167 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3728.54 ms /   167 tokens (   22.33 ms per token,    44.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2630.82 ms /    19 runs   (  138.46 ms per token,     7.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    6362.48 ms /   186 tokens\n",
      "Llama.generate: 208 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     231.46 ms /     9 tokens (   25.72 ms per token,    38.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2494.66 ms /    18 runs   (  138.59 ms per token,     7.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    2728.90 ms /    27 tokens\n",
      "Llama.generate: 210 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     252.94 ms /    10 tokens (   25.29 ms per token,    39.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3473.09 ms /    25 runs   (  138.92 ms per token,     7.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    3729.83 ms /    35 tokens\n",
      "Llama.generate: 214 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     220.83 ms /     8 tokens (   27.60 ms per token,    36.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3496.89 ms /    25 runs   (  139.88 ms per token,     7.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    3721.68 ms /    33 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1723.81 ms /    77 tokens (   22.39 ms per token,    44.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2368.35 ms /    17 runs   (  139.31 ms per token,     7.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    4094.85 ms /    94 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2524.96 ms /   113 tokens (   22.34 ms per token,    44.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3037.44 ms /    22 runs   (  138.07 ms per token,     7.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    5565.86 ms /   135 tokens\n",
      "Llama.generate: 157 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     232.29 ms /     9 tokens (   25.81 ms per token,    38.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3310.69 ms /    24 runs   (  137.95 ms per token,     7.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    3546.73 ms /    33 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2676.03 ms /   120 tokens (   22.30 ms per token,    44.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2649.49 ms /    19 runs   (  139.45 ms per token,     7.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    5328.54 ms /   139 tokens\n",
      "Llama.generate: 167 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     211.56 ms /     8 tokens (   26.44 ms per token,    37.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3182.45 ms /    23 runs   (  138.37 ms per token,     7.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    3397.53 ms /    31 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2139.43 ms /    96 tokens (   22.29 ms per token,    44.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4573.47 ms /    33 runs   (  138.59 ms per token,     7.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    6718.35 ms /   129 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1680.74 ms /    75 tokens (   22.41 ms per token,    44.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4547.81 ms /    33 runs   (  137.81 ms per token,     7.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    6233.79 ms /   108 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3128.11 ms /   140 tokens (   22.34 ms per token,    44.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2208.69 ms /    16 runs   (  138.04 ms per token,     7.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    5339.45 ms /   156 tokens\n",
      "Llama.generate: 185 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     211.75 ms /     8 tokens (   26.47 ms per token,    37.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2509.70 ms /    18 runs   (  139.43 ms per token,     7.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    2724.33 ms /    26 tokens\n",
      "Llama.generate: 187 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     231.52 ms /     9 tokens (   25.72 ms per token,    38.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2498.26 ms /    18 runs   (  138.79 ms per token,     7.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    2732.58 ms /    27 tokens\n",
      "Llama.generate: 190 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     212.46 ms /     8 tokens (   26.56 ms per token,    37.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3599.16 ms /    26 runs   (  138.43 ms per token,     7.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    3815.74 ms /    34 tokens\n",
      "Llama.generate: 185 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     192.35 ms /     7 tokens (   27.48 ms per token,    36.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3178.59 ms /    23 runs   (  138.20 ms per token,     7.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    3374.56 ms /    30 tokens\n",
      "Llama.generate: 183 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     377.77 ms /    16 tokens (   23.61 ms per token,    42.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3318.47 ms /    24 runs   (  138.27 ms per token,     7.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    3699.93 ms /    40 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 109 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2482.33 ms /   109 tokens (   22.77 ms per token,    43.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4611.62 ms /    33 runs   (  139.75 ms per token,     7.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    7099.21 ms /   142 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2365.72 ms /   106 tokens (   22.32 ms per token,    44.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4827.58 ms /    35 runs   (  137.93 ms per token,     7.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    7199.09 ms /   141 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 253 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    5663.82 ms /   253 tokens (   22.39 ms per token,    44.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14741.56 ms /   106 runs   (  139.07 ms per token,     7.19 tokens per second)\n",
      "llama_perf_context_print:       total time =   20426.34 ms /   359 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 132 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 1 to translated_queries_TEST.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2977.08 ms /   132 tokens (   22.55 ms per token,    44.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4365.93 ms /    31 runs   (  140.84 ms per token,     7.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    7348.39 ms /   163 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2002.07 ms /    88 tokens (   22.75 ms per token,    43.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5764.11 ms /    41 runs   (  140.59 ms per token,     7.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    7773.04 ms /   129 tokens\n",
      "Llama.generate: 107 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1078.13 ms /    47 tokens (   22.94 ms per token,    43.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7402.84 ms /    53 runs   (  139.68 ms per token,     7.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    8490.01 ms /   100 tokens\n",
      "Llama.generate: 111 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     925.43 ms /    40 tokens (   23.14 ms per token,    43.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7068.14 ms /    51 runs   (  138.59 ms per token,     7.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    8002.17 ms /    91 tokens\n",
      "Llama.generate: 116 prefix-match hit, remaining 156 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3544.40 ms /   156 tokens (   22.72 ms per token,    44.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23131.06 ms /   166 runs   (  139.34 ms per token,     7.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   26716.61 ms /   322 tokens\n",
      "Llama.generate: 127 prefix-match hit, remaining 425 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    9935.58 ms /   425 tokens (   23.38 ms per token,    42.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36106.39 ms /   255 runs   (  141.59 ms per token,     7.06 tokens per second)\n",
      "llama_perf_context_print:       total time =   46124.00 ms /   680 tokens\n",
      "Llama.generate: 107 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     271.28 ms /    10 tokens (   27.13 ms per token,    36.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2889.38 ms /    21 runs   (  137.59 ms per token,     7.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3164.06 ms /    31 tokens\n",
      "Llama.generate: 108 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     953.63 ms /    42 tokens (   22.71 ms per token,    44.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6900.24 ms /    50 runs   (  138.00 ms per token,     7.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    7862.18 ms /    92 tokens\n",
      "Llama.generate: 107 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1135.76 ms /    50 tokens (   22.72 ms per token,    44.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7491.47 ms /    54 runs   (  138.73 ms per token,     7.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    8636.63 ms /   104 tokens\n",
      "Llama.generate: 107 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     574.22 ms /    24 tokens (   23.93 ms per token,    41.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4840.62 ms /    35 runs   (  138.30 ms per token,     7.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    5420.44 ms /    59 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1181.47 ms /    52 tokens (   22.72 ms per token,    44.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4404.70 ms /    32 runs   (  137.65 ms per token,     7.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    5591.36 ms /    84 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 148 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3310.12 ms /   148 tokens (   22.37 ms per token,    44.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5937.25 ms /    43 runs   (  138.08 ms per token,     7.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    9254.70 ms /   191 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2764.70 ms /   124 tokens (   22.30 ms per token,    44.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5554.21 ms /    40 runs   (  138.86 ms per token,     7.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    8325.66 ms /   164 tokens\n",
      "Llama.generate: 167 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     292.92 ms /    12 tokens (   24.41 ms per token,    40.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4278.76 ms /    31 runs   (  138.02 ms per token,     7.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4576.51 ms /    43 tokens\n",
      "Llama.generate: 171 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     192.34 ms /     7 tokens (   27.48 ms per token,    36.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4429.16 ms /    32 runs   (  138.41 ms per token,     7.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    4626.58 ms /    39 tokens\n",
      "Llama.generate: 170 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     175.42 ms /     6 tokens (   29.24 ms per token,    34.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4282.08 ms /    31 runs   (  138.13 ms per token,     7.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    4462.64 ms /    37 tokens\n",
      "Llama.generate: 167 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =     293.00 ms /    12 tokens (   24.42 ms per token,    40.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4966.13 ms /    36 runs   (  137.95 ms per token,     7.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    5264.94 ms /    48 tokens\n",
      "Llama.generate: 58 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2713.03 ms /   121 tokens (   22.42 ms per token,    44.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5256.20 ms /    38 runs   (  138.32 ms per token,     7.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    7975.45 ms /   159 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 235 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    5265.41 ms /   235 tokens (   22.41 ms per token,    44.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17237.78 ms /   124 runs   (  139.01 ms per token,     7.19 tokens per second)\n",
      "llama_perf_context_print:       total time =   22530.91 ms /   359 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3324.11 ms /   149 tokens (   22.31 ms per token,    44.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5123.85 ms /    37 runs   (  138.48 ms per token,     7.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    8454.15 ms /   186 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3015.68 ms /   135 tokens (   22.34 ms per token,    44.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4996.72 ms /    36 runs   (  138.80 ms per token,     7.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    8018.30 ms /   171 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    5680.35 ms /   254 tokens (   22.36 ms per token,    44.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4019.32 ms /    29 runs   (  138.60 ms per token,     7.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    9704.63 ms /   283 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 311 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    7001.65 ms /   311 tokens (   22.51 ms per token,    44.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4456.59 ms /    32 runs   (  139.27 ms per token,     7.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   11463.95 ms /   343 tokens\n",
      "Llama.generate: 196 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1965.87 ms /    86 tokens (   22.86 ms per token,    43.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5020.94 ms /    36 runs   (  139.47 ms per token,     7.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    6993.03 ms /   122 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 372 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    8430.15 ms /   372 tokens (   22.66 ms per token,    44.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3904.29 ms /    28 runs   (  139.44 ms per token,     7.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   12339.33 ms /   400 tokens\n",
      "Llama.generate: 273 prefix-match hit, remaining 150 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3398.02 ms /   150 tokens (   22.65 ms per token,    44.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4890.30 ms /    35 runs   (  139.72 ms per token,     7.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    8294.21 ms /   185 tokens\n",
      "Llama.generate: 273 prefix-match hit, remaining 1178 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =   28249.75 ms /  1178 tokens (   23.98 ms per token,    41.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9870.94 ms /    68 runs   (  145.16 ms per token,     6.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   38133.19 ms /  1246 tokens\n",
      "Llama.generate: 273 prefix-match hit, remaining 1306 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =   31475.06 ms /  1306 tokens (   24.10 ms per token,    41.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37378.44 ms /   255 runs   (  146.58 ms per token,     6.82 tokens per second)\n",
      "llama_perf_context_print:       total time =   68935.90 ms /  1561 tokens\n",
      "Llama.generate: 273 prefix-match hit, remaining 529 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =   12450.73 ms /   529 tokens (   23.54 ms per token,    42.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10700.07 ms /    75 runs   (  142.67 ms per token,     7.01 tokens per second)\n",
      "llama_perf_context_print:       total time =   23165.16 ms /   604 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 1514 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =   37061.80 ms /  1514 tokens (   24.48 ms per token,    40.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35587.38 ms /   244 runs   (  145.85 ms per token,     6.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   72725.97 ms /  1758 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 305 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    6763.90 ms /   305 tokens (   22.18 ms per token,    45.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7978.49 ms /    58 runs   (  137.56 ms per token,     7.27 tokens per second)\n",
      "llama_perf_context_print:       total time =   14752.93 ms /   363 tokens\n",
      "Llama.generate: 129 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2388.47 ms /   108 tokens (   22.12 ms per token,    45.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6828.83 ms /    50 runs   (  136.58 ms per token,     7.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    9225.79 ms /   158 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    5260.40 ms /   239 tokens (   22.01 ms per token,    45.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5455.28 ms /    40 runs   (  136.38 ms per token,     7.33 tokens per second)\n",
      "llama_perf_context_print:       total time =   10722.62 ms /   279 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 834 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =   18934.58 ms /   834 tokens (   22.70 ms per token,    44.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13158.34 ms /    93 runs   (  141.49 ms per token,     7.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   32111.70 ms /   927 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 547 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =   12389.21 ms /   547 tokens (   22.65 ms per token,    44.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11975.82 ms /    86 runs   (  139.25 ms per token,     7.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   24382.02 ms /   633 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 281 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    6255.26 ms /   281 tokens (   22.26 ms per token,    44.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8078.98 ms /    59 runs   (  136.93 ms per token,     7.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   14344.57 ms /   340 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 1628 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =   38315.69 ms /  1628 tokens (   23.54 ms per token,    42.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37297.31 ms /   255 runs   (  146.26 ms per token,     6.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   75694.31 ms /  1883 tokens\n",
      "Llama.generate: 1243 prefix-match hit, remaining 266 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    6414.94 ms /   266 tokens (   24.12 ms per token,    41.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36982.88 ms /   255 runs   (  145.03 ms per token,     6.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   43478.50 ms /   521 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 411 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    9145.19 ms /   411 tokens (   22.25 ms per token,    44.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10082.78 ms /    73 runs   (  138.12 ms per token,     7.24 tokens per second)\n",
      "llama_perf_context_print:       total time =   19241.88 ms /   484 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 244 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    5370.02 ms /   244 tokens (   22.01 ms per token,    45.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8498.03 ms /    62 runs   (  137.06 ms per token,     7.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   13879.18 ms /   306 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 336 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    7462.77 ms /   336 tokens (   22.21 ms per token,    45.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7282.02 ms /    53 runs   (  137.40 ms per token,     7.28 tokens per second)\n",
      "llama_perf_context_print:       total time =   14754.06 ms /   389 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    5003.46 ms /   228 tokens (   21.94 ms per token,    45.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7952.71 ms /    58 runs   (  137.12 ms per token,     7.29 tokens per second)\n",
      "llama_perf_context_print:       total time =   12966.34 ms /   286 tokens\n",
      "Llama.generate: 109 prefix-match hit, remaining 343 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    7651.56 ms /   343 tokens (   22.31 ms per token,    44.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14245.34 ms /   103 runs   (  138.30 ms per token,     7.23 tokens per second)\n",
      "llama_perf_context_print:       total time =   21918.14 ms /   446 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 273 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    6020.53 ms /   273 tokens (   22.05 ms per token,    45.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13171.09 ms /    96 runs   (  137.20 ms per token,     7.29 tokens per second)\n",
      "llama_perf_context_print:       total time =   19210.72 ms /   369 tokens\n",
      "Llama.generate: 117 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1064.92 ms /    48 tokens (   22.19 ms per token,    45.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2595.22 ms /    19 runs   (  136.59 ms per token,     7.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3663.30 ms /    67 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2297.27 ms /   105 tokens (   21.88 ms per token,    45.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4743.25 ms /    35 runs   (  135.52 ms per token,     7.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    7046.11 ms /   140 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3643.41 ms /   166 tokens (   21.95 ms per token,    45.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5316.73 ms /    39 runs   (  136.33 ms per token,     7.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    8966.53 ms /   205 tokens\n",
      "Llama.generate: 104 prefix-match hit, remaining 148 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3280.91 ms /   148 tokens (   22.17 ms per token,    45.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9005.26 ms /    66 runs   (  136.44 ms per token,     7.33 tokens per second)\n",
      "llama_perf_context_print:       total time =   12298.08 ms /   214 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2580.78 ms /   118 tokens (   21.87 ms per token,    45.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4612.03 ms /    34 runs   (  135.65 ms per token,     7.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    7198.55 ms /   152 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 162 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3560.20 ms /   162 tokens (   21.98 ms per token,    45.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7098.03 ms /    52 runs   (  136.50 ms per token,     7.33 tokens per second)\n",
      "llama_perf_context_print:       total time =   10667.03 ms /   214 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    4668.14 ms /   211 tokens (   22.12 ms per token,    45.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5197.68 ms /    38 runs   (  136.78 ms per token,     7.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    9872.31 ms /   249 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    3851.00 ms /   176 tokens (   21.88 ms per token,    45.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6564.38 ms /    48 runs   (  136.76 ms per token,     7.31 tokens per second)\n",
      "llama_perf_context_print:       total time =   10423.42 ms /   224 tokens\n",
      "Llama.generate: 80 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    1808.41 ms /    82 tokens (   22.05 ms per token,    45.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3134.11 ms /    23 runs   (  136.27 ms per token,     7.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4946.22 ms /   105 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    4825.37 ms /   220 tokens (   21.93 ms per token,    45.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3142.90 ms /    23 runs   (  136.65 ms per token,     7.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    7972.12 ms /   243 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 1517 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =   35542.02 ms /  1517 tokens (   23.43 ms per token,    42.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9435.36 ms /    65 runs   (  145.16 ms per token,     6.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   44989.32 ms /  1582 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 255 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    5592.55 ms /   255 tokens (   21.93 ms per token,    45.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3975.36 ms /    29 runs   (  137.08 ms per token,     7.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    9572.92 ms /   284 tokens\n",
      "Llama.generate: 95 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2249.02 ms /   102 tokens (   22.05 ms per token,    45.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3403.54 ms /    25 runs   (  136.14 ms per token,     7.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    5656.48 ms /   127 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 881 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =   20121.84 ms /   881 tokens (   22.84 ms per token,    43.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12617.86 ms /    89 runs   (  141.77 ms per token,     7.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   32757.58 ms /   970 tokens\n",
      "Llama.generate: 406 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2029.27 ms /    90 tokens (   22.55 ms per token,    44.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6797.29 ms /    49 runs   (  138.72 ms per token,     7.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    8835.11 ms /   139 tokens\n",
      "Llama.generate: 406 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =   10309.55 ms /   450 tokens (   22.91 ms per token,    43.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10001.76 ms /    71 runs   (  140.87 ms per token,     7.10 tokens per second)\n",
      "llama_perf_context_print:       total time =   20324.67 ms /   521 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    4618.73 ms /   210 tokens (   21.99 ms per token,    45.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8875.16 ms /    65 runs   (  136.54 ms per token,     7.32 tokens per second)\n",
      "llama_perf_context_print:       total time =   13505.87 ms /   275 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 806 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =   18330.80 ms /   806 tokens (   22.74 ms per token,    43.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34924.56 ms /   246 runs   (  141.97 ms per token,     7.04 tokens per second)\n",
      "llama_perf_context_print:       total time =   53330.56 ms /  1052 tokens\n",
      "Llama.generate: 394 prefix-match hit, remaining 569 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =   13202.44 ms /   569 tokens (   23.20 ms per token,    43.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36467.80 ms /   255 runs   (  143.01 ms per token,     6.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   49751.58 ms /   824 tokens\n",
      "Llama.generate: 124 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2625.29 ms /   119 tokens (   22.06 ms per token,    45.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4912.39 ms /    36 runs   (  136.46 ms per token,     7.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    7543.69 ms /   155 tokens\n",
      "Llama.generate: 126 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =    2930.37 ms /   133 tokens (   22.03 ms per token,    45.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6288.41 ms /    46 runs   (  136.70 ms per token,     7.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    9226.55 ms /   179 tokens\n",
      "Llama.generate: 55 prefix-match hit, remaining 1671 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   12848.95 ms\n",
      "llama_perf_context_print: prompt eval time =   39307.42 ms /  1671 tokens (   23.52 ms per token,    42.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37264.88 ms /   255 runs   (  146.14 ms per token,     6.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   76654.54 ms /  1926 tokens\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m translated_batch = []\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     translated = \u001b[43mtranslate_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     translated_batch.append(translated)\n\u001b[32m     21\u001b[39m df_batch = pd.DataFrame({\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEsgish\u001b[39m\u001b[33m\"\u001b[39m: batch,\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEnglish\u001b[39m\u001b[33m\"\u001b[39m: translated_batch\n\u001b[32m     24\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mtranslate_query\u001b[39m\u001b[34m(query, max_total_tokens, max_output_tokens)\u001b[39m\n\u001b[32m     57\u001b[39m m_tokens = max_afforded_tokens(codes)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Build full context blocks for each code\u001b[39;00m\n\u001b[32m     60\u001b[39m context_blocks = [\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmax_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_lookup\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNo metadata found.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m code \u001b[38;5;129;01min\u001b[39;00m codes\n\u001b[32m     62\u001b[39m ]\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Initial prompt pieces\u001b[39;00m\n\u001b[32m     65\u001b[39m instruction = \u001b[33m\"\u001b[39m\u001b[33m### Instruction:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRephrase the following ESGish query into a concise natural English sentence. Each query is asking for all companies or issuers that match some paramater. Use the following metadata definitions for clarity:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mmax_tokens\u001b[39m\u001b[34m(text, max_tokens)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens) <= max_tokens:\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m truncated = llm.detokenize(\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m]\u001b[49m).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m, errors=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m truncated + \u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "81772e84-bbeb-4000-a351-281347bf87a7",
   "metadata": {},
   "outputs": [],
>>>>>>> 1b082938f708d7b70008a355a076f30a20f6b6e3
   "source": [
    "import pandas as pd\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "#Reads and stores the Esgish queries\n",
<<<<<<< HEAD
    "df = pd.read_excel(\"esgish_short500.xlsx\")\n",
=======
    "df = pd.read_excel(\"esgish_short.xlsx\")\n",
>>>>>>> 1b082938f708d7b70008a355a076f30a20f6b6e3
    "queries = df[\"Esgish\"].tolist()\n",
    "#Ensures no overload and efficiency\n",
    "batch_size = 100 \n",
    "output_file = \"translated_queries_TEST.xlsx\"\n",
    "\n",
    "#Looks at each query in each batch, calls the translate_query function, and stores it\n",
    "for i in range(0, len(queries), batch_size):\n",
    "    batch = queries[i:i + batch_size]\n",
    "    translated_batch = []\n",
    "    \n",
    "    for query in batch:\n",
    "        translated = translate_query(query)\n",
    "        translated_batch.append(translated)\n",
    "    \n",
    "    df_batch = pd.DataFrame({\n",
    "        \"Esgish\": batch,\n",
    "        \"English\": translated_batch\n",
    "    })\n",
    "\n",
    "    #Makes a new file if needed, or adds onto the current file during each batch in case the program crashes at some point\n",
    "    if i == 0:\n",
    "        df_batch.to_excel(output_file, index=False)  \n",
    "    else:\n",
    "        with pd.ExcelWriter(output_file, mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"overlay\") as writer:\n",
    "            df_batch.to_excel(writer, index=False, header=False, startrow=i + 1)\n",
    "    \n",
<<<<<<< HEAD
    "    print(f\"Saved batch {i // batch_size + 1} to {output_file}\")"
=======
    "    print(f\"Saved batch {i // batch_size + 1} to {output_file}\")\n",
    "    #Avoids overwhelming the system/API\n",
    "    time.sleep(10) "
>>>>>>> 1b082938f708d7b70008a355a076f30a20f6b6e3
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814e3a0-0ddd-467e-8074-18a33f13a0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f42872-6939-4c34-b74f-f177f8eb3d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3",
=======
   "display_name": "myenv",
>>>>>>> 1b082938f708d7b70008a355a076f30a20f6b6e3
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.13.2"
=======
   "version": "3.11.11"
>>>>>>> 1b082938f708d7b70008a355a076f30a20f6b6e3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
