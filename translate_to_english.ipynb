{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11329b8f-ae0a-48d4-9da9-7c1c06875c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas openpyxl transformers torch\n",
    "%pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a607fdc-3c30-4006-af0d-60789c816fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "model_path = \"/Users/pierr/Desktop/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "max_context = 4096\n",
    "llm = Llama(\n",
    "    model_path,\n",
    "    n_ctx=max_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4566bf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest entry code: EUTaxManIntSerElcRevOverAlign\n",
      "Length: 1353\n",
      "Content: EU Taxon - Man, Instal, Serv of Elec Equip Overall Align. EU Taxon - Man, Instal, Serv of Elec Equip Overall Align: This factor identifies the overall alignment for Manufacture, installation, and servicing of high, medium and low voltage electrical equipment for electrical transmission and distribution that result in or enable a substantial contribution to climate change mitigation, covering the substantial contribution criteria, the do no significant harm criteria, and the minimum social safeguards. This is the aggregated result across all Taxonomy objectives. The possible values are: Aligned,Aligned (>90%),Aligned (>80%),Aligned (>70%),Aligned (>60%),Aligned (>50%),Aligned (>40%),Aligned (>30%),Aligned (>20%),Aligned (>10%),Aligned (>0%),Likely aligned (100%),Likely aligned (>90%),Likely aligned (>80%),Likely aligned (>70%),Likely aligned (>60%),Likely aligned (>50%),Likely aligned (>40%),Likely aligned (>30%),Likely aligned (>20%),Likely aligned (>10%),Likely aligned (>0%),Potentially aligned (100%),Potentially aligned (>90%),Potentially aligned (>80%),Potentially aligned (>70%),Potentially aligned (>60%),Potentially aligned (>50%),Potentially aligned (>40%),Potentially aligned (>30%),Potentially aligned (>20%),Potentially aligned (>10%),Potentially aligned (>0%),Likely not aligned,Not aligned, Not collected, and Not applicable.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open(\"metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_metadata = json.load(f)\n",
    "\n",
    "# Create a lookup table\n",
    "metadata_lookup = {\n",
    "    entry[\"code\"]: f'{entry[\"name\"]}. {entry[\"description\"]}' \n",
    "    for entry in raw_metadata\n",
    "}\n",
    "\n",
    "def extract_codes(query):\n",
    "    return re.findall(r\"\\[([A-Za-z0-9_]+)\\]\", query)\n",
    "\n",
    "list_keywords = {\"IN\", \"ANY\", \"NONE\"}\n",
    "null_keywords = {\":NC\": \"Null type \\\"Not collected\\\"\", \":NA\": \"Null type \\\"Not applicable\\\"\", \":ND\": \"Null type \\\"Not disclosed\\\"\", \":NI\": \"Null type \\\"No information\\\"\", \":NM\": \"Null type \\\"Not meaningful\\\"\"}\n",
    "def extract_nulls_and_lists(query):\n",
    "    nulls = []\n",
    "    lists = []\n",
    "    contains_null = re.findall(r\"\\[([A-Za-z0-9_]+)\\]\\s+IS\\s+(:[A-Z]{2})\", query)\n",
    "    for code, null_keyword in contains_null:\n",
    "        nulls.append((code, null_keyword))\n",
    "    \n",
    "    contains_list = re.findall(r\"\\[([A-Za-z0-9_]+)\\]\\s+(IN|ANY|NONE)\\b\", query, re.IGNORECASE)\n",
    "    for code, list_keyword in contains_list:\n",
    "        lists.append((code, list_keyword.upper()))\n",
    "    \n",
    "    return nulls, lists\n",
    "\n",
    "# Find the entry with the longest name + description combo\n",
    "longest_entry = max(metadata_lookup.items(), key=lambda item: len(item[1]))\n",
    "\n",
    "# Print the result\n",
    "print(f'Longest entry code: {longest_entry[0]}')\n",
    "print(f'Length: {len(longest_entry[1])}')\n",
    "print(f'Content: {longest_entry[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efb3fc4c-e7b9-4bfb-9340-779aaba31116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_tokens(text, max_tokens=40):\n",
    "    #moved this to use in below function\n",
    "    tokens = llm.tokenize(text.encode(\"utf-8\"))\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "    truncated = llm.detokenize(tokens[:max_tokens]).decode(\"utf-8\", errors=\"ignore\")\n",
    "    return truncated + \"...\"\n",
    "\n",
    "def build_ordered_context(query, token_budget):\n",
    "    #builds context in-order: null type definitions, enumerations for list types, and metadata lookups\n",
    "    context_lines = []\n",
    "    seen = set()\n",
    "    used_tokens = 0\n",
    "\n",
    "    null_hits, list_hits = extract_nulls_and_lists(query)\n",
    "    codes_in_order = re.findall(r\"\\[([A-Za-z0-9_]+)\\]\", query)\n",
    "\n",
    "    #tokenizer = llm.tokenizer\n",
    "    for code in codes_in_order:\n",
    "        if code in seen:\n",
    "            continue\n",
    "        seen.add(code)\n",
    "\n",
    "        null_entry = next((kw for c, kw in null_hits if c == code), None)\n",
    "        if null_entry:\n",
    "            null_def = null_keywords.get(null_entry, f\"No definition for {null_entry}\")\n",
    "            line = f\"{code} = {null_entry} â†’ {null_def}\"\n",
    "            tokens = len(llm.tokenize(line.encode(\"utf-8\")))\n",
    "            if used_tokens + tokens > token_budget:\n",
    "                break\n",
    "            context_lines.append(line)\n",
    "            used_tokens += tokens\n",
    "\n",
    "        base_meta = metadata_lookup.get(code, \"No metadata found.\")\n",
    "        metadata_line = f\"{code}: {max_tokens(base_meta, 100)}\" \n",
    "        tokens = len(llm.tokenize(metadata_line.encode(\"utf-8\")))\n",
    "        if used_tokens + tokens > token_budget:\n",
    "            break\n",
    "        context_lines.append(metadata_line)\n",
    "        used_tokens += tokens\n",
    "\n",
    "        if any(c == code for c, _ in list_hits):\n",
    "            enum_line = f\"{code} (enumeration): {max_tokens(base_meta, 100)}\"\n",
    "            tokens = len(llm.tokenize(enum_line.encode(\"utf-8\")))\n",
    "            if used_tokens + tokens > token_budget:\n",
    "                break\n",
    "            context_lines.append(enum_line)\n",
    "            used_tokens += tokens\n",
    "\n",
    "    return \"\\n\".join(context_lines)\n",
    "\n",
    "def max_afforded_tokens(codes):\n",
    "    return max(4096 // max(1, len(codes)), 100)\n",
    "\n",
    "\n",
    "def translate_query(query, max_total_tokens=2048, max_output_tokens=256):\n",
    "    codes = extract_codes(query)\n",
    "\n",
    "    m_tokens = max_afforded_tokens(codes)\n",
    "\n",
    "    # Initial prompt pieces\n",
    "    instruction = \"### Instruction:\\nRephrase the following ESGish query into a concise natural English sentence. Each query is asking for all companies or issuers that match some paramater. Use the following metadata definitions for clarity:\\n\\n\"\n",
    "    query_part = f\"\\n\\nQuery: {query}\\n\\n### Response:\"\n",
    "\n",
    "    # Tokenize instruction and query to calculate remaining token budget\n",
    "    #tokenizer = llm.tokenize  # Built-in tokenizer\n",
    "    instruction_tokens = len(llm.tokenize(instruction.encode(\"utf-8\")))\n",
    "    query_tokens = len(llm.tokenize(query_part.encode(\"utf-8\")))\n",
    "    token_budget = max_total_tokens - max_output_tokens - instruction_tokens - query_tokens\n",
    "\n",
    "    # Build full context blocks for each code\n",
    "    context = build_ordered_context(query, token_budget)\n",
    "    print(\"query: \", query)\n",
    "    print(context)\n",
    "\n",
    "    \"\"\"\n",
    "    # Now iteratively add context blocks until budget is exhausted\n",
    "    context = \"\"\n",
    "    used_tokens = 0\n",
    "    for block in context_blocks:\n",
    "        block_tokens = len(tokenizer(block.encode(\"utf-8\"))) + 1  # +1 for newline\n",
    "        if used_tokens + block_tokens <= token_budget:\n",
    "            context += block + \"\\n\"\n",
    "            used_tokens += block_tokens\n",
    "        else:\n",
    "            break  # stop once we're out of budget\n",
    "    \"\"\"\n",
    "    # Final prompt\n",
    "    prompt = instruction + context + query_part\n",
    "\n",
    "    # Call model\n",
    "    response = llm(prompt, max_tokens=max_output_tokens, temperature=0.1)\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81772e84-bbeb-4000-a351-281347bf87a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "#Reads and stores the Esgish queries\n",
    "df = pd.read_excel(\"NullType_10Queries.xlsx\")\n",
    "queries = df[\"Esgish\"].tolist()\n",
    "\n",
    "#Ensures no overload and efficiency\n",
    "batch_size = 100 \n",
    "output_file = \"translated_queries_TEST.xlsx\"\n",
    "\n",
    "#Looks at each query in each batch, calls the translate_query function, and stores it\n",
    "for i in range(0, len(queries), batch_size):\n",
    "    batch = queries[i:i + batch_size]\n",
    "    translated_batch = []\n",
    "    \n",
    "    for query in batch:\n",
    "        translated = translate_query(query)\n",
    "        translated_batch.append(translated)\n",
    "    \n",
    "    df_batch = pd.DataFrame({\n",
    "        \"Esgish\": batch,\n",
    "        \"English\": translated_batch\n",
    "    })\n",
    "\n",
    "    #Makes a new file if needed, or adds onto the current file during each batch in case the program crashes at some point\n",
    "    if i == 0:\n",
    "        df_batch.to_excel(output_file, index=False)  \n",
    "    else:\n",
    "        with pd.ExcelWriter(output_file, mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"overlay\") as writer:\n",
    "            df_batch.to_excel(writer, index=False, header=False, startrow=i + 1)\n",
    "    \n",
    "    print(f\"Saved batch {i // batch_size + 1} to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814e3a0-0ddd-467e-8074-18a33f13a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_file = \"translated_queries_TEST5.xlsx\"\n",
    "\n",
    "# Extract the English column\n",
    "df = pd.read_excel(output_file)\n",
    "english_queries = df[\"English\"].tolist()\n",
    "\n",
    "# Function to request a comprehensibility rating from the model\n",
    "def rate_comprehensibility(text, max_tokens=256):\n",
    "    # Adjust the prompt to ask the model for a rating from 1 to 10\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Please rate the following text's comprehensibility from 1 to 10, where:\n",
    "- 1 = Completely incomprehensible, nonsensical, or full of errors.\n",
    "- 5 = Understandable with effort; some awkwardness, complexity, or minor errors.\n",
    "- 10 = Perfectly clear, natural, and easy to understand.\n",
    "\n",
    "Here are some examples:\n",
    "Text: \"asjdk asjd aksd\" â†’ Rating: 1\n",
    "Text: \"Provide list companies ESG data incomplete understandable\" â†’ Rating: 4\n",
    "Text: \"Please provide a list of companies with complete ESG data.\" â†’ Rating: 9\n",
    "\n",
    "Now, rate this text:\n",
    "Text: {text}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    # Call model (adjust temperature and other params as needed)\n",
    "    response = llm(prompt, max_tokens=max_tokens, temperature=0.2)\n",
    "    rating = response[\"choices\"][0][\"text\"].strip()\n",
    "    print(rating)\n",
    "    # Ensure the response is a valid number between 1 and 10\n",
    "    try:\n",
    "        rating = int(rating)\n",
    "        if 1 <= rating <= 10:\n",
    "            return rating\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return None  # Return None if no valid rating is obtained\n",
    "\n",
    "# List to store ratings\n",
    "ratings = []\n",
    "\n",
    "# Iterate through each English translation and get a rating\n",
    "for query in english_queries:\n",
    "    rating = rate_comprehensibility(query)\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Add the ratings as a new column to the dataframe\n",
    "df[\"Comprehensibility Rating\"] = ratings\n",
    "\n",
    "# Save the updated dataframe with ratings to a new Excel file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"Comprehensibility ratings added and saved to 'translated_queries_with_ratings.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f42872-6939-4c34-b74f-f177f8eb3d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark on a sample of queries\n",
    "benchmark = TranslationBenchmark(llm, metadata_lookup, max_context=max_context)\n",
    "\n",
    "# Load queries from CSV files\n",
    "csv_files = ['res_len_from_1000_to_1200_len(10).csv',\n",
    "                       'res_len_from_1200_to_1400_len(10).csv',\n",
    "                       'res_len_from_1400_to_1600_len(10).csv',\n",
    "                       'res_len_from_1600_to_1800_len(10).csv',\n",
    "                       'res_len_from_1800_to_2000_len(10).csv']  # Add your CSV files here\n",
    "queries = benchmark.load_queries_from_csv(csv_files)\n",
    "\n",
    "# Use a subset of queries for benchmarking\n",
    "sample_size = min(100, len(queries))  # Benchmark up to 100 queries\n",
    "sample_queries = queries[:sample_size]\n",
    "\n",
    "print(f\"Running benchmark on {sample_size} queries...\")\n",
    "results = benchmark.run_benchmark(sample_queries)\n",
    "\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(f\"Total Queries: {results['total_queries']}\")\n",
    "print(f\"Success Rate: {results['success_rate']:.2%}\")\n",
    "print(f\"Average Latency: {results['avg_latency']:.2f} seconds\")\n",
    "print(f\"Latency Stats:\")\n",
    "print(f\"  Min: {results['latency_stats']['min']:.2f}s\")\n",
    "print(f\"  Max: {results['latency_stats']['max']:.2f}s\")\n",
    "print(f\"  Median: {results['latency_stats']['median']:.2f}s\")\n",
    "print(f\"  Std Dev: {results['latency_stats']['std']:.2f}s\")\n",
    "\n",
    "if results['error_types']:\n",
    "    print(\"\\nError Types:\")\n",
    "    for error, count in results['error_types'].items():\n",
    "        print(f\"  {error}: {count}\")\n",
    "\n",
    "# Generate visualizations\n",
    "benchmark.plot_results(\"translation_benchmark_results(1).png\")\n",
    "benchmark.save_results(\"translation_benchmark_results(1).json\")\n",
    "benchmark.save_translations_to_csv(\"translated_queries(1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaca16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "class TranslationBenchmark:\n",
    "    def __init__(self, llm, metadata_lookup: Dict[str, str], max_context: int = None):\n",
    "        self.llm = llm\n",
    "        self.metadata_lookup = metadata_lookup\n",
    "        self.max_context = max_context\n",
    "        self.results = {\n",
    "            'latencies': [],\n",
    "            'file_names': [],\n",
    "            'successes': [],\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "    def load_queries_from_csv(self, csv_files: List[str]) -> List[str]:\n",
    "        \"\"\"Load queries from multiple CSV files and track their source files.\"\"\"\n",
    "        all_queries = []\n",
    "        for file in csv_files:\n",
    "            df = pd.read_csv(file)\n",
    "            queries = df['query_text'].tolist()\n",
    "            all_queries.extend(queries)\n",
    "            self.results['file_names'].extend([file] * len(queries))\n",
    "        return all_queries\n",
    "\n",
    "    def remove_outliers(self, data: List[float], threshold: float = 1.5) -> List[bool]:\n",
    "        \"\"\"\n",
    "        Remove outliers using the IQR method.\n",
    "        Returns a boolean mask where True indicates non-outlier values.\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            return []\n",
    "        \n",
    "        # Convert to numpy array for easier computation\n",
    "        data_array = np.array(data)\n",
    "        \n",
    "        # Calculate Q1, Q3 and IQR\n",
    "        Q1 = np.percentile(data_array, 25)\n",
    "        Q3 = np.percentile(data_array, 75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define bounds\n",
    "        lower_bound = Q1 - threshold * IQR\n",
    "        upper_bound = Q3 + threshold * IQR\n",
    "        \n",
    "        # Create mask for non-outliers\n",
    "        mask = (data_array >= lower_bound) & (data_array <= upper_bound)\n",
    "        return mask.tolist()\n",
    "\n",
    "    def run_benchmark(self, queries: List[str], iterations: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Run benchmark on a list of queries and track results by source file.\"\"\"\n",
    "        original_file_names = self.results['file_names'].copy()\n",
    "        \n",
    "        # Clear the results arrays\n",
    "        self.results['latencies'] = []\n",
    "        self.results['file_names'] = []\n",
    "        self.results['successes'] = []\n",
    "        self.results['errors'] = []\n",
    "        \n",
    "        for i, query in enumerate(queries):\n",
    "            for _ in range(iterations):\n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    result = self.translate_query(query)\n",
    "                    end_time = time.time()\n",
    "                    \n",
    "                    latency = end_time - start_time\n",
    "                    self.results['latencies'].append(latency)\n",
    "                    self.results['successes'].append(True)\n",
    "                    self.results['errors'].append(None)\n",
    "                    self.results['file_names'].append(original_file_names[i])\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.results['latencies'].append(None)\n",
    "                    self.results['successes'].append(False)\n",
    "                    self.results['errors'].append(str(e))\n",
    "                    self.results['file_names'].append(original_file_names[i])\n",
    "        \n",
    "        return self._calculate_statistics()\n",
    "\n",
    "    def _calculate_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate statistics for the benchmark results with outlier removal.\"\"\"\n",
    "        # Filter out None values and get non-outlier mask\n",
    "        latencies = np.array(self.results['latencies'])\n",
    "        valid_mask = ~np.isnan(latencies)\n",
    "        valid_latencies = latencies[valid_mask]\n",
    "        \n",
    "        if len(valid_latencies) > 0:\n",
    "            # Get mask for non-outliers\n",
    "            outlier_mask = self.remove_outliers(valid_latencies.tolist())\n",
    "            clean_latencies = valid_latencies[outlier_mask]\n",
    "        else:\n",
    "            clean_latencies = []\n",
    "        \n",
    "        stats = {\n",
    "            'total_queries': len(self.results['latencies']),\n",
    "            'success_rate': sum(self.results['successes']) / len(self.results['successes']),\n",
    "            'avg_latency': np.mean(clean_latencies) if len(clean_latencies) > 0 else 0,\n",
    "            'latency_stats': {\n",
    "                'min': np.min(clean_latencies) if len(clean_latencies) > 0 else 0,\n",
    "                'max': np.max(clean_latencies) if len(clean_latencies) > 0 else 0,\n",
    "                'median': np.median(clean_latencies) if len(clean_latencies) > 0 else 0,\n",
    "                'std': np.std(clean_latencies) if len(clean_latencies) > 0 else 0,\n",
    "                'outliers_removed': len(valid_latencies) - len(clean_latencies) if len(valid_latencies) > 0 else 0\n",
    "            },\n",
    "            'error_types': self._count_error_types()\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "    def _count_error_types(self) -> Dict[str, int]:\n",
    "        \"\"\"Count occurrences of each error type.\"\"\"\n",
    "        error_counts = {}\n",
    "        for error in self.results['errors']:\n",
    "            if error:\n",
    "                error_counts[error] = error_counts.get(error, 0) + 1\n",
    "        return error_counts\n",
    "\n",
    "    def plot_results(self, output_file: str = None):\n",
    "        \"\"\"Create visualizations of the benchmark results with outlier handling.\"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'File': self.results['file_names'],\n",
    "            'Latency (secs)': self.results['latencies'],\n",
    "            'Success': self.results['successes']\n",
    "        })\n",
    "        \n",
    "        # Remove outliers for plotting\n",
    "        df_clean = df.copy()\n",
    "        df_clean['is_outlier'] = False\n",
    "        \n",
    "        for file in df['File'].unique():\n",
    "            file_mask = df['File'] == file\n",
    "            latencies = df.loc[file_mask, 'Latency (secs)'].dropna()\n",
    "            if len(latencies) > 0:\n",
    "                outlier_mask = self.remove_outliers(latencies.tolist())\n",
    "                df_clean.loc[file_mask, 'is_outlier'] = ~pd.Series(outlier_mask, index=latencies.index)\n",
    "        \n",
    "        # Plot 1: Box plot of latencies by file (with outliers)\n",
    "        plt.subplot(2, 2, 1)\n",
    "        sns.boxplot(data=df, x='File', y='Latency (secs)', showfliers=True)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title('Latency Distribution by File (with outliers)')\n",
    "        \n",
    "        # Plot 2: Box plot of latencies by file (without outliers)\n",
    "        plt.subplot(2, 2, 2)\n",
    "        sns.boxplot(data=df_clean[~df_clean['is_outlier']], x='File', y='Latency (secs)')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title('Latency Distribution by File (without outliers)')\n",
    "        \n",
    "        # Plot 3: Latency over time (without outliers)\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.scatter(df_clean[~df_clean['is_outlier']].index, \n",
    "                   df_clean[~df_clean['is_outlier']]['Latency (secs)'], \n",
    "                   alpha=0.7, s=50)\n",
    "        plt.title('Latency Over Time (without outliers)')\n",
    "        plt.xlabel('Query Index')\n",
    "        plt.ylabel('Latency (seconds)')\n",
    "        \n",
    "        # Plot 4: Outlier distribution\n",
    "        plt.subplot(2, 2, 4)\n",
    "        outlier_counts = df_clean.groupby('File')['is_outlier'].sum()\n",
    "        outlier_counts.plot(kind='bar')\n",
    "        plt.title('Number of Outliers by File')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.ylabel('Number of Outliers')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if output_file:\n",
    "            plt.savefig(output_file)\n",
    "        plt.show()\n",
    "\n",
    "    def save_results(self, output_file: str):\n",
    "        \"\"\"Save benchmark results to a JSON file.\"\"\"\n",
    "        results = {\n",
    "            'statistics': self._calculate_statistics(),\n",
    "            'raw_results': {\n",
    "                'latencies': self.results['latencies'],\n",
    "                'file_names': self.results['file_names'],\n",
    "                'successes': self.results['successes'],\n",
    "                'errors': self.results['errors']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "    def save_translations_to_csv(self, output_file: str):\n",
    "        \"\"\"Save translations and their metadata to a CSV file.\"\"\"\n",
    "        df = pd.DataFrame({\n",
    "            'File': self.results['file_names'],\n",
    "            'Latency (secs)': self.results['latencies'],\n",
    "            'Success': self.results['successes'],\n",
    "            'Error': self.results['errors']\n",
    "        })\n",
    "        df.to_csv(output_file, index=False)\n",
    "\n",
    "    def translate_query(self, query: str):\n",
    "        \"\"\"Example translate_query function. Replace with your actual LLM call.\"\"\"\n",
    "        time.sleep(0.1)  # Simulating a 100 ms translation time\n",
    "        return f\"Translated: {query}\"  # Simulated translation result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
