{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11329b8f-ae0a-48d4-9da9-7c1c06875c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (3.1.5)\n",
      "Requirement already satisfied: transformers in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-cpp-python in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (0.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (2.2.3)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\solar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas openpyxl transformers torch\n",
    "%pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a607fdc-3c30-4006-af0d-60789c816fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/solar/OneDrive/Documents/Capstone/Capstone-Jupyter/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 4096\n",
      "llama_init_from_model: n_ctx_per_seq = 4096\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_init_from_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "model_path = \"/Users/solar/OneDrive/Documents/Capstone/Capstone-Jupyter/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "max_context = 4096\n",
    "llm = Llama(\n",
    "    model_path,\n",
    "    n_ctx=max_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4566bf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest entry code: EUTaxManIntSerElcRevOverAlign\n",
      "Length: 1353\n",
      "Content: EU Taxon - Man, Instal, Serv of Elec Equip Overall Align. EU Taxon - Man, Instal, Serv of Elec Equip Overall Align: This factor identifies the overall alignment for Manufacture, installation, and servicing of high, medium and low voltage electrical equipment for electrical transmission and distribution that result in or enable a substantial contribution to climate change mitigation, covering the substantial contribution criteria, the do no significant harm criteria, and the minimum social safeguards. This is the aggregated result across all Taxonomy objectives. The possible values are: Aligned,Aligned (>90%),Aligned (>80%),Aligned (>70%),Aligned (>60%),Aligned (>50%),Aligned (>40%),Aligned (>30%),Aligned (>20%),Aligned (>10%),Aligned (>0%),Likely aligned (100%),Likely aligned (>90%),Likely aligned (>80%),Likely aligned (>70%),Likely aligned (>60%),Likely aligned (>50%),Likely aligned (>40%),Likely aligned (>30%),Likely aligned (>20%),Likely aligned (>10%),Likely aligned (>0%),Potentially aligned (100%),Potentially aligned (>90%),Potentially aligned (>80%),Potentially aligned (>70%),Potentially aligned (>60%),Potentially aligned (>50%),Potentially aligned (>40%),Potentially aligned (>30%),Potentially aligned (>20%),Potentially aligned (>10%),Potentially aligned (>0%),Likely not aligned,Not aligned, Not collected, and Not applicable.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open(\"metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_metadata = json.load(f)\n",
    "\n",
    "# Create a lookup table\n",
    "metadata_lookup = {\n",
    "    entry[\"code\"]: f'{entry[\"name\"]}. {entry[\"description\"]}' \n",
    "    for entry in raw_metadata\n",
    "}\n",
    "\n",
    "def extract_codes(query):\n",
    "    return re.findall(r\"\\[([A-Za-z0-9_]+)\\]\", query)\n",
    "\n",
    "# Find the entry with the longest name + description combo\n",
    "longest_entry = max(metadata_lookup.items(), key=lambda item: len(item[1]))\n",
    "\n",
    "# Print the result\n",
    "print(f'Longest entry code: {longest_entry[0]}')\n",
    "print(f'Length: {len(longest_entry[1])}')\n",
    "print(f'Content: {longest_entry[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb3fc4c-e7b9-4bfb-9340-779aaba31116",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def translate_query(query):\n",
    "    prompt = f\"### Instruction:\\nRephrase this ESGish query as a natural English sentence. Each query is asking for all companies or issuers that match some paramater.\\n\\n{query}\\n\\n### Response:\"\n",
    "\n",
    "    # Use llama_cpp tokenizer to get exact number of tokens in prompt\n",
    "    prompt_tokens = llm.tokenize(prompt.encode(\"utf-8\"))\n",
    "    num_prompt_tokens = len(prompt_tokens)\n",
    "\n",
    "    max_total_tokens = 512\n",
    "    max_response_tokens = max_total_tokens - num_prompt_tokens\n",
    "\n",
    "    if max_response_tokens < 1:\n",
    "        print(f\"Skipping query â€” prompt too long: {query}\")\n",
    "        return \"[Prompt too long]\"\n",
    "\n",
    "    response = llm(prompt, max_tokens=max_response_tokens, temperature=0.1)\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def translate_query(query):\n",
    "    codes = extract_codes(query)\n",
    "    context = \"\\n\".join(\n",
    "        f\"{code}: {metadata_lookup.get(code, 'No metadata found.')}\"\n",
    "        for code in codes\n",
    "    )\n",
    "    \n",
    "    prompt = f\"### Instruction:\n",
    "Rephrase the following ESGish query into a concise natural English sentence. Each query is asking for all companies or issuers that match some paramater. Use the following metadata definitions for clarity:\n",
    "\n",
    "{context}\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "### Response:\"\n",
    "\n",
    "    response = llm(prompt, max_tokens=2048, temperature=0.1)\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n",
    "\"\"\"\n",
    "def max_tokens(text, max_tokens=40):\n",
    "    tokens = llm.tokenize(text.encode(\"utf-8\"))\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "    truncated = llm.detokenize(tokens[:max_tokens]).decode(\"utf-8\", errors=\"ignore\")\n",
    "    return truncated + \"...\"\n",
    "\n",
    "def max_afforded_tokens(codes):\n",
    "    count = len(codes)\n",
    "    m_tokens = int(max_context/count)\n",
    "    if m_tokens < 100:\n",
    "        m_tokens = 100\n",
    "    return m_tokens\n",
    "\n",
    "\n",
    "def translate_query(query, max_total_tokens=2048, max_output_tokens=256):\n",
    "    codes = extract_codes(query)\n",
    "\n",
    "    m_tokens = max_afforded_tokens(codes)\n",
    "\n",
    "    # Build full context blocks for each code\n",
    "    context_blocks = [\n",
    "        f\"{code}: {max_tokens(metadata_lookup.get(code, 'No metadata found.'), max_tokens=m_tokens)}\" for code in codes\n",
    "    ]\n",
    "\n",
    "    # Initial prompt pieces\n",
    "    instruction = \"### Instruction:\\nRephrase the following ESGish query into a concise natural English sentence. Each query is asking for all companies or issuers that match some paramater. Use the following metadata definitions for clarity:\\n\\n\"\n",
    "    query_part = f\"\\n\\nQuery: {query}\\n\\n### Response:\"\n",
    "\n",
    "    # Tokenize instruction and query to calculate remaining token budget\n",
    "    tokenizer = llm.tokenize  # Built-in tokenizer\n",
    "    instruction_tokens = len(tokenizer(instruction.encode(\"utf-8\")))\n",
    "    query_tokens = len(tokenizer(query_part.encode(\"utf-8\")))\n",
    "    token_budget = max_total_tokens - max_output_tokens - instruction_tokens - query_tokens\n",
    "\n",
    "    # Now iteratively add context blocks until budget is exhausted\n",
    "    context = \"\"\n",
    "    used_tokens = 0\n",
    "    for block in context_blocks:\n",
    "        block_tokens = len(tokenizer(block.encode(\"utf-8\"))) + 1  # +1 for newline\n",
    "        if used_tokens + block_tokens <= token_budget:\n",
    "            context += block + \"\\n\"\n",
    "            used_tokens += block_tokens\n",
    "        else:\n",
    "            break  # stop once we're out of budget\n",
    "\n",
    "    # Final prompt\n",
    "    prompt = instruction + context + query_part\n",
    "\n",
    "    # Call model\n",
    "    response = llm(prompt, max_tokens=max_output_tokens, temperature=0.1)\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81772e84-bbeb-4000-a351-281347bf87a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4309.21 ms /   175 tokens (   24.62 ms per token,    40.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6558.97 ms /    44 runs   (  149.07 ms per token,     6.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   10876.28 ms /   219 tokens\n",
      "Llama.generate: 164 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     300.51 ms /    12 tokens (   25.04 ms per token,    39.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6279.60 ms /    43 runs   (  146.04 ms per token,     6.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    6587.40 ms /    55 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2745.15 ms /   116 tokens (   23.67 ms per token,    42.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6139.25 ms /    42 runs   (  146.17 ms per token,     6.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    8891.45 ms /   158 tokens\n",
      "Llama.generate: 55 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3168.14 ms /   136 tokens (   23.30 ms per token,    42.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2853.40 ms /    19 runs   (  150.18 ms per token,     6.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    6024.69 ms /   155 tokens\n",
      "Llama.generate: 183 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     233.33 ms /     8 tokens (   29.17 ms per token,    34.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2111.33 ms /    14 runs   (  150.81 ms per token,     6.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    2347.05 ms /    22 tokens\n",
      "Llama.generate: 185 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     267.64 ms /     9 tokens (   29.74 ms per token,    33.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3574.04 ms /    24 runs   (  148.92 ms per token,     6.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    3845.61 ms /    33 tokens\n",
      "Llama.generate: 188 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     264.42 ms /     8 tokens (   33.05 ms per token,    30.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4863.10 ms /    32 runs   (  151.97 ms per token,     6.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    5132.57 ms /    40 tokens\n",
      "Llama.generate: 183 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     193.38 ms /     7 tokens (   27.63 ms per token,    36.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2698.26 ms /    17 runs   (  158.72 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    2894.49 ms /    24 tokens\n",
      "Llama.generate: 55 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3084.62 ms /   130 tokens (   23.73 ms per token,    42.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4271.40 ms /    29 runs   (  147.29 ms per token,     6.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    7360.81 ms /   159 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3273.26 ms /   121 tokens (   27.05 ms per token,    36.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3022.14 ms /    20 runs   (  151.11 ms per token,     6.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    6299.13 ms /   141 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2492.29 ms /   108 tokens (   23.08 ms per token,    43.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6508.99 ms /    44 runs   (  147.93 ms per token,     6.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    9008.86 ms /   152 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1100.52 ms /    47 tokens (   23.42 ms per token,    42.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1585.98 ms /    11 runs   (  144.18 ms per token,     6.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    2688.61 ms /    58 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2066.50 ms /    79 tokens (   26.16 ms per token,    38.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4452.62 ms /    29 runs   (  153.54 ms per token,     6.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    6524.21 ms /   108 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3021.82 ms /   130 tokens (   23.24 ms per token,    43.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1782.40 ms /    12 runs   (  148.53 ms per token,     6.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    4806.46 ms /   142 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1457.38 ms /    60 tokens (   24.29 ms per token,    41.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2753.39 ms /    19 runs   (  144.92 ms per token,     6.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    4213.86 ms /    79 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     748.52 ms /    29 tokens (   25.81 ms per token,    38.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1817.48 ms /    12 runs   (  151.46 ms per token,     6.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    2568.34 ms /    41 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2764.14 ms /   105 tokens (   26.33 ms per token,    37.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5213.86 ms /    34 runs   (  153.35 ms per token,     6.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    7984.05 ms /   139 tokens\n",
      "Llama.generate: 143 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     294.62 ms /    12 tokens (   24.55 ms per token,    40.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3437.01 ms /    23 runs   (  149.44 ms per token,     6.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    3735.76 ms /    35 tokens\n",
      "Llama.generate: 147 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     215.50 ms /     7 tokens (   30.79 ms per token,    32.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3841.23 ms /    24 runs   (  160.05 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4061.09 ms /    31 tokens\n",
      "Llama.generate: 143 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     296.91 ms /    11 tokens (   26.99 ms per token,    37.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2924.01 ms /    20 runs   (  146.20 ms per token,     6.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    3224.21 ms /    31 tokens\n",
      "Llama.generate: 54 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3096.17 ms /   100 tokens (   30.96 ms per token,    32.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4215.80 ms /    23 runs   (  183.30 ms per token,     5.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    7316.22 ms /   123 tokens\n",
      "Llama.generate: 143 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     378.83 ms /    12 tokens (   31.57 ms per token,    31.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3419.50 ms /    22 runs   (  155.43 ms per token,     6.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    3802.05 ms /    34 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2373.26 ms /   104 tokens (   22.82 ms per token,    43.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4839.46 ms /    32 runs   (  151.23 ms per token,     6.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    7218.61 ms /   136 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5828.90 ms /   227 tokens (   25.68 ms per token,    38.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3277.44 ms /    20 runs   (  163.87 ms per token,     6.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    9110.33 ms /   247 tokens\n",
      "Llama.generate: 273 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     290.02 ms /    11 tokens (   26.37 ms per token,    37.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4483.47 ms /    28 runs   (  160.12 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4778.30 ms /    39 tokens\n",
      "Llama.generate: 271 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     217.34 ms /     8 tokens (   27.17 ms per token,    36.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6465.67 ms /    43 runs   (  150.36 ms per token,     6.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    6690.35 ms /    51 tokens\n",
      "Llama.generate: 273 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     311.02 ms /    11 tokens (   28.27 ms per token,    35.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4256.12 ms /    28 runs   (  152.00 ms per token,     6.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    4571.90 ms /    39 tokens\n",
      "Llama.generate: 274 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     210.45 ms /     7 tokens (   30.06 ms per token,    33.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3720.50 ms /    23 runs   (  161.76 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    3935.23 ms /    30 tokens\n",
      "Llama.generate: 271 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     226.01 ms /     7 tokens (   32.29 ms per token,    30.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2923.47 ms /    18 runs   (  162.42 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    3152.69 ms /    25 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2060.81 ms /    82 tokens (   25.13 ms per token,    39.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4915.53 ms /    29 runs   (  169.50 ms per token,     5.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    6982.37 ms /   111 tokens\n",
      "Llama.generate: 122 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     430.99 ms /    14 tokens (   30.79 ms per token,    32.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4083.09 ms /    24 runs   (  170.13 ms per token,     5.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    4518.52 ms /    38 tokens\n",
      "Llama.generate: 120 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1693.28 ms /    61 tokens (   27.76 ms per token,    36.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5637.29 ms /    37 runs   (  152.36 ms per token,     6.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    7336.59 ms /    98 tokens\n",
      "Llama.generate: 123 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1199.38 ms /    52 tokens (   23.06 ms per token,    43.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11457.23 ms /    74 runs   (  154.83 ms per token,     6.46 tokens per second)\n",
      "llama_perf_context_print:       total time =   12670.80 ms /   126 tokens\n",
      "Llama.generate: 155 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     399.52 ms /    13 tokens (   30.73 ms per token,    32.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10338.42 ms /    65 runs   (  159.05 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =   10750.89 ms /    78 tokens\n",
      "Llama.generate: 123 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     880.56 ms /    37 tokens (   23.80 ms per token,    42.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9390.28 ms /    56 runs   (  167.68 ms per token,     5.96 tokens per second)\n",
      "llama_perf_context_print:       total time =   10281.97 ms /    93 tokens\n",
      "Llama.generate: 122 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     777.55 ms /    29 tokens (   26.81 ms per token,    37.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8329.72 ms /    50 runs   (  166.59 ms per token,     6.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    9116.83 ms /    79 tokens\n",
      "Llama.generate: 122 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     508.59 ms /    19 tokens (   26.77 ms per token,    37.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7212.00 ms /    48 runs   (  150.25 ms per token,     6.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    7728.77 ms /    67 tokens\n",
      "Llama.generate: 135 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     991.66 ms /    40 tokens (   24.79 ms per token,    40.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4234.26 ms /    28 runs   (  151.22 ms per token,     6.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    5230.73 ms /    68 tokens\n",
      "Llama.generate: 123 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     335.91 ms /    13 tokens (   25.84 ms per token,    38.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4316.30 ms /    28 runs   (  154.15 ms per token,     6.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    4657.11 ms /    41 tokens\n",
      "Llama.generate: 120 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     251.19 ms /     9 tokens (   27.91 ms per token,    35.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2790.00 ms /    17 runs   (  164.12 ms per token,     6.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    3043.99 ms /    26 tokens\n",
      "Llama.generate: 54 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2297.10 ms /    78 tokens (   29.45 ms per token,    33.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3366.59 ms /    22 runs   (  153.03 ms per token,     6.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    5667.30 ms /   100 tokens\n",
      "Llama.generate: 120 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     315.98 ms /    12 tokens (   26.33 ms per token,    37.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3347.64 ms /    22 runs   (  152.17 ms per token,     6.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    3667.39 ms /    34 tokens\n",
      "Llama.generate: 124 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     194.92 ms /     7 tokens (   27.85 ms per token,    35.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3134.34 ms /    20 runs   (  156.72 ms per token,     6.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    3332.74 ms /    27 tokens\n",
      "Llama.generate: 120 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     313.29 ms /    12 tokens (   26.11 ms per token,    38.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3256.24 ms /    22 runs   (  148.01 ms per token,     6.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    3572.94 ms /    34 tokens\n",
      "Llama.generate: 124 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     192.82 ms /     7 tokens (   27.55 ms per token,    36.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3216.72 ms /    21 runs   (  153.18 ms per token,     6.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    3413.07 ms /    28 tokens\n",
      "Llama.generate: 54 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1797.49 ms /    78 tokens (   23.04 ms per token,    43.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4121.88 ms /    27 runs   (  152.66 ms per token,     6.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    5923.81 ms /   105 tokens\n",
      "Llama.generate: 120 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     300.77 ms /    12 tokens (   25.06 ms per token,    39.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3561.73 ms /    24 runs   (  148.41 ms per token,     6.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    3866.48 ms /    36 tokens\n",
      "Llama.generate: 124 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     202.81 ms /     7 tokens (   28.97 ms per token,    34.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3676.31 ms /    25 runs   (  147.05 ms per token,     6.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    3883.20 ms /    32 tokens\n",
      "Llama.generate: 123 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     181.41 ms /     6 tokens (   30.24 ms per token,    33.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3723.87 ms /    25 runs   (  148.95 ms per token,     6.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    3909.52 ms /    31 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3078.85 ms /   120 tokens (   25.66 ms per token,    38.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4418.00 ms /    30 runs   (  147.27 ms per token,     6.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    7501.93 ms /   150 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2387.71 ms /    99 tokens (   24.12 ms per token,    41.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4357.89 ms /    27 runs   (  161.40 ms per token,     6.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    6750.40 ms /   126 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4532.12 ms /   195 tokens (   23.24 ms per token,    43.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7738.37 ms /    53 runs   (  146.01 ms per token,     6.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   12279.71 ms /   248 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 151 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3421.97 ms /   151 tokens (   22.66 ms per token,    44.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4638.86 ms /    32 runs   (  144.96 ms per token,     6.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    8066.12 ms /   183 tokens\n",
      "Llama.generate: 184 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     251.23 ms /    10 tokens (   25.12 ms per token,    39.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38124.16 ms /   255 runs   (  149.51 ms per token,     6.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   38452.54 ms /   265 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2133.89 ms /    93 tokens (   22.95 ms per token,    43.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3259.19 ms /    22 runs   (  148.14 ms per token,     6.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    5396.87 ms /   115 tokens\n",
      "Llama.generate: 140 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     258.74 ms /    10 tokens (   25.87 ms per token,    38.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3224.33 ms /    22 runs   (  146.56 ms per token,     6.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    3487.10 ms /    32 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2077.91 ms /    91 tokens (   22.83 ms per token,    43.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2523.03 ms /    17 runs   (  148.41 ms per token,     6.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    4603.86 ms /   108 tokens\n",
      "Llama.generate: 135 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     268.11 ms /    10 tokens (   26.81 ms per token,    37.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3608.25 ms /    25 runs   (  144.33 ms per token,     6.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    3880.45 ms /    35 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2096.22 ms /    91 tokens (   23.04 ms per token,    43.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3805.85 ms /    26 runs   (  146.38 ms per token,     6.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    5906.28 ms /   117 tokens\n",
      "Llama.generate: 124 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     370.16 ms /    15 tokens (   24.68 ms per token,    40.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3161.19 ms /    22 runs   (  143.69 ms per token,     6.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    3535.29 ms /    37 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4872.03 ms /   212 tokens (   22.98 ms per token,    43.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4250.46 ms /    29 runs   (  146.57 ms per token,     6.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    9127.58 ms /   241 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3133.57 ms /   136 tokens (   23.04 ms per token,    43.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3349.92 ms /    23 runs   (  145.65 ms per token,     6.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    6487.47 ms /   159 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4921.31 ms /   212 tokens (   23.21 ms per token,    43.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4539.01 ms /    31 runs   (  146.42 ms per token,     6.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    9465.59 ms /   243 tokens\n",
      "Llama.generate: 237 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1901.20 ms /    81 tokens (   23.47 ms per token,    42.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15576.52 ms /   106 runs   (  146.95 ms per token,     6.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   17499.60 ms /   187 tokens\n",
      "Llama.generate: 55 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1969.24 ms /    86 tokens (   22.90 ms per token,    43.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5156.24 ms /    35 runs   (  147.32 ms per token,     6.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    7131.53 ms /   121 tokens\n",
      "Llama.generate: 55 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2399.06 ms /   103 tokens (   23.29 ms per token,    42.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3475.54 ms /    23 runs   (  151.11 ms per token,     6.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    5878.44 ms /   126 tokens\n",
      "Llama.generate: 149 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     254.13 ms /     9 tokens (   28.24 ms per token,    35.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3745.34 ms /    25 runs   (  149.81 ms per token,     6.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    4003.79 ms /    34 tokens\n",
      "Llama.generate: 54 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2651.09 ms /   114 tokens (   23.26 ms per token,    43.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4337.67 ms /    30 runs   (  144.59 ms per token,     6.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    6993.69 ms /   144 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2650.11 ms /   118 tokens (   22.46 ms per token,    44.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3778.85 ms /    26 runs   (  145.34 ms per token,     6.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    6433.34 ms /   144 tokens\n",
      "Llama.generate: 159 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     238.27 ms /     9 tokens (   26.47 ms per token,    37.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3804.24 ms /    26 runs   (  146.32 ms per token,     6.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    4047.11 ms /    35 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1444.95 ms /    64 tokens (   22.58 ms per token,    44.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3201.67 ms /    22 runs   (  145.53 ms per token,     6.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    4650.30 ms /    86 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1972.03 ms /    87 tokens (   22.67 ms per token,    44.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3973.89 ms /    27 runs   (  147.18 ms per token,     6.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    5950.52 ms /   114 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1604.42 ms /    70 tokens (   22.92 ms per token,    43.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2652.77 ms /    18 runs   (  147.38 ms per token,     6.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    4260.47 ms /    88 tokens\n",
      "Llama.generate: 113 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     172.99 ms /     6 tokens (   28.83 ms per token,    34.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2319.64 ms /    16 runs   (  144.98 ms per token,     6.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    2495.41 ms /    22 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 53 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1233.35 ms /    53 tokens (   23.27 ms per token,    42.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2520.47 ms /    17 runs   (  148.26 ms per token,     6.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    3756.53 ms /    70 tokens\n",
      "Llama.generate: 95 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     228.09 ms /     9 tokens (   25.34 ms per token,    39.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3170.42 ms /    22 runs   (  144.11 ms per token,     6.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    3402.04 ms /    31 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1482.03 ms /    65 tokens (   22.80 ms per token,    43.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3339.66 ms /    23 runs   (  145.20 ms per token,     6.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    4825.53 ms /    88 tokens\n",
      "Llama.generate: 109 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     240.28 ms /     9 tokens (   26.70 ms per token,    37.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4044.90 ms /    28 runs   (  144.46 ms per token,     6.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    4289.90 ms /    37 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1722.04 ms /    75 tokens (   22.96 ms per token,    43.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4256.99 ms /    29 runs   (  146.79 ms per token,     6.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    5983.84 ms /   104 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1254.69 ms /    54 tokens (   23.24 ms per token,    43.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2621.46 ms /    18 runs   (  145.64 ms per token,     6.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    3879.12 ms /    72 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 167 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3831.90 ms /   167 tokens (   22.95 ms per token,    43.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2881.35 ms /    19 runs   (  151.65 ms per token,     6.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    6716.57 ms /   186 tokens\n",
      "Llama.generate: 208 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     233.37 ms /     9 tokens (   25.93 ms per token,    38.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2612.15 ms /    18 runs   (  145.12 ms per token,     6.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    2848.45 ms /    27 tokens\n",
      "Llama.generate: 210 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     252.34 ms /    10 tokens (   25.23 ms per token,    39.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3652.69 ms /    25 runs   (  146.11 ms per token,     6.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    3909.37 ms /    35 tokens\n",
      "Llama.generate: 214 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     217.21 ms /     8 tokens (   27.15 ms per token,    36.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3660.26 ms /    25 runs   (  146.41 ms per token,     6.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    3881.71 ms /    33 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1754.80 ms /    77 tokens (   22.79 ms per token,    43.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2477.70 ms /    17 runs   (  145.75 ms per token,     6.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    4235.48 ms /    94 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2557.78 ms /   113 tokens (   22.64 ms per token,    44.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3171.03 ms /    22 runs   (  144.14 ms per token,     6.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    5732.55 ms /   135 tokens\n",
      "Llama.generate: 157 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     229.35 ms /     9 tokens (   25.48 ms per token,    39.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3491.16 ms /    24 runs   (  145.46 ms per token,     6.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    3724.43 ms /    33 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2751.63 ms /   120 tokens (   22.93 ms per token,    43.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2836.56 ms /    19 runs   (  149.29 ms per token,     6.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    5591.73 ms /   139 tokens\n",
      "Llama.generate: 167 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     210.65 ms /     8 tokens (   26.33 ms per token,    37.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3612.84 ms /    23 runs   (  157.08 ms per token,     6.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    3827.45 ms /    31 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2353.28 ms /    96 tokens (   24.51 ms per token,    40.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4941.51 ms /    33 runs   (  149.74 ms per token,     6.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    7300.80 ms /   129 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1765.10 ms /    75 tokens (   23.53 ms per token,    42.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4797.60 ms /    33 runs   (  145.38 ms per token,     6.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    6568.11 ms /   108 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3219.94 ms /   140 tokens (   23.00 ms per token,    43.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2360.67 ms /    16 runs   (  147.54 ms per token,     6.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    5583.67 ms /   156 tokens\n",
      "Llama.generate: 185 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     222.28 ms /     8 tokens (   27.78 ms per token,    35.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2628.86 ms /    18 runs   (  146.05 ms per token,     6.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    2854.15 ms /    26 tokens\n",
      "Llama.generate: 187 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     237.53 ms /     9 tokens (   26.39 ms per token,    37.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2672.91 ms /    18 runs   (  148.50 ms per token,     6.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    2913.53 ms /    27 tokens\n",
      "Llama.generate: 190 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     215.42 ms /     8 tokens (   26.93 ms per token,    37.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3757.91 ms /    26 runs   (  144.53 ms per token,     6.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    3977.69 ms /    34 tokens\n",
      "Llama.generate: 185 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     202.32 ms /     7 tokens (   28.90 ms per token,    34.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3345.74 ms /    23 runs   (  145.47 ms per token,     6.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    3551.80 ms /    30 tokens\n",
      "Llama.generate: 183 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     384.62 ms /    16 tokens (   24.04 ms per token,    41.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3554.65 ms /    24 runs   (  148.11 ms per token,     6.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    3943.31 ms /    40 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 109 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2504.29 ms /   109 tokens (   22.98 ms per token,    43.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4792.30 ms /    33 runs   (  145.22 ms per token,     6.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    7302.42 ms /   142 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2485.40 ms /   106 tokens (   23.45 ms per token,    42.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5078.00 ms /    35 runs   (  145.09 ms per token,     6.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    7569.50 ms /   141 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 253 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5860.28 ms /   253 tokens (   23.16 ms per token,    43.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15740.08 ms /   106 runs   (  148.49 ms per token,     6.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   21622.66 ms /   359 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 132 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 1 to translated_queries_TEST.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3373.68 ms /   132 tokens (   25.56 ms per token,    39.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4794.73 ms /    31 runs   (  154.67 ms per token,     6.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    8173.88 ms /   163 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2595.72 ms /    88 tokens (   29.50 ms per token,    33.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6621.84 ms /    41 runs   (  161.51 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    9225.41 ms /   129 tokens\n",
      "Llama.generate: 107 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1260.46 ms /    47 tokens (   26.82 ms per token,    37.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8724.31 ms /    53 runs   (  164.61 ms per token,     6.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    9994.86 ms /   100 tokens\n",
      "Llama.generate: 111 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1013.80 ms /    40 tokens (   25.34 ms per token,    39.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8052.88 ms /    51 runs   (  157.90 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    9075.80 ms /    91 tokens\n",
      "Llama.generate: 116 prefix-match hit, remaining 156 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4138.48 ms /   156 tokens (   26.53 ms per token,    37.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25871.46 ms /   166 runs   (  155.85 ms per token,     6.42 tokens per second)\n",
      "llama_perf_context_print:       total time =   30052.52 ms /   322 tokens\n",
      "Llama.generate: 127 prefix-match hit, remaining 425 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   10490.54 ms /   425 tokens (   24.68 ms per token,    40.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41415.96 ms /   255 runs   (  162.42 ms per token,     6.16 tokens per second)\n",
      "llama_perf_context_print:       total time =   51993.35 ms /   680 tokens\n",
      "Llama.generate: 107 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     263.09 ms /    10 tokens (   26.31 ms per token,    38.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3175.54 ms /    21 runs   (  151.22 ms per token,     6.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    3442.48 ms /    31 tokens\n",
      "Llama.generate: 108 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1059.05 ms /    42 tokens (   25.22 ms per token,    39.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7610.37 ms /    50 runs   (  152.21 ms per token,     6.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    8678.57 ms /    92 tokens\n",
      "Llama.generate: 107 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1233.19 ms /    50 tokens (   24.66 ms per token,    40.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9116.10 ms /    54 runs   (  168.82 ms per token,     5.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   10359.92 ms /   104 tokens\n",
      "Llama.generate: 107 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     651.99 ms /    24 tokens (   27.17 ms per token,    36.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5300.78 ms /    35 runs   (  151.45 ms per token,     6.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    5959.38 ms /    59 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1322.44 ms /    52 tokens (   25.43 ms per token,    39.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4925.33 ms /    32 runs   (  153.92 ms per token,     6.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    6253.89 ms /    84 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 148 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3854.21 ms /   148 tokens (   26.04 ms per token,    38.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6577.21 ms /    43 runs   (  152.96 ms per token,     6.54 tokens per second)\n",
      "llama_perf_context_print:       total time =   10439.47 ms /   191 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3080.67 ms /   124 tokens (   24.84 ms per token,    40.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6557.62 ms /    40 runs   (  163.94 ms per token,     6.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    9646.46 ms /   164 tokens\n",
      "Llama.generate: 167 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     306.32 ms /    12 tokens (   25.53 ms per token,    39.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4572.41 ms /    31 runs   (  147.50 ms per token,     6.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    4883.92 ms /    43 tokens\n",
      "Llama.generate: 171 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     203.34 ms /     7 tokens (   29.05 ms per token,    34.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4912.10 ms /    32 runs   (  153.50 ms per token,     6.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    5120.71 ms /    39 tokens\n",
      "Llama.generate: 170 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     187.39 ms /     6 tokens (   31.23 ms per token,    32.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4632.28 ms /    31 runs   (  149.43 ms per token,     6.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    4824.96 ms /    37 tokens\n",
      "Llama.generate: 167 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     337.77 ms /    12 tokens (   28.15 ms per token,    35.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5670.73 ms /    36 runs   (  157.52 ms per token,     6.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    6015.22 ms /    48 tokens\n",
      "Llama.generate: 58 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3163.75 ms /   121 tokens (   26.15 ms per token,    38.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6153.17 ms /    38 runs   (  161.93 ms per token,     6.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    9324.35 ms /   159 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 235 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5465.14 ms /   235 tokens (   23.26 ms per token,    43.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18904.83 ms /   124 runs   (  152.46 ms per token,     6.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   24398.29 ms /   359 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3511.19 ms /   149 tokens (   23.57 ms per token,    42.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5731.27 ms /    37 runs   (  154.90 ms per token,     6.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    9249.28 ms /   186 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3628.91 ms /   135 tokens (   26.88 ms per token,    37.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6032.06 ms /    36 runs   (  167.56 ms per token,     5.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    9668.29 ms /   171 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    6127.05 ms /   254 tokens (   24.12 ms per token,    41.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4408.06 ms /    29 runs   (  152.00 ms per token,     6.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   10540.71 ms /   283 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 311 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    9614.54 ms /   311 tokens (   30.91 ms per token,    32.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5526.18 ms /    32 runs   (  172.69 ms per token,     5.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   15147.20 ms /   343 tokens\n",
      "Llama.generate: 196 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2482.60 ms /    86 tokens (   28.87 ms per token,    34.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5769.45 ms /    36 runs   (  160.26 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    8258.70 ms /   122 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 372 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    9466.77 ms /   372 tokens (   25.45 ms per token,    39.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4491.76 ms /    28 runs   (  160.42 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =   13963.89 ms /   400 tokens\n",
      "Llama.generate: 273 prefix-match hit, remaining 150 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3776.64 ms /   150 tokens (   25.18 ms per token,    39.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5807.47 ms /    35 runs   (  165.93 ms per token,     6.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    9591.21 ms /   185 tokens\n",
      "Llama.generate: 273 prefix-match hit, remaining 1178 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   31357.77 ms /  1178 tokens (   26.62 ms per token,    37.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10684.89 ms /    68 runs   (  157.13 ms per token,     6.36 tokens per second)\n",
      "llama_perf_context_print:       total time =   42055.75 ms /  1246 tokens\n",
      "Llama.generate: 273 prefix-match hit, remaining 1306 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   33085.40 ms /  1306 tokens (   25.33 ms per token,    39.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39996.23 ms /   255 runs   (  156.85 ms per token,     6.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   73162.80 ms /  1561 tokens\n",
      "Llama.generate: 273 prefix-match hit, remaining 529 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   12921.26 ms /   529 tokens (   24.43 ms per token,    40.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11373.36 ms /    75 runs   (  151.64 ms per token,     6.59 tokens per second)\n",
      "llama_perf_context_print:       total time =   24309.40 ms /   604 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 1514 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   37296.51 ms /  1514 tokens (   24.63 ms per token,    40.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38097.76 ms /   244 runs   (  156.14 ms per token,     6.40 tokens per second)\n",
      "llama_perf_context_print:       total time =   75468.80 ms /  1758 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 305 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    7148.13 ms /   305 tokens (   23.44 ms per token,    42.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8555.43 ms /    58 runs   (  147.51 ms per token,     6.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   15714.32 ms /   363 tokens\n",
      "Llama.generate: 129 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2502.60 ms /   108 tokens (   23.17 ms per token,    43.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7255.30 ms /    50 runs   (  145.11 ms per token,     6.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    9766.61 ms /   158 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5498.98 ms /   239 tokens (   23.01 ms per token,    43.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5841.58 ms /    40 runs   (  146.04 ms per token,     6.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   11347.40 ms /   279 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 834 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   19942.67 ms /   834 tokens (   23.91 ms per token,    41.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14156.65 ms /    93 runs   (  152.22 ms per token,     6.57 tokens per second)\n",
      "llama_perf_context_print:       total time =   34118.32 ms /   927 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 547 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   13459.55 ms /   547 tokens (   24.61 ms per token,    40.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12997.54 ms /    86 runs   (  151.13 ms per token,     6.62 tokens per second)\n",
      "llama_perf_context_print:       total time =   26474.48 ms /   633 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 281 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    6549.20 ms /   281 tokens (   23.31 ms per token,    42.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8805.92 ms /    59 runs   (  149.25 ms per token,     6.70 tokens per second)\n",
      "llama_perf_context_print:       total time =   15366.00 ms /   340 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 1628 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   40994.77 ms /  1628 tokens (   25.18 ms per token,    39.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40470.93 ms /   255 runs   (  158.71 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   81544.82 ms /  1883 tokens\n",
      "Llama.generate: 1243 prefix-match hit, remaining 266 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    7064.83 ms /   266 tokens (   26.56 ms per token,    37.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40639.18 ms /   255 runs   (  159.37 ms per token,     6.27 tokens per second)\n",
      "llama_perf_context_print:       total time =   47783.08 ms /   521 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 411 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    9720.45 ms /   411 tokens (   23.65 ms per token,    42.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11255.93 ms /    73 runs   (  154.19 ms per token,     6.49 tokens per second)\n",
      "llama_perf_context_print:       total time =   20990.60 ms /   484 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 244 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5764.03 ms /   244 tokens (   23.62 ms per token,    42.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9109.57 ms /    62 runs   (  146.93 ms per token,     6.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   14884.54 ms /   306 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 336 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    8128.26 ms /   336 tokens (   24.19 ms per token,    41.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8300.37 ms /    53 runs   (  156.61 ms per token,     6.39 tokens per second)\n",
      "llama_perf_context_print:       total time =   16438.42 ms /   389 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5780.83 ms /   228 tokens (   25.35 ms per token,    39.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8793.42 ms /    58 runs   (  151.61 ms per token,     6.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   14585.12 ms /   286 tokens\n",
      "Llama.generate: 109 prefix-match hit, remaining 343 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    8629.91 ms /   343 tokens (   25.16 ms per token,    39.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16536.88 ms /   103 runs   (  160.55 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =   25189.32 ms /   446 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 273 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    6704.57 ms /   273 tokens (   24.56 ms per token,    40.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16051.06 ms /    96 runs   (  167.20 ms per token,     5.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   22776.23 ms /   369 tokens\n",
      "Llama.generate: 117 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1130.90 ms /    48 tokens (   23.56 ms per token,    42.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2782.83 ms /    19 runs   (  146.46 ms per token,     6.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    3916.78 ms /    67 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2586.86 ms /   105 tokens (   24.64 ms per token,    40.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5403.83 ms /    35 runs   (  154.40 ms per token,     6.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    7996.76 ms /   140 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4117.60 ms /   166 tokens (   24.80 ms per token,    40.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5973.79 ms /    39 runs   (  153.17 ms per token,     6.53 tokens per second)\n",
      "llama_perf_context_print:       total time =   10098.05 ms /   205 tokens\n",
      "Llama.generate: 104 prefix-match hit, remaining 148 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3733.07 ms /   148 tokens (   25.22 ms per token,    39.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9911.51 ms /    66 runs   (  150.17 ms per token,     6.66 tokens per second)\n",
      "llama_perf_context_print:       total time =   13656.42 ms /   214 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2768.34 ms /   118 tokens (   23.46 ms per token,    42.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5170.62 ms /    34 runs   (  152.08 ms per token,     6.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    7944.75 ms /   152 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 162 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4037.47 ms /   162 tokens (   24.92 ms per token,    40.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7694.27 ms /    52 runs   (  147.97 ms per token,     6.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   11740.87 ms /   214 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5194.38 ms /   211 tokens (   24.62 ms per token,    40.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5734.88 ms /    38 runs   (  150.92 ms per token,     6.63 tokens per second)\n",
      "llama_perf_context_print:       total time =   10935.68 ms /   249 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4266.61 ms /   176 tokens (   24.24 ms per token,    41.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7564.15 ms /    48 runs   (  157.59 ms per token,     6.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   11841.52 ms /   224 tokens\n",
      "Llama.generate: 80 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1956.79 ms /    82 tokens (   23.86 ms per token,    41.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3553.03 ms /    23 runs   (  154.48 ms per token,     6.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    5513.89 ms /   105 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5674.85 ms /   220 tokens (   25.79 ms per token,    38.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3558.60 ms /    23 runs   (  154.72 ms per token,     6.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    9237.72 ms /   243 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 1517 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   41440.59 ms /  1517 tokens (   27.32 ms per token,    36.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11005.10 ms /    65 runs   (  169.31 ms per token,     5.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   52458.87 ms /  1582 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 255 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    6287.03 ms /   255 tokens (   24.66 ms per token,    40.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4805.84 ms /    29 runs   (  165.72 ms per token,     6.03 tokens per second)\n",
      "llama_perf_context_print:       total time =   11098.50 ms /   284 tokens\n",
      "Llama.generate: 95 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2667.03 ms /   102 tokens (   26.15 ms per token,    38.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3883.22 ms /    25 runs   (  155.33 ms per token,     6.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    6554.84 ms /   127 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 881 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   23260.78 ms /   881 tokens (   26.40 ms per token,    37.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14242.23 ms /    89 runs   (  160.03 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =   37522.21 ms /   970 tokens\n",
      "Llama.generate: 406 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2337.85 ms /    90 tokens (   25.98 ms per token,    38.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7877.80 ms /    49 runs   (  160.77 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =   10225.14 ms /   139 tokens\n",
      "Llama.generate: 406 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   12088.04 ms /   450 tokens (   26.86 ms per token,    37.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11471.37 ms /    71 runs   (  161.57 ms per token,     6.19 tokens per second)\n",
      "llama_perf_context_print:       total time =   23573.67 ms /   521 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5175.89 ms /   210 tokens (   24.65 ms per token,    40.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10226.87 ms /    65 runs   (  157.34 ms per token,     6.36 tokens per second)\n",
      "llama_perf_context_print:       total time =   15415.54 ms /   275 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 806 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   20555.51 ms /   806 tokens (   25.50 ms per token,    39.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37160.60 ms /   246 runs   (  151.06 ms per token,     6.62 tokens per second)\n",
      "llama_perf_context_print:       total time =   57792.82 ms /  1052 tokens\n",
      "Llama.generate: 394 prefix-match hit, remaining 569 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   14396.46 ms /   569 tokens (   25.30 ms per token,    39.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40499.42 ms /   255 runs   (  158.82 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   54984.41 ms /   824 tokens\n",
      "Llama.generate: 124 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2749.97 ms /   119 tokens (   23.11 ms per token,    43.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5322.01 ms /    36 runs   (  147.83 ms per token,     6.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    8078.07 ms /   155 tokens\n",
      "Llama.generate: 126 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3299.37 ms /   133 tokens (   24.81 ms per token,    40.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6833.39 ms /    46 runs   (  148.55 ms per token,     6.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   10140.85 ms /   179 tokens\n",
      "Llama.generate: 55 prefix-match hit, remaining 1671 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   41131.35 ms /  1671 tokens (   24.61 ms per token,    40.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39844.00 ms /   255 runs   (  156.25 ms per token,     6.40 tokens per second)\n",
      "llama_perf_context_print:       total time =   81058.81 ms /  1926 tokens\n",
      "Llama.generate: 230 prefix-match hit, remaining 1453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   37000.17 ms /  1453 tokens (   25.46 ms per token,    39.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41644.64 ms /   255 runs   (  163.31 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =   78725.54 ms /  1708 tokens\n",
      "Llama.generate: 1664 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     240.88 ms /     6 tokens (   40.15 ms per token,    24.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41957.29 ms /   255 runs   (  164.54 ms per token,     6.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   42279.33 ms /   261 tokens\n",
      "Llama.generate: 1323 prefix-match hit, remaining 324 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    8469.14 ms /   324 tokens (   26.14 ms per token,    38.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33126.82 ms /   200 runs   (  165.63 ms per token,     6.04 tokens per second)\n",
      "llama_perf_context_print:       total time =   41652.62 ms /   524 tokens\n",
      "Llama.generate: 1490 prefix-match hit, remaining 291 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    7686.49 ms /   291 tokens (   26.41 ms per token,    37.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   46131.32 ms /   255 runs   (  180.91 ms per token,     5.53 tokens per second)\n",
      "llama_perf_context_print:       total time =   53913.03 ms /   546 tokens\n",
      "Llama.generate: 1625 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3614.05 ms /    96 tokens (   37.65 ms per token,    26.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50108.94 ms /   255 runs   (  196.51 ms per token,     5.09 tokens per second)\n",
      "llama_perf_context_print:       total time =   53818.43 ms /   351 tokens\n",
      "Llama.generate: 124 prefix-match hit, remaining 395 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   10288.09 ms /   395 tokens (   26.05 ms per token,    38.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10414.23 ms /    63 runs   (  165.31 ms per token,     6.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   20714.93 ms /   458 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 367 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   10455.79 ms /   367 tokens (   28.49 ms per token,    35.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10547.07 ms /    64 runs   (  164.80 ms per token,     6.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   21016.02 ms /   431 tokens\n",
      "Llama.generate: 173 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3027.59 ms /   122 tokens (   24.82 ms per token,    40.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4913.47 ms /    31 runs   (  158.50 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    7946.81 ms /   153 tokens\n",
      "Llama.generate: 177 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3095.92 ms /   121 tokens (   25.59 ms per token,    39.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6403.67 ms /    40 runs   (  160.09 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    9507.24 ms /   161 tokens\n",
      "Llama.generate: 286 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     319.61 ms /    12 tokens (   26.63 ms per token,    37.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6191.85 ms /    40 runs   (  154.80 ms per token,     6.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    6518.79 ms /    52 tokens\n",
      "Llama.generate: 290 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     205.37 ms /     7 tokens (   29.34 ms per token,    34.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4922.59 ms /    29 runs   (  169.74 ms per token,     5.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    5133.44 ms /    36 tokens\n",
      "Llama.generate: 173 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    6482.51 ms /   254 tokens (   25.52 ms per token,    39.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11376.22 ms /    71 runs   (  160.23 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =   17872.79 ms /   325 tokens\n",
      "Llama.generate: 171 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1262.96 ms /    49 tokens (   25.77 ms per token,    38.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2755.38 ms /    17 runs   (  162.08 ms per token,     6.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    4021.65 ms /    66 tokens\n",
      "Llama.generate: 171 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5337.44 ms /   223 tokens (   23.93 ms per token,    41.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5956.83 ms /    38 runs   (  156.76 ms per token,     6.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   11301.39 ms /   261 tokens\n",
      "Llama.generate: 369 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1925.96 ms /    74 tokens (   26.03 ms per token,    38.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16347.72 ms /    99 runs   (  165.13 ms per token,     6.06 tokens per second)\n",
      "llama_perf_context_print:       total time =   18295.78 ms /   173 tokens\n",
      "Llama.generate: 384 prefix-match hit, remaining 53 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2138.74 ms /    53 tokens (   40.35 ms per token,    24.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15484.61 ms /    91 runs   (  170.16 ms per token,     5.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   17642.58 ms /   144 tokens\n",
      "Llama.generate: 177 prefix-match hit, remaining 109 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2626.64 ms /   109 tokens (   24.10 ms per token,    41.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9641.99 ms /    58 runs   (  166.24 ms per token,     6.02 tokens per second)\n",
      "llama_perf_context_print:       total time =   12279.79 ms /   167 tokens\n",
      "Llama.generate: 174 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    6414.32 ms /   217 tokens (   29.56 ms per token,    33.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16570.03 ms /   100 runs   (  165.70 ms per token,     6.03 tokens per second)\n",
      "llama_perf_context_print:       total time =   23007.10 ms /   317 tokens\n",
      "Llama.generate: 174 prefix-match hit, remaining 271 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    6821.12 ms /   271 tokens (   25.17 ms per token,    39.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16320.60 ms /   103 runs   (  158.45 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =   23163.99 ms /   374 tokens\n",
      "Llama.generate: 386 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     573.20 ms /    22 tokens (   26.05 ms per token,    38.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8555.94 ms /    56 runs   (  152.78 ms per token,     6.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    9139.19 ms /    78 tokens\n",
      "Llama.generate: 386 prefix-match hit, remaining 53 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1303.35 ms /    53 tokens (   24.59 ms per token,    40.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14350.89 ms /    93 runs   (  154.31 ms per token,     6.48 tokens per second)\n",
      "llama_perf_context_print:       total time =   15672.68 ms /   146 tokens\n",
      "Llama.generate: 177 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2619.10 ms /   111 tokens (   23.60 ms per token,    42.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8984.67 ms /    58 runs   (  154.91 ms per token,     6.46 tokens per second)\n",
      "llama_perf_context_print:       total time =   11614.39 ms /   169 tokens\n",
      "Llama.generate: 177 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2565.60 ms /   111 tokens (   23.11 ms per token,    43.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9075.44 ms /    59 runs   (  153.82 ms per token,     6.50 tokens per second)\n",
      "llama_perf_context_print:       total time =   11652.33 ms /   170 tokens\n",
      "Llama.generate: 171 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1304.57 ms /    54 tokens (   24.16 ms per token,    41.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4686.88 ms /    31 runs   (  151.19 ms per token,     6.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    5996.54 ms /    85 tokens\n",
      "Llama.generate: 171 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2941.16 ms /   125 tokens (   23.53 ms per token,    42.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7451.33 ms /    48 runs   (  155.24 ms per token,     6.44 tokens per second)\n",
      "llama_perf_context_print:       total time =   10401.21 ms /   173 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 600 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   14598.13 ms /   600 tokens (   24.33 ms per token,    41.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22982.71 ms /   146 runs   (  157.42 ms per token,     6.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   37616.04 ms /   746 tokens\n",
      "Llama.generate: 139 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2729.15 ms /   116 tokens (   23.53 ms per token,    42.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5581.79 ms /    37 runs   (  150.86 ms per token,     6.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    8317.25 ms /   153 tokens\n",
      "Llama.generate: 139 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3027.39 ms /   132 tokens (   22.93 ms per token,    43.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6483.63 ms /    42 runs   (  154.37 ms per token,     6.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    9518.61 ms /   174 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 281 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    6487.07 ms /   281 tokens (   23.09 ms per token,    43.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13775.08 ms /    90 runs   (  153.06 ms per token,     6.53 tokens per second)\n",
      "llama_perf_context_print:       total time =   20280.59 ms /   371 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   11841.41 ms /   486 tokens (   24.37 ms per token,    41.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9217.03 ms /    58 runs   (  158.91 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =   21069.37 ms /   544 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 933 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   23003.17 ms /   933 tokens (   24.66 ms per token,    40.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12949.24 ms /    81 runs   (  159.87 ms per token,     6.26 tokens per second)\n",
      "llama_perf_context_print:       total time =   35969.05 ms /  1014 tokens\n",
      "Llama.generate: 851 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3412.60 ms /   138 tokens (   24.73 ms per token,    40.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12993.31 ms /    81 runs   (  160.41 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =   16422.48 ms /   219 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 230 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5433.16 ms /   230 tokens (   23.62 ms per token,    42.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7168.59 ms /    47 runs   (  152.52 ms per token,     6.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   12610.38 ms /   277 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5342.88 ms /   227 tokens (   23.54 ms per token,    42.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3342.74 ms /    22 runs   (  151.94 ms per token,     6.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    8689.57 ms /   249 tokens\n",
      "Llama.generate: 143 prefix-match hit, remaining 268 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 2 to translated_queries_TEST.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    6476.40 ms /   268 tokens (   24.17 ms per token,    41.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6724.66 ms /    43 runs   (  156.39 ms per token,     6.39 tokens per second)\n",
      "llama_perf_context_print:       total time =   13209.04 ms /   311 tokens\n",
      "Llama.generate: 145 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4795.85 ms /   203 tokens (   23.62 ms per token,    42.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12684.75 ms /    82 runs   (  154.69 ms per token,     6.46 tokens per second)\n",
      "llama_perf_context_print:       total time =   17496.53 ms /   285 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 1668 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   42028.99 ms /  1668 tokens (   25.20 ms per token,    39.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42517.60 ms /   255 runs   (  166.74 ms per token,     6.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   84631.22 ms /  1923 tokens\n",
      "Llama.generate: 91 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2073.27 ms /    82 tokens (   25.28 ms per token,    39.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3486.62 ms /    23 runs   (  151.59 ms per token,     6.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    5564.06 ms /   105 tokens\n",
      "Llama.generate: 51 prefix-match hit, remaining 250 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5890.42 ms /   250 tokens (   23.56 ms per token,    42.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5647.14 ms /    36 runs   (  156.87 ms per token,     6.37 tokens per second)\n",
      "llama_perf_context_print:       total time =   11544.18 ms /   286 tokens\n",
      "Llama.generate: 129 prefix-match hit, remaining 358 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    8654.25 ms /   358 tokens (   24.17 ms per token,    41.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9097.52 ms /    58 runs   (  156.85 ms per token,     6.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   17762.94 ms /   416 tokens\n",
      "Llama.generate: 129 prefix-match hit, remaining 225 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5323.77 ms /   225 tokens (   23.66 ms per token,    42.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8652.61 ms /    56 runs   (  154.51 ms per token,     6.47 tokens per second)\n",
      "llama_perf_context_print:       total time =   13987.19 ms /   281 tokens\n",
      "Llama.generate: 58 prefix-match hit, remaining 1501 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   38982.60 ms /  1501 tokens (   25.97 ms per token,    38.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43392.90 ms /   255 runs   (  170.17 ms per token,     5.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   82463.70 ms /  1756 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 1579 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   40481.50 ms /  1579 tokens (   25.64 ms per token,    39.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36715.40 ms /   228 runs   (  161.03 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =   77265.68 ms /  1807 tokens\n",
      "Llama.generate: 1221 prefix-match hit, remaining 359 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    9190.14 ms /   359 tokens (   25.60 ms per token,    39.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33989.00 ms /   215 runs   (  158.09 ms per token,     6.33 tokens per second)\n",
      "llama_perf_context_print:       total time =   43241.36 ms /   574 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 705 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   16765.69 ms /   705 tokens (   23.78 ms per token,    42.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11362.21 ms /    75 runs   (  151.50 ms per token,     6.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   28142.20 ms /   780 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3921.14 ms /   172 tokens (   22.80 ms per token,    43.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6462.64 ms /    44 runs   (  146.88 ms per token,     6.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   10391.55 ms /   216 tokens\n",
      "Llama.generate: 115 prefix-match hit, remaining 956 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   22953.46 ms /   956 tokens (   24.01 ms per token,    41.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20036.12 ms /   130 runs   (  154.12 ms per token,     6.49 tokens per second)\n",
      "llama_perf_context_print:       total time =   43019.17 ms /  1086 tokens\n",
      "Llama.generate: 673 prefix-match hit, remaining 347 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    8404.76 ms /   347 tokens (   24.22 ms per token,    41.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22686.23 ms /   148 runs   (  153.29 ms per token,     6.52 tokens per second)\n",
      "llama_perf_context_print:       total time =   31125.97 ms /   495 tokens\n",
      "Llama.generate: 59 prefix-match hit, remaining 309 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    7118.55 ms /   309 tokens (   23.04 ms per token,    43.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5755.97 ms /    39 runs   (  147.59 ms per token,     6.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   12881.48 ms /   348 tokens\n",
      "Llama.generate: 318 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1487.31 ms /    61 tokens (   24.38 ms per token,    41.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7619.02 ms /    51 runs   (  149.39 ms per token,     6.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    9115.62 ms /   112 tokens\n",
      "Llama.generate: 61 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4511.00 ms /   198 tokens (   22.78 ms per token,    43.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5812.89 ms /    40 runs   (  145.32 ms per token,     6.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   10330.97 ms /   238 tokens\n",
      "Llama.generate: 58 prefix-match hit, remaining 809 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   19222.92 ms /   809 tokens (   23.76 ms per token,    42.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18547.80 ms /   123 runs   (  150.80 ms per token,     6.63 tokens per second)\n",
      "llama_perf_context_print:       total time =   37797.79 ms /   932 tokens\n",
      "Llama.generate: 842 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =     547.46 ms /    22 tokens (   24.88 ms per token,    40.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9758.98 ms /    63 runs   (  154.90 ms per token,     6.46 tokens per second)\n",
      "llama_perf_context_print:       total time =   10317.87 ms /    85 tokens\n",
      "Llama.generate: 135 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   11251.37 ms /   479 tokens (   23.49 ms per token,    42.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8989.08 ms /    60 runs   (  149.82 ms per token,     6.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   20251.84 ms /   539 tokens\n",
      "Llama.generate: 137 prefix-match hit, remaining 924 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   22374.80 ms /   924 tokens (   24.22 ms per token,    41.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20016.00 ms /   130 runs   (  153.97 ms per token,     6.49 tokens per second)\n",
      "llama_perf_context_print:       total time =   42421.00 ms /  1054 tokens\n",
      "Llama.generate: 137 prefix-match hit, remaining 1434 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   35642.83 ms /  1434 tokens (   24.86 ms per token,    40.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40836.35 ms /   255 runs   (  160.14 ms per token,     6.24 tokens per second)\n",
      "llama_perf_context_print:       total time =   76560.65 ms /  1689 tokens\n",
      "Llama.generate: 58 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3233.92 ms /   141 tokens (   22.94 ms per token,    43.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5247.05 ms /    36 runs   (  145.75 ms per token,     6.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    8487.46 ms /   177 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3767.27 ms /   166 tokens (   22.69 ms per token,    44.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6177.98 ms /    42 runs   (  147.09 ms per token,     6.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    9952.40 ms /   208 tokens\n",
      "Llama.generate: 62 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5456.60 ms /   239 tokens (   22.83 ms per token,    43.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6878.82 ms /    47 runs   (  146.36 ms per token,     6.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   12344.27 ms /   286 tokens\n",
      "Llama.generate: 135 prefix-match hit, remaining 234 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5339.26 ms /   234 tokens (   22.82 ms per token,    43.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8058.90 ms /    55 runs   (  146.53 ms per token,     6.82 tokens per second)\n",
      "llama_perf_context_print:       total time =   13408.44 ms /   289 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 740 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   17537.64 ms /   740 tokens (   23.70 ms per token,    42.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17424.28 ms /   115 runs   (  151.52 ms per token,     6.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   34987.97 ms /   855 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 309 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    7257.44 ms /   309 tokens (   23.49 ms per token,    42.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8069.21 ms /    54 runs   (  149.43 ms per token,     6.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   15336.75 ms /   363 tokens\n",
      "Llama.generate: 96 prefix-match hit, remaining 53 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1429.45 ms /    53 tokens (   26.97 ms per token,    37.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2679.17 ms /    16 runs   (  167.45 ms per token,     5.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    4111.48 ms /    69 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5427.13 ms /   202 tokens (   26.87 ms per token,    37.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3729.81 ms /    24 runs   (  155.41 ms per token,     6.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    9161.35 ms /   226 tokens\n",
      "Llama.generate: 180 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3126.37 ms /   123 tokens (   25.42 ms per token,    39.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5428.53 ms /    35 runs   (  155.10 ms per token,     6.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    8563.23 ms /   158 tokens\n",
      "Llama.generate: 180 prefix-match hit, remaining 625 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   15340.79 ms /   625 tokens (   24.55 ms per token,    40.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11091.84 ms /    74 runs   (  149.89 ms per token,     6.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   26446.84 ms /   699 tokens\n",
      "Llama.generate: 180 prefix-match hit, remaining 318 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    7342.02 ms /   318 tokens (   23.09 ms per token,    43.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4767.27 ms /    32 runs   (  148.98 ms per token,     6.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   12114.93 ms /   350 tokens\n",
      "Llama.generate: 180 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2289.28 ms /    98 tokens (   23.36 ms per token,    42.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5984.41 ms /    41 runs   (  145.96 ms per token,     6.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    8281.08 ms /   139 tokens\n",
      "Llama.generate: 180 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1213.52 ms /    52 tokens (   23.34 ms per token,    42.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2446.62 ms /    17 runs   (  143.92 ms per token,     6.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    3663.21 ms /    69 tokens\n",
      "Llama.generate: 180 prefix-match hit, remaining 287 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    6621.91 ms /   287 tokens (   23.07 ms per token,    43.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3548.52 ms /    24 runs   (  147.85 ms per token,     6.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   10174.57 ms /   311 tokens\n",
      "Llama.generate: 54 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4330.61 ms /   190 tokens (   22.79 ms per token,    43.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5539.91 ms /    38 runs   (  145.79 ms per token,     6.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    9877.23 ms /   228 tokens\n",
      "Llama.generate: 55 prefix-match hit, remaining 182 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4236.97 ms /   182 tokens (   23.28 ms per token,    42.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6573.99 ms /    45 runs   (  146.09 ms per token,     6.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   10818.89 ms /   227 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   11159.61 ms /   481 tokens (   23.20 ms per token,    43.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11735.52 ms /    80 runs   (  146.69 ms per token,     6.82 tokens per second)\n",
      "llama_perf_context_print:       total time =   22910.19 ms /   561 tokens\n",
      "Llama.generate: 58 prefix-match hit, remaining 326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    7460.40 ms /   326 tokens (   22.88 ms per token,    43.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22790.08 ms /   150 runs   (  151.93 ms per token,     6.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   30286.41 ms /   476 tokens\n",
      "Llama.generate: 191 prefix-match hit, remaining 711 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   19235.46 ms /   711 tokens (   27.05 ms per token,    36.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16930.18 ms /   100 runs   (  169.30 ms per token,     5.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   36187.73 ms /   811 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2710.80 ms /   117 tokens (   23.17 ms per token,    43.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4739.64 ms /    29 runs   (  163.44 ms per token,     6.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    7455.85 ms /   146 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 434 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   10488.13 ms /   434 tokens (   24.17 ms per token,    41.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9567.77 ms /    58 runs   (  164.96 ms per token,     6.06 tokens per second)\n",
      "llama_perf_context_print:       total time =   20067.10 ms /   492 tokens\n",
      "Llama.generate: 174 prefix-match hit, remaining 383 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    9419.48 ms /   383 tokens (   24.59 ms per token,    40.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8154.04 ms /    54 runs   (  151.00 ms per token,     6.62 tokens per second)\n",
      "llama_perf_context_print:       total time =   17583.61 ms /   437 tokens\n",
      "Llama.generate: 109 prefix-match hit, remaining 300 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    7393.32 ms /   300 tokens (   24.64 ms per token,    40.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10269.92 ms /    66 runs   (  155.60 ms per token,     6.43 tokens per second)\n",
      "llama_perf_context_print:       total time =   17676.03 ms /   366 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 182 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4186.87 ms /   182 tokens (   23.00 ms per token,    43.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5240.55 ms /    35 runs   (  149.73 ms per token,     6.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    9433.49 ms /   217 tokens\n",
      "Llama.generate: 53 prefix-match hit, remaining 556 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   13378.63 ms /   556 tokens (   24.06 ms per token,    41.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19506.06 ms /   126 runs   (  154.81 ms per token,     6.46 tokens per second)\n",
      "llama_perf_context_print:       total time =   32914.36 ms /   682 tokens\n",
      "Llama.generate: 99 prefix-match hit, remaining 242 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5899.51 ms /   242 tokens (   24.38 ms per token,    41.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5092.65 ms /    34 runs   (  149.78 ms per token,     6.68 tokens per second)\n",
      "llama_perf_context_print:       total time =   10998.09 ms /   276 tokens\n",
      "Llama.generate: 99 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4944.74 ms /   214 tokens (   23.11 ms per token,    43.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17138.08 ms /   117 runs   (  146.48 ms per token,     6.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   22108.70 ms /   331 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 374 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    8640.71 ms /   374 tokens (   23.10 ms per token,    43.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5450.40 ms /    37 runs   (  147.31 ms per token,     6.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   14097.64 ms /   411 tokens\n",
      "Llama.generate: 52 prefix-match hit, remaining 364 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    8384.34 ms /   364 tokens (   23.03 ms per token,    43.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7703.62 ms /    52 runs   (  148.15 ms per token,     6.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   16097.45 ms /   416 tokens\n",
      "Llama.generate: 54 prefix-match hit, remaining 151 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3531.38 ms /   151 tokens (   23.39 ms per token,    42.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4687.36 ms /    30 runs   (  156.25 ms per token,     6.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    8224.05 ms /   181 tokens\n",
      "Llama.generate: 100 prefix-match hit, remaining 864 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   21085.19 ms /   864 tokens (   24.40 ms per token,    40.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14339.33 ms /    86 runs   (  166.74 ms per token,     6.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   35442.74 ms /   950 tokens\n",
      "Llama.generate: 100 prefix-match hit, remaining 155 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3624.96 ms /   155 tokens (   23.39 ms per token,    42.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5368.01 ms /    36 runs   (  149.11 ms per token,     6.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    8999.37 ms /   191 tokens\n",
      "Llama.generate: 54 prefix-match hit, remaining 301 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    7189.48 ms /   301 tokens (   23.89 ms per token,    41.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6137.97 ms /    40 runs   (  153.45 ms per token,     6.52 tokens per second)\n",
      "llama_perf_context_print:       total time =   13334.67 ms /   341 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1037.89 ms /    45 tokens (   23.06 ms per token,    43.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3135.49 ms /    21 runs   (  149.31 ms per token,     6.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    4176.93 ms /    66 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    2703.41 ms /   116 tokens (   23.31 ms per token,    42.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5688.65 ms /    38 runs   (  149.70 ms per token,     6.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    8398.80 ms /   154 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3296.11 ms /   132 tokens (   24.97 ms per token,    40.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7412.80 ms /    49 runs   (  151.28 ms per token,     6.61 tokens per second)\n",
      "llama_perf_context_print:       total time =   10717.57 ms /   181 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1040.00 ms /    43 tokens (   24.19 ms per token,    41.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4192.85 ms /    27 runs   (  155.29 ms per token,     6.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    5237.60 ms /    70 tokens\n",
      "Llama.generate: 54 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4473.20 ms /   185 tokens (   24.18 ms per token,    41.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7021.63 ms /    46 runs   (  152.64 ms per token,     6.55 tokens per second)\n",
      "llama_perf_context_print:       total time =   11503.41 ms /   231 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5627.60 ms /   220 tokens (   25.58 ms per token,    39.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10778.11 ms /    68 runs   (  158.50 ms per token,     6.31 tokens per second)\n",
      "llama_perf_context_print:       total time =   16419.28 ms /   288 tokens\n",
      "Llama.generate: 133 prefix-match hit, remaining 442 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   10931.80 ms /   442 tokens (   24.73 ms per token,    40.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15953.30 ms /   104 runs   (  153.40 ms per token,     6.52 tokens per second)\n",
      "llama_perf_context_print:       total time =   26908.09 ms /   546 tokens\n",
      "Llama.generate: 125 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1185.46 ms /    49 tokens (   24.19 ms per token,    41.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4666.41 ms /    31 runs   (  150.53 ms per token,     6.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    5857.29 ms /    80 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 330 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    7788.18 ms /   330 tokens (   23.60 ms per token,    42.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13424.48 ms /    87 runs   (  154.30 ms per token,     6.48 tokens per second)\n",
      "llama_perf_context_print:       total time =   21230.77 ms /   417 tokens\n",
      "Llama.generate: 58 prefix-match hit, remaining 274 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    6405.90 ms /   274 tokens (   23.38 ms per token,    42.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10240.41 ms /    64 runs   (  160.01 ms per token,     6.25 tokens per second)\n",
      "llama_perf_context_print:       total time =   16658.96 ms /   338 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 1199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   30517.28 ms /  1199 tokens (   25.45 ms per token,    39.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41081.98 ms /   255 runs   (  161.11 ms per token,     6.21 tokens per second)\n",
      "llama_perf_context_print:       total time =   71679.85 ms /  1454 tokens\n",
      "Llama.generate: 808 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    5458.19 ms /   224 tokens (   24.37 ms per token,    41.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26724.67 ms /   169 runs   (  158.13 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =   32226.02 ms /   393 tokens\n",
      "Llama.generate: 134 prefix-match hit, remaining 1020 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   26461.66 ms /  1020 tokens (   25.94 ms per token,    38.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28139.18 ms /   173 runs   (  162.65 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   54646.94 ms /  1193 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 1694 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   45861.32 ms /  1694 tokens (   27.07 ms per token,    36.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42619.20 ms /   255 runs   (  167.13 ms per token,     5.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   88573.41 ms /  1949 tokens\n",
      "Llama.generate: 127 prefix-match hit, remaining 534 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   12958.72 ms /   534 tokens (   24.27 ms per token,    41.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9985.39 ms /    65 runs   (  153.62 ms per token,     6.51 tokens per second)\n",
      "llama_perf_context_print:       total time =   22957.01 ms /   599 tokens\n",
      "Llama.generate: 132 prefix-match hit, remaining 751 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   19194.24 ms /   751 tokens (   25.56 ms per token,    39.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13319.58 ms /    83 runs   (  160.48 ms per token,     6.23 tokens per second)\n",
      "llama_perf_context_print:       total time =   32531.16 ms /   834 tokens\n",
      "Llama.generate: 231 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    1225.11 ms /    50 tokens (   24.50 ms per token,    40.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4564.06 ms /    30 runs   (  152.14 ms per token,     6.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    5794.45 ms /    80 tokens\n",
      "Llama.generate: 127 prefix-match hit, remaining 155 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3880.95 ms /   155 tokens (   25.04 ms per token,    39.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8094.07 ms /    53 runs   (  152.72 ms per token,     6.55 tokens per second)\n",
      "llama_perf_context_print:       total time =   11984.89 ms /   208 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   11706.76 ms /   460 tokens (   25.45 ms per token,    39.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23059.90 ms /   147 runs   (  156.87 ms per token,     6.37 tokens per second)\n",
      "llama_perf_context_print:       total time =   34803.15 ms /   607 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    4526.58 ms /   189 tokens (   23.95 ms per token,    41.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3439.92 ms /    22 runs   (  156.36 ms per token,     6.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    7970.49 ms /   211 tokens\n",
      "Llama.generate: 122 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3098.31 ms /   130 tokens (   23.83 ms per token,    41.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9988.01 ms /    64 runs   (  156.06 ms per token,     6.41 tokens per second)\n",
      "llama_perf_context_print:       total time =   13098.68 ms /   194 tokens\n",
      "Llama.generate: 122 prefix-match hit, remaining 146 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =    3497.50 ms /   146 tokens (   23.96 ms per token,    41.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9104.36 ms /    61 runs   (  149.25 ms per token,     6.70 tokens per second)\n",
      "llama_perf_context_print:       total time =   12613.24 ms /   207 tokens\n",
      "Llama.generate: 122 prefix-match hit, remaining 896 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4309.42 ms\n",
      "llama_perf_context_print: prompt eval time =   22038.71 ms /   896 tokens (   24.60 ms per token,    40.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18024.43 ms /   114 runs   (  158.11 ms per token,     6.32 tokens per second)\n",
      "llama_perf_context_print:       total time =   40089.61 ms /  1010 tokens\n",
      "Llama.generate: 57 prefix-match hit, remaining 1065 prompt tokens to eval\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m translated_batch = []\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     translated = \u001b[43mtranslate_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     translated_batch.append(translated)\n\u001b[32m     21\u001b[39m df_batch = pd.DataFrame({\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEsgish\u001b[39m\u001b[33m\"\u001b[39m: batch,\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEnglish\u001b[39m\u001b[33m\"\u001b[39m: translated_batch\n\u001b[32m     24\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mtranslate_query\u001b[39m\u001b[34m(query, max_total_tokens, max_output_tokens)\u001b[39m\n\u001b[32m     86\u001b[39m prompt = instruction + context + query_part\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Call model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\llama_cpp\\llama.py:1902\u001b[39m, in \u001b[36mLlama.__call__\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1838\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m   1839\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1840\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1864\u001b[39m     logit_bias: Optional[Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1865\u001b[39m ) -> Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[32m   1866\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[32m   1867\u001b[39m \n\u001b[32m   1868\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1900\u001b[39m \u001b[33;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[32m   1901\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1902\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1903\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1904\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m        \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m=\u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1922\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\llama_cpp\\llama.py:1835\u001b[39m, in \u001b[36mLlama.create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1833\u001b[39m     chunks: Iterator[CreateCompletionStreamResponse] = completion_or_chunks\n\u001b[32m   1834\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[32m-> \u001b[39m\u001b[32m1835\u001b[39m completion: Completion = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1836\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\llama_cpp\\llama.py:1320\u001b[39m, in \u001b[36mLlama._create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1318\u001b[39m finish_reason = \u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1319\u001b[39m multibyte_fix = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllama_token_is_eog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\llama_cpp\\llama.py:912\u001b[39m, in \u001b[36mLlama.generate\u001b[39m\u001b[34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    913\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx < \u001b[38;5;28mself\u001b[39m.n_tokens:\n\u001b[32m    914\u001b[39m         token = \u001b[38;5;28mself\u001b[39m.sample(\n\u001b[32m    915\u001b[39m             top_k=top_k,\n\u001b[32m    916\u001b[39m             top_p=top_p,\n\u001b[32m   (...)\u001b[39m\u001b[32m    930\u001b[39m             idx=sample_idx,\n\u001b[32m    931\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\llama_cpp\\llama.py:646\u001b[39m, in \u001b[36mLlama.eval\u001b[39m\u001b[34m(self, tokens)\u001b[39m\n\u001b[32m    642\u001b[39m n_tokens = \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[32m    643\u001b[39m \u001b[38;5;28mself\u001b[39m._batch.set_batch(\n\u001b[32m    644\u001b[39m     batch=batch, n_past=n_past, logits_all=\u001b[38;5;28mself\u001b[39m.context_params.logits_all\n\u001b[32m    645\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[32m    648\u001b[39m \u001b[38;5;28mself\u001b[39m.input_ids[n_past : n_past + n_tokens] = batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\llama_cpp\\_internals.py:306\u001b[39m, in \u001b[36mLlamaContext.decode\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     return_code = \u001b[43mllama_cpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_code != \u001b[32m0\u001b[39m:\n\u001b[32m    311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "#Reads and stores the Esgish queries\n",
    "df = pd.read_excel(\"esgish_short.xlsx\")\n",
    "queries = df[\"Esgish\"].tolist()\n",
    "#Ensures no overload and efficiency\n",
    "batch_size = 100 \n",
    "output_file = \"translated_queries_TEST.xlsx\"\n",
    "\n",
    "#Looks at each query in each batch, calls the translate_query function, and stores it\n",
    "for i in range(0, len(queries), batch_size):\n",
    "    batch = queries[i:i + batch_size]\n",
    "    translated_batch = []\n",
    "    \n",
    "    for query in batch:\n",
    "        translated = translate_query(query)\n",
    "        translated_batch.append(translated)\n",
    "    \n",
    "    df_batch = pd.DataFrame({\n",
    "        \"Esgish\": batch,\n",
    "        \"English\": translated_batch\n",
    "    })\n",
    "\n",
    "    #Makes a new file if needed, or adds onto the current file during each batch in case the program crashes at some point\n",
    "    if i == 0:\n",
    "        df_batch.to_excel(output_file, index=False)  \n",
    "    else:\n",
    "        with pd.ExcelWriter(output_file, mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"overlay\") as writer:\n",
    "            df_batch.to_excel(writer, index=False, header=False, startrow=i + 1)\n",
    "    \n",
    "    print(f\"Saved batch {i // batch_size + 1} to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814e3a0-0ddd-467e-8074-18a33f13a0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f42872-6939-4c34-b74f-f177f8eb3d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
