{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25f859b-559e-4007-a500-1762ea9f2276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: transformers in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (3.5.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (1.6.0)\n",
      "Requirement already satisfied: peft in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (0.15.1)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (0.45.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: psutil in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: networkx in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d1a4b90-4bfa-465b-af13-41a3e09c7460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32adb204-5ad1-4534-9554-467d20136161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d910914975a4fab9684748e8da4bfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36698a96405d4f2c94bc27c3cf739a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951539243d6d4fc78423708cc32470fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c739a8af27364680b29f160febe34bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f90c72d372847b3bfe9776851aa53d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1527390ecbd042bca93dfd7069105ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a528342e9e4b4aaf53e06e86f18cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46222d8b2867402fbc33e33fdd4abba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa1993d0de94da8bc6c46b8bfa2eeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a472ecb4c5404e9e85be1fecd601ee82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\", force_download=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", force_download=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ab97c72-333a-4bcb-a1aa-6f539efbb301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded successfully.\n",
      "                                           Esgish  \\\n",
      "0      [AbortifacientsDistrMaxRevRatio1Y] < '0.1'   \n",
      "1     [AbortifacientsDistrMaxRevRatio1Y] > '0.01'   \n",
      "2        [AbortifacientsDistrMaxRevRatio3Y] > '0'   \n",
      "3  [AbortifacientsInvolvement] ANY 'Distribution'   \n",
      "4    [AbortifacientsInvolvement] ANY 'Production'   \n",
      "\n",
      "                                             English  \n",
      "0  All companies or issuers where the maximum sha...  \n",
      "1  All companies or issuers where the maximum sha...  \n",
      "2  All companies that have a maximum share of rev...  \n",
      "3  All companies or issuers that are involved in ...  \n",
      "4  All companies or issuers that produce abortifa...  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235e9c44ee734c489c1faf77b86b58b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd6fa214ab34361964a4aaafdf97cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d5bb4c230642c6bcf3becadcba21d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [863, 370, 3, 9, 570, 13, 66, 688, 42, 962, 52, 7, 24, 942, 8, 826, 6683, 10, 1300, 20534, 53, 2336, 3, 18, 2570, 115, 5, 21489, 1575, 41, 6210, 10, 37, 349, 65, 3, 9, 1516, 3134, 6275, 13, 44, 709, 3, 18, 4704, 12, 8, 10970, 13, 8, 5997, 96, 4302, 3473, 53, 387, 1280, 1682, 20534, 53, 2336, 3, 18, 2570, 115, 5, 21489, 1575, 41, 3174, 26, 5, 61, 10, 37, 349, 704, 494, 42, 364, 24, 43, 3, 9, 1516, 3134, 6275, 13, 44, 709, 3, 18, 4704, 12, 8, 10970, 13, 8, 5997, 96, 4302, 3473, 53, 387, 1280, 1877, 20534, 53, 2336, 3, 18, 2570, 115, 5, 4249, 7593, 15, 26, 41, 6210, 10, 37, 349, 65, 3, 9, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [4674, 599, 6306, 134, 302, 5231, 40, 28632, 12988, 3728, 5890, 115, 4302, 17, 908, 2490, 2423, 3, 31, 2292, 31, 6, 6306, 134, 302, 5231, 40, 28632, 3174, 26, 5890, 115, 4302, 17, 908, 206, 195, 3, 31, 9, 31, 6, 6306, 134, 302, 5231, 40, 28632, 3174, 26, 5890, 115, 4302, 17, 908, 206, 195, 3, 31, 15, 31, 6, 6306, 134, 302, 5231, 40, 28632, 12988, 3728, 5890, 115, 667, 115, 7593, 908, 2490, 2423, 3, 31, 2292, 31, 6, 6306, 134, 302, 5231, 40, 28632, 3174, 26, 5890, 115, 667, 115, 7593, 908, 206, 195, 3, 31, 15, 31, 6, 6306, 134, 302, 5231, 40, 28632, 3174, 26, 5890, 115, 667, 115, 7593, 908, 206, 195, 3, 31, 9, 31, 6, 6306, 134, 1]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# English to Esgish Data\n",
    "train_excel_path = \"translated_queries_TEST4.xlsx\"\n",
    "\n",
    "train_df = pd.read_excel(train_excel_path)\n",
    "expected_columns = [\"Esgish\", \"English\"]\n",
    "train_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "for col in expected_columns:\n",
    "    assert col in train_df.columns, f\"Missing expected column: {col}\"\n",
    "print(\"Training data loaded successfully.\")\n",
    "print(train_df.head())\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "    train_data.append({\n",
    "        \"input\": f\"Convert to Esgish, a query language: {row['English']}\",\n",
    "        \"output\": row[\"Esgish\"]\n",
    "    })\n",
    "\n",
    "train_df, temp_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "eval_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"English\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    labels = tokenizer(examples[\"Esgish\"], max_length=128, truncation=True, padding=\"max_length\")    \n",
    "    model_inputs = {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": labels[\"input_ids\"], \n",
    "    }\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"English\", \"Esgish\"])\n",
    "eval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=[\"English\", \"Esgish\"])\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=[\"English\", \"Esgish\"])\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bfec16e2-5f48-48b5-ba82-80022d17d474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded successfully.\n",
      "                                           Esgish  \\\n",
      "0      [AbortifacientsDistrMaxRevRatio1Y] < '0.1'   \n",
      "1     [AbortifacientsDistrMaxRevRatio1Y] > '0.01'   \n",
      "2        [AbortifacientsDistrMaxRevRatio3Y] > '0'   \n",
      "3  [AbortifacientsInvolvement] ANY 'Distribution'   \n",
      "4    [AbortifacientsInvolvement] ANY 'Production'   \n",
      "\n",
      "                                             English  \n",
      "0  All companies or issuers where the maximum sha...  \n",
      "1  All companies or issuers where the maximum sha...  \n",
      "2  All companies that have a maximum share of rev...  \n",
      "3  All companies or issuers that are involved in ...  \n",
      "4  All companies or issuers that produce abortifa...  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295d7f1639bd43a189f87a3f6274b3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35d662e524c4f2c839e30ab5eadff09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81459f530484f1fb463180a826416d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [4674, 599, 6306, 134, 302, 5231, 40, 28632, 12988, 3728, 5890, 115, 4302, 17, 908, 2490, 2423, 3, 31, 2292, 31, 6, 6306, 134, 302, 5231, 40, 28632, 3174, 26, 5890, 115, 4302, 17, 908, 206, 195, 3, 31, 9, 31, 6, 6306, 134, 302, 5231, 40, 28632, 3174, 26, 5890, 115, 4302, 17, 908, 206, 195, 3, 31, 15, 31, 6, 6306, 134, 302, 5231, 40, 28632, 12988, 3728, 5890, 115, 667, 115, 7593, 908, 2490, 2423, 3, 31, 2292, 31, 6, 6306, 134, 302, 5231, 40, 28632, 3174, 26, 5890, 115, 667, 115, 7593, 908, 206, 195, 3, 31, 15, 31, 6, 6306, 134, 302, 5231, 40, 28632, 3174, 26, 5890, 115, 667, 115, 7593, 908, 206, 195, 3, 31, 9, 31, 6, 6306, 134, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [863, 370, 3, 9, 570, 13, 66, 688, 42, 962, 52, 7, 24, 942, 8, 826, 6683, 10, 1300, 20534, 53, 2336, 3, 18, 2570, 115, 5, 21489, 1575, 41, 6210, 10, 37, 349, 65, 3, 9, 1516, 3134, 6275, 13, 44, 709, 3, 18, 4704, 12, 8, 10970, 13, 8, 5997, 96, 4302, 3473, 53, 387, 1280, 1682, 20534, 53, 2336, 3, 18, 2570, 115, 5, 21489, 1575, 41, 3174, 26, 5, 61, 10, 37, 349, 704, 494, 42, 364, 24, 43, 3, 9, 1516, 3134, 6275, 13, 44, 709, 3, 18, 4704, 12, 8, 10970, 13, 8, 5997, 96, 4302, 3473, 53, 387, 1280, 1877, 20534, 53, 2336, 3, 18, 2570, 115, 5, 4249, 7593, 15, 26, 41, 6210, 10, 37, 349, 65, 3, 9, 1]}\n"
     ]
    }
   ],
   "source": [
    "#Esgish to English Data\n",
    "train_excel_path = \"translated_queries_TEST4.xlsx\"\n",
    "\n",
    "train_df = pd.read_excel(train_excel_path)\n",
    "expected_columns = [\"Esgish\", \"English\"]\n",
    "train_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "for col in expected_columns:\n",
    "    assert col in train_df.columns, f\"Missing expected column: {col}\"\n",
    "print(\"Training data loaded successfully.\")\n",
    "print(train_df.head())\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "    train_data.append({\n",
    "        \"input\": f\"Convert to English: {row['Esgish']}\",\n",
    "        \"output\": row[\"English\"]\n",
    "    })\n",
    "\n",
    "train_df, temp_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "eval_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"Esgish\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    labels = tokenizer(examples[\"English\"], max_length=128, truncation=True, padding=\"max_length\")    \n",
    "    model_inputs = {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": labels[\"input_ids\"], \n",
    "    }\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"Esgish\", \"English\"])\n",
    "eval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=[\"Esgish\", \"English\"])\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=[\"Esgish\", \"English\"])\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "996291bb-3007-4358-9515-5af3a9874a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\n",
      "Requirement already satisfied: requests in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jason chen\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
      "Training started...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='291' max='291' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [291/291 20:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.670527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.616724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.602195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training done\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq, T5Tokenizer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"new_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class = tokenizer,\n",
    "    data_collator=data_collator,\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Training started...\")\n",
    "\n",
    "trainer.train()\n",
    "print(\"training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e905afd2-9c9e-4491-b38e-6350a5aef951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./new_model2\\\\tokenizer_config.json',\n",
       " './new_model2\\\\special_tokens_map.json',\n",
       " './new_model2\\\\spiece.model',\n",
       " './new_model2\\\\added_tokens.json',\n",
       " './new_model2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./new_model2\")\n",
    "tokenizer.save_pretrained(\"./new_model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4766afbd-cf5f-41cd-9c49-0c925d7540b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleanup\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04c73cac-2013-4d6f-8202-bf4690f9c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./new_model2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./new_model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2788bdf7-8bc6-465c-aab5-2b1cae796ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  All companies or issuers listed on the AFL-CIO Boycott List.\n",
      "Correct:  [AFLCIOBoycottList] True\n",
      "Model:  All companies or issuers listed on the AFL-CIO Boycott List. \n",
      "\n",
      "\n",
      "Query:  All companies or issuers that have an alcohol revenue share interval between 10% and 15%, or between 15% and 20%, or between 20% and 25%, or between 25% and 50%, or between 5% and 10%, or between 50% and 100%.\n",
      "Correct:  [AlcoholRevShareInterval] IN '[10-15%)|[15-20%)|[20-25%)|[25-50%)|[5-10%)|[50-100%]'\n",
      "Model:  All companies or issuers that have an alcohol revenue share interval between 10% and 15%, between 15% and 20%, between 20% and 25%, between 25% and 50%, or between 5% and 10%, or between 50% and 100%. \n",
      "\n",
      "\n",
      "Query:  All companies with a minimum alcohol distribution percentage of revenue greater than 50% in the latest financial year.\n",
      "Correct:  [AlcoholDistributionMinRev] > '0.5'\n",
      "Model:  All companies with a minimum alcohol distribution percentage of revenue greater than 50% in the latest financial year. \n",
      "\n",
      "\n",
      "Query:  All companies or issuers involved in the production, distribution, or provision of services related to abortifacients.\n",
      "Correct:  [AbortifacientsInvolvement] ANY 'Production|Distribution|Services'\n",
      "Model:  All companies or issuers that are involved in the production, distribution, or provision of services related to abortifacients. \n",
      "\n",
      "\n",
      "Query:  All companies with a maximum share of revenue from abortifacients greater than 5% in the latest financial year.\n",
      "Correct:  [AbortifacientsTotalMaxRevRatio1Y] > '0.05'\n",
      "Model:  All companies with a maximum share of revenue from abortifacients greater than 5% in the latest financial year. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#translate random queries\n",
    "excel = pd.read_excel(\"translated_queries_TEST3_50q.xlsx\")\n",
    "sample = excel[[\"Esgish\", \"English\"]].dropna().sample(5).values\n",
    "\n",
    "for esg, eng in sample:\n",
    "    input_text = \"Convert to Esgish, a query language: \" + eng\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=1024,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    print(\"Query: \", eng)\n",
    "    print(\"Correct: \", esg)\n",
    "    print(\"Model: \", tokenizer.decode(output_ids[0], skip_special_tokens=True), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4b13597-63a4-4881-9590-05be0e1b0a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  AND([FossilFuelInvolvement] ANY 'Production|Exploration',[FossilFuelRevShareMin] > '0')\n",
      "Correct:  All companies or issuers that are involved in the production or exploration of fossil fuels and have a minimum percentage of revenue derived from fossil fuel business activities greater than 0.\n",
      "Model:  Please provide a list of all companies or issuers that are involved in the production or exploration of fossil fuels and have a minimum percentage of revenue derived from fossil fuel involvement greater than 0. \n",
      "\n",
      "\n",
      "Query:  OR([TobaccoDistMaxRev] > '0.05',[TobaccoProdMaxRev] > '0',[TobaccoServMaxRev] > '0.05',[AlcoholServiceMaxRev] > '0.05',[AlcoholTotalProdMaxRev] == '0.05',[AlcoholDistributionMaxRev] > '0.05')\n",
      "Correct:  Please provide a list of companies or issuers that have a tobacco-related revenue share of more than 5% or an alcohol-related revenue share of more than 5%, or a tobacco-related revenue share of more than 5% and an alcohol-related revenue share of more than 5%, or a tobacco-related revenue share of more than 5% and an alcohol-related revenue share of 5%, or a tobacco-related revenue share of more than 5% and an alcohol-related revenue share of more than 5% and a tobacco-related revenue share of more than 5%, or an alcohol-related revenue share of more than 5% and a tobacco-related revenue share of more than 5% and a tobacco-related revenue share of more than 5%.\n",
      "Model:  Please provide a list of companies or issuers that have a tobacco distribution maximum revenue share of more than 5%, a tobacco production maximum revenue share of more than 0%, a tobacco service maximum revenue share of more than 5%, a tobacco total revenue share of more than 5%, and a tobacco distribution maximum revenue share of more than 5%. \n",
      "\n",
      "\n",
      "Query:  AND([FossilFuelCoalExtractRevShareMax] <= '0.05',[FossilFuelCoalTradeRevShareMax] <= '0.3',[FossilFuelCoalPowerRevShareMax] <= '0.05',[FossilFuelTotCoalProdRevShareMax] <= '0.05',[NuclearPowerProdRevShareMax] <= '0.1',[AnimalWelfareFurOnlyProdSignInv] NONE 'At least 5%',[MilitaryEqmtRevShareInterval] IN '[0-0%]|(0-1%)|[1-5%)|[5-10%)',[GamblingRevShareInterval] IN '[0-0%]|(0-1%)|[1-5%)',[PornographyRevShareInterval] IN '[0-0%]|(0-1%)|[1-5%)',[CivFARevShareInterval] IN '[0-0%]|(0-1%)|[1-5%)|[5-10%)')\n",
      "Correct:  The query is asking for companies that meet the following criteria:\n",
      "\n",
      "* Fossil Fuel - Coal Extraction Maximum Revenue Share (%): Less than or equal to 5%\n",
      "* Fossil Fuel - Coal Trade Maximum Percentage of Revenue (%): Less than or equal to 30%\n",
      "* Fossil Fuel - Coal Power Maximum Revenue Share (%): Less than or equal to 5%\n",
      "* Fossil Fuel - Total Coal Production Maximum Revenue Share (%): Less than or equal to 5%\n",
      "* Nuclear Power - Production Maximum Percentage of Revenues (%): Less than or equal to 10%\n",
      "* Fur - Production Significant Involvement: None\n",
      "* Military Equipment and Services - Revenue Share Interval: 0-1%, 1-5%, 5-10%\n",
      "* Gambling - Revenue Share Interval: 0-1%, 1-5%\n",
      "* Pornography - Revenue Share Interval: 0-1%, 1-5%\n",
      "* Civilian Firearms - Revenue Share Interval: 0-1%, 1-\n",
      "Model:  Please provide a list of companies or issuers that have a maximum revenue share of 5% from coal extraction, 5% from coal trade, 5% from coal power, 5% from coal tot coal production, 1% from nuclear power production, 1% from pornography, 1% from civilian firearms, and 5% from coal mining. \n",
      "\n",
      "\n",
      "Query:  [AlcoholInvolvement] ANY 'Production|Services'\n",
      "Correct:  All companies or issuers that are involved in the production or services related to alcoholic beverages.\n",
      "Model:  All companies or issuers that are involved in the production or provision of services related to alcoholic beverages. \n",
      "\n",
      "\n",
      "Query:  OR([SusSolPromotingSusBuildingsScore] < '10000000',[SusSolProvidingBasicSrvcsScore] < '1000000000',[SusSolEnsuringHealthScore] > '1000000000',[SusSolMitigatingClimateChngScore] > '-10000000',[SusSolConservingWaterScore] > '-10000000',[SusSolAlleviatingPovertyScore] > '-10000000',[SusSolOptimisingMaterialUseScore] > '-10000000',[SusSolContSusEnergyUseScore] > '-10000000',[SusSolClimatePercentCombCont] < '0.1',[SusSolWaterPercentCombCont] > '-100000',[SusSolPromotingSusBuildingsScore] > '1000000000',[SusSolAlleviatingPovertyScore] > '1000000000',[SusSolAttainingGenEqualityScore] > '1000000000',[SusSolCombatingHungerScore] > '1000000000',[SusSolDeliveringEducationScore] > '1000000000',[SusSolEnsuringHealthScore] > '1000000000')\n",
      "Correct:  Please provide a list of companies or issuers that meet the following criteria:\n",
      "\n",
      "1. SDG Solutions Score - Promoting Sustainable Buildings: Less than 10000000\n",
      "2. SDG Solutions Score - Providing Basic Services: Less than 1000000000\n",
      "3. SDG Solutions Score - Ensuring Health: Greater than 1000000000\n",
      "4. SDG Solutions Score - Mitigating Climate Change: Greater than -10000000\n",
      "5. SDG Solutions Score - Conserving Water: Greater than -10000000\n",
      "6. SDG Solutions Score - Alleviating Poverty: Greater than -10000000\n",
      "7. SDG Solutions Score - Optimising Material Use: Greater than -10000000\n",
      "8. SDG Solutions Score - Controlling Sustainable Energy Use: Greater than -10000000\n",
      "9. SDG Solutions Score - Climate Percentage Combustion Controlled: Less than 0.1\n",
      "Model:  Please provide a list of companies or issuers that meet the following criteria: 1. SusSolPromotingSusBuildingsScore == '10000000' 2. SusSolProvidingBasic SrvcsScore == '1000000000' 3. SusSolEnsuringHealthScore == '1000000000' 4. SusSolMitigatingClimateChngScore == '-10000000' 5. SusSolConservingWaterScore == '-10000000' 6. SusSolOptimisingMaterialUseScore == '-10000000' 7. SusSolContSusEnergyUseScore == '-10000000' 8. SusSolContSusEnergyUseScore == '-100000' 9. SusSolPromotingSusBuildingsScore == '1000000000' 10. SusSolAlleviatingPovertyScore == '1000000000' 11. SusSolAttainingGenEqualityScore == '0.1' 12. SusSolContSusEnergyUseScore == '-100000' 13. SusSolDeliveringEducationScore == '1000000000' 14. SusSolEnsuringHealthScore == '1000000000' \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#translate esgish queries\n",
    "excel = pd.read_excel(\"translated_queries_TEST4.xlsx\")\n",
    "sample = excel[[\"Esgish\", \"English\"]].dropna().sample(5).values\n",
    "\n",
    "for esg, eng in sample:\n",
    "    input_text = \"Convert to English: \" + esg\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=1024,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    print(\"Query: \", esg)\n",
    "    print(\"Correct: \", eng)\n",
    "    print(\"Model: \", tokenizer.decode(output_ids[0], skip_special_tokens=True), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83a19a2e-ecbd-4660-8a68-e50d922fc7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  OR([NBSOverallFlag] == 'RED',[CoalMiningRevShareMaxThermal] > '0.05',[PowGenRevShareCoalMax] > '0.05',[TobaccoProdMaxRev] > '0.05',[TobaccoDistMaxRev] > '0.05',[PornographyDistMaxRev] > '0',[PornographyProdMaxRev] > '0',[GamblingDistMaxRev] >= '0.05',[GamblingProdMaxRev] >= '0.05',[StemCellCloning] True,[APMinesOverallFlag] == 'RED',[BiologicalWeaponsOverallFlag] == 'RED',[ChemicalWeaponsOverallFlag] == 'RED',[ClusterMunitionsOverallFlag] == 'RED',[DepletedUraniumOverallFlag] == 'RED',[NuclearWeaponsOverallFlag] == 'RED',[NuclearWeaponsNonNPTOverallFlag] == 'RED',[WhitePhosphorusOverallFlag] == 'RED',[issuerID] == '0')\n",
      "Model:  Please provide a list of companies or issuers that meet the following criteria: 1. NBS Overall Flag: Red 2. Coal Mining Revenue Share Max Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage of Revenues Percentage \n",
      "\n",
      "\n",
      "Query:  OR([APMinesOverallFlag] == 'RED',[BiologicalWeaponsOverallFlag] == 'RED',[ChemicalWeaponsOverallFlag] == 'RED',[ClusterMunitionsOverallFlag] == 'RED',[NuclearWeaponsNonNPTOverallFlag] == 'RED',[NuclearWeaponsOverallFlag] == 'RED',[DepletedUraniumOverallFlag] == 'RED')\n",
      "Model:  Please provide a list of all companies or issuers that meet the following criteria: 1. All companies or issuers that have a Red Flag for PGMines, 2. All companies or issuers that have a Red Flag for Biological Weapons, 3. Chemical Weapons, 4. Cluster Munitions, 5. Nuclear Weapons Non-Proliferation Treaty, 6. Nuclear Weapons, 7. Depleted Uranium \n",
      "\n",
      "\n",
      "Query:  AND([MeetingDate] PST '2019-07-05',[PayPracticePolicyProblem] == 'Yes, policy and practice')\n",
      "Model:  Please provide a list of all companies or issuers that meet the following criteria: 1. Meeting Date: July 5th, 2. Payment Practice Policy Problem: Yes, policy and practice 3. Meeting Date: July 5th, 4. Payment Practice Policy Problem: Yes, policy and practice 5. Meeting Date: July 5th, 6. Payment Practice Policy Problem: Yes, policy and practice \n",
      "\n",
      "\n",
      "Query:  [SDGROverall] IS :NC\n",
      "Model:  All companies or issuers that meet the following criteria: \n",
      "\n",
      "\n",
      "Query:  OR(AND([CaseScore] >= '5',[CaseBeginDate] PST '2024-01-04',[CaseType] == 'Norms-Based Screening'),AND([CaseScore] >= '7',[NBRCaseUpdatedDate] PST '2024-04-01'))\n",
      "Model:  Please provide a list of all companies or issuers that have a Case Score of 5 or higher, a Case Start Date of 2024-01-04, and a Case Type of Screening of 7 or higher. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "excel = pd.read_excel(\"C:/Users/Jason Chen/Desktop/OU/OU24-25/CS4273/Esgish2_edited.xlsx\")\n",
    "sample = excel[\"Esgish\"].dropna().sample(5).values\n",
    "\n",
    "for esg in sample:\n",
    "    input_text = \"Convert to English: \" + esg\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=1024,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    print(\"Query: \", esg)\n",
    "    print(\"Model: \", tokenizer.decode(output_ids[0], skip_special_tokens=True), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e71e132-b261-4fa2-a1fc-d9734e602ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
